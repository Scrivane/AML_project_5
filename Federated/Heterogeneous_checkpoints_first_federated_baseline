{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lGxnD3UbdWmb"},"outputs":[],"source":["# ── A. Imports & Reproducibility ────────────────────────────────────────────────\n","import os, copy\n","import csv                                                  # For result logging :contentReference[oaicite:0]{index=0}\n","import random                                               # For seeding :contentReference[oaicite:1]{index=1}\n","import numpy as np                                          # For numeric ops :contentReference[oaicite:2]{index=2}\n","import torch                                               # Core PyTorch :contentReference[oaicite:3]{index=3}\n","import torch.nn as nn                                       # Neural-net modules :contentReference[oaicite:4]{index=4}\n","import torch.nn.functional as F                             # Functional API :contentReference[oaicite:5]{index=5}\n","import torch.optim as optim                                 # Optimizers :contentReference[oaicite:6]{index=6}\n","from torch.optim.lr_scheduler import CosineAnnealingLR      # Scheduler :contentReference[oaicite:7]{index=7}\n","from torch.utils.data import DataLoader, random_split       # Data loaders & splits :contentReference[oaicite:8]{index=8}\n","import torchvision                                          # Datasets & transforms :contentReference[oaicite:9]{index=9}\n","import torchvision.transforms as T                          # Transforms :contentReference[oaicite:10]{index=10}\n","from torch.utils.tensorboard import SummaryWriter           # TensorBoard logging :contentReference[oaicite:11]{index=11}\n","import matplotlib.pyplot as plt                             # Plotting :contentReference[oaicite:12]{index=12}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39Q3SwyBdWnP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754333730686,"user_tz":-120,"elapsed":10,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}},"outputId":"1217864f-26a1-4a57-aba7-c89560e86d4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["# Seed everything for reproducibility\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","\n","# ── B. Device ───────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")                             # Confirm GPU vs CPU :contentReference[oaicite:13]{index=13}\n","\n","\n","\n","# ── C. Data Preparation ─────────────────────────────────────────────────────────\n","# Transforms\n","transform_train = T.Compose([\n","    T.RandomCrop(32, padding=4), T.RandomHorizontalFlip(),\n","    T.ToTensor(),\n","    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n","])\n","transform_test = T.Compose([\n","    T.ToTensor(),\n","    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n","])\n"]},{"cell_type":"code","source":["import glob, torch, os\n","\n","def latest_ckpt(dirpath, pattern=None):\n","    \"\"\"\n","    If you pass pattern=None, we'll look for anything matching\n","    *_last_ckpt_round_*.pth and return the numerically latest file.\n","    \"\"\"\n","    if pattern is None:\n","        pattern = \"*_last_ckpt_round_*.pth\"\n","    paths = glob.glob(os.path.join(dirpath, pattern))\n","    if not paths:\n","        return None\n","    # Extract the round number from each filename, assuming it ends in _<round>.pth\n","    def round_num(p):\n","        base = os.path.basename(p)\n","        # splits on underscores: shard_J{J}_last_ckpt_round_{rnd}.pth\n","        return int(base.rsplit(\"_\",1)[1].split(\".\")[0])\n","    paths.sort(key=round_num)\n","    return paths[-1]\n","\n","\n","def load_checkpoint(model, optimizer, ckpt_dir, shard_key, J, resume=True):\n","    if not resume:\n","        return 1\n","    pat = f\"{shard_key}_J{J}_last_ckpt_round_*.pth\"\n","    ckpt_path = latest_ckpt(ckpt_dir, pat)\n","    if ckpt_path is None:\n","        return 1\n","    ckpt = torch.load(ckpt_path, map_location='cpu')\n","    model.load_state_dict(ckpt['model_state'])\n","    optimizer.load_state_dict(ckpt['optimizer_state'])\n","    rng_state = ckpt['rng_state']\n","    if rng_state.device.type != 'cpu':\n","        rng_state = rng_state.cpu()\n","    torch.set_rng_state(rng_state)\n","    return ckpt['round'] + 1\n","\n","\n","\n","def save_checkpoint(model, optimizer, round_num, ckpt_dir,\n","                    shard_key, J, is_best=False):\n","    \"\"\"\n","    Saves:\n","      <shard_key>_J<J>_last_ckpt_round_<round_num>.pth\n","    and if is_best=True, also\n","      <shard_key>_J<J>_best_ckpt.pth\n","    \"\"\"\n","    prefix = f\"{shard_key}_J{J}\"\n","    state = {\n","        'round': round_num,\n","        'model_state': model.state_dict(),\n","        'optimizer_state': optimizer.state_dict(),\n","        'rng_state': torch.get_rng_state(),\n","    }\n","    # unique filename per config\n","    last_name = f\"{prefix}_last_ckpt_round_{round_num}.pth\"\n","    torch.save(state, os.path.join(ckpt_dir, last_name))\n","    if is_best:\n","        best_name = f\"{prefix}_best_ckpt.pth\"\n","        torch.save(model.state_dict(), os.path.join(ckpt_dir, best_name))\n","    print(f\"[Checkpoint] Saved {last_name}\")\n"],"metadata":{"id":"bF8TWhdImubH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":88040,"status":"ok","timestamp":1754333818733,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"},"user_tz":-120},"id":"1KsmNAdjdWqH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c4e1e3c-a188-4dde-e4b7-305d35a7102e"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:06<00:00, 24.3MB/s]\n"]}],"source":["# ── C. Data Preparation ─────────────────────────────────────────────────────────\n","# Transforms (as before)…\n","\n","# Download full CIFAR‑100 training set\n","full_train = torchvision.datasets.CIFAR100(\n","    root='./data', train=True, download=True, transform=transform_train\n",")\n","\n","# 1) Centralized validation split\n","val_size   = 5000\n","train_size = len(full_train) - val_size\n","train_dataset, val_dataset = random_split(\n","    full_train,\n","    [train_size, val_size],\n","    generator=torch.Generator().manual_seed(seed)\n",")\n","\n","# ── C.1 Build validation loader ───────────────────────────────\n","BS_VAL = 256\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=BS_VAL,\n","    shuffle=False,\n","    num_workers=2\n",")\n","\n","# ── C.2 Non-IID Sharding Helper ────────────────────────────────────────────────\n","\n","from collections import defaultdict\n","from torch.utils.data import Subset\n","\n","def create_labelwise_shards(dataset, K, Nc, seed=42):\n","    # 1) Group indices by label\n","    label2idx = defaultdict(list)\n","    for idx, (_, lbl) in enumerate(dataset):\n","        label2idx[lbl].append(idx)\n","\n","    # 2) Shuffle each label’s pool\n","    rng = random.Random(seed)\n","    for lbl in label2idx:\n","        rng.shuffle(label2idx[lbl])\n","\n","    # 3) Prepare an iterator per label\n","    pointers = {lbl: 0 for lbl in label2idx}\n","\n","    # 4) Build shards\n","    samples_per_client = len(dataset) // K\n","    shards_idx = []\n","    labels_cycle = list(label2idx.keys())\n","\n","    for client_id in range(K):\n","        client_idxs = []\n","        # Rotate start point so clients don’t always pick the same first label\n","        rng.shuffle(labels_cycle)\n","        for lbl in labels_cycle:\n","            if len(client_idxs) >= samples_per_client:\n","                break\n","            # How many to take from this label\n","            needed = samples_per_client - len(client_idxs)\n","            available = len(label2idx[lbl]) - pointers[lbl]\n","            take = min(needed, available)\n","            if take > 0:\n","                start = pointers[lbl]\n","                end   = start + take\n","                client_idxs.extend(label2idx[lbl][start:end])\n","                pointers[lbl] += take\n","        # If we still haven’t reached samples_per_client (rare), fill randomly\n","        if len(client_idxs) < samples_per_client:\n","            all_remaining = [i for lbl in label2idx\n","                                 for i in label2idx[lbl][pointers[lbl]:]]\n","            client_idxs.extend(rng.sample(all_remaining,\n","                                          samples_per_client - len(client_idxs)))\n","        shards_idx.append(client_idxs)\n","\n","    return [Subset(dataset, idxs) for idxs in shards_idx]\n","\n","\n","\n","# ── C.3 Build All Shardings ────────────────────────────────────────────────────\n","K       = 100\n","base  = train_size // K\n","\n","sizes   = [base] * (K - 1) + [train_size - base * (K - 1)]\n","iid_shards = random_split(\n","    train_dataset, sizes,\n","    generator=torch.Generator().manual_seed(seed)\n",")\n","\n","# Non-IID for Nc in {1,5,10,50}\n","Nc_list      = [1, 5, 10, 50]\n","shardings    = {'iid': iid_shards}\n","for Nc in Nc_list:\n","    shardings[f'non_iid_{Nc}'] = create_labelwise_shards(\n","        train_dataset, K=K, Nc=Nc, seed=seed\n","    )\n","\n","# Now `shardings` is a dict mapping:\n","#   'iid'          → list of 100 Subsets (IID)\n","#   'non_iid_1'    → 100 shards each with 1 label\n","#   'non_iid_5'    → 100 shards each with 5 labels\n","#   etc.\n","\n","# ── C.4 Per-Client DataLoaders ────────────────────────────────────────────────\n","# (You can build these on-the-fly inside your training loops;\n","#  or precompute for each sharding if memory allows.)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zukJAc0dWq0"},"outputs":[],"source":["# ── D. Model Definition ─────────────────────────────────────────────────────────\n","class LELeNetCIFAR(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n","        self.pool1 = nn.MaxPool2d(2)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n","        self.pool2 = nn.MaxPool2d(2)\n","        self.fc1   = nn.Linear(64*8*8, 384)\n","        self.fc2   = nn.Linear(384, 192)\n","        self.fc3   = nn.Linear(192, 100)\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x)))\n","        x = self.pool2(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x)); x = F.relu(self.fc2(x))\n","        return self.fc3(x)"]},{"cell_type":"code","source":["# ── E. Utilities: Train/Eval & Checkpointing ────────────────────────────────────\n","def train_one_epoch(model, optimizer, criterion, loader):\n","    model.train()\n","    running_loss = correct = total = 0\n","    for imgs, lbls in loader:\n","        imgs, lbls = imgs.to(device), lbls.to(device)\n","        optimizer.zero_grad()\n","        out = model(imgs)\n","        loss = criterion(out, lbls)\n","        loss.backward(); optimizer.step()\n","        running_loss += loss.item()*imgs.size(0)\n","        correct += out.argmax(1).eq(lbls).sum().item()\n","        total += lbls.size(0)\n","    return running_loss/total, correct/total\n","\n","def eval_model(model, criterion, loader):\n","    model.eval()\n","    running_loss = correct = total = 0\n","    with torch.no_grad():\n","        for imgs, lbls in loader:\n","            imgs, lbls = imgs.to(device), lbls.to(device)\n","            out = model(imgs); loss = criterion(out, lbls)\n","            running_loss += loss.item()*imgs.size(0)\n","            correct += out.argmax(1).eq(lbls).sum().item()\n","            total += lbls.size(0)\n","    return running_loss/total, correct/total\n","\n","\n","def sample_clients_dirichlet(K, m, gamma, rng):\n","    \"\"\"\n","    Sample m out of K client indices, with probabilities:\n","      • uniform if gamma == 'uniform'\n","      • drawn from Dirichlet([gamma]*K) otherwise.\n","    Returns:\n","      selected: list of m client indices\n","      p:        length-K numpy array of sampling probs (sums to 1)\n","    \"\"\"\n","    if gamma == 'uniform':\n","        p = np.ones(K) / K\n","    else:\n","        alpha = np.ones(K) * gamma\n","        p     = rng.dirichlet(alpha)\n","    selected = rng.choice(K, size=m, replace=False, p=p)\n","    return selected.tolist(), p\n","\n"],"metadata":{"id":"bnOGpNdtw-X0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx5SLtcQggbL"},"outputs":[],"source":["# ── Configuration Summary & Utilities for FedAvg ──────────────────────────────\n","\n","import os, sys, platform, time\n","import torch\n","import numpy as np\n","import random\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","\n","# 3) Hyperparameters\n","K, C = 100, 0.1\n","BS, BS_VAL = 128, 256\n","LR, WD = 0.01, 1e-4\n","J0, R0   = 4, 2000\n","budget   = J0 * R0\n","J_list   = [4, 8, 16]\n","\n","\n","\n","# 1) Define and instantiate your TensorBoard writer\n","log_dir = f\"./logs/FedAvg_lr{LR}_wd{WD}_bs{BS}\"\n","tb_writer = SummaryWriter(log_dir=log_dir)\n","\n","# 2) Summary utility\n","def summarize_run(cfg, client_loaders, test_loader, writer=None):\n","    \"\"\"\n","    Print and log summary for a FedAvg run.\n","    \"\"\"\n","    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n","    print(f\"\\n========== FEDAVG RUN SUMMARY ({ts}) ==========\")\n","    # Hyperparameters\n","    for key in ['lr','weight_decay','batch_size','K','C','J','ROUNDS']:\n","        print(f\"    • {key}: {cfg[key]}\")\n","    # Data info\n","    num_clients  = len(client_loaders)\n","    shard_size   = len(client_loaders[0].dataset)\n","    test_samples = len(test_loader.dataset)\n","    print(f\"    • clients (K): {num_clients}, shard size: {shard_size}\")\n","    print(f\"    • test samples: {test_samples}, batch size: {cfg['batch_size']}\")\n","    # Log to TensorBoard\n","    if writer:\n","        for key in ['lr','weight_decay','batch_size','K','C','J','ROUNDS']:\n","            writer.add_text(f'RunInfo/{key}', str(cfg[key]), 0)\n","\n","# 3) Checkpoint utility\n","ckpt_dir = './checkpoints'\n","os.makedirs(ckpt_dir, exist_ok=True)\n","\n","\n","# ── Example Usage ────────────────────────────────────────────────────────────────\n","\n","cfg = {\n","    'lr':           LR,\n","    'weight_decay': WD,\n","    'batch_size':   BS,\n","    'K':            K,\n","    'C':            C,\n","    'J0':            J0,\n","    'ROUNDS':       R0\n","\n","}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"QAGLQObvdWs7","outputId":"a75d6736-762e-4193-d135-1f967a48c378","executionInfo":{"status":"error","timestamp":1754333892033,"user_tz":-120,"elapsed":73287,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}}},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-490108315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Point to a folder inside your Drive for persistent checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mCKPT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/fl_checkpoints'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["\n","# ── A. Mount Google Drive ─────────────────────────────────────────────────────\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","# Point to a folder inside your Drive for persistent checkpoints\n","CKPT_DIR = '/content/drive/MyDrive/fl_checkpoints'\n","os.makedirs(CKPT_DIR, exist_ok=True)\n","\n","\n","\n","\n","# Set this to True to resume from the last checkpoint; False to start from scratch\n","RESUME = True\n","\n","\n","\n","# 3) Hyperparameters\n","K, C = 100, 0.1\n","BS, BS_VAL = 128, 256\n","LR, WD = 0.01, 1e-4\n","J0, R0   = 4, 2000\n","budget   = J0 * R0\n","J_list   = [4, 8, 16]\n","\n","# 4) Transforms & Data\n","transform_train = T.Compose([\n","    T.RandomCrop(32, 4), T.RandomHorizontalFlip(),\n","    T.ToTensor(),\n","    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n","])\n","transform_test = T.Compose([\n","    T.ToTensor(),\n","    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n","])\n","\n","full_train = torchvision.datasets.CIFAR100(\n","    './data', train=True, download=True, transform=transform_train\n",")\n","val_size   = 5000\n","train_size = len(full_train) - val_size\n","train_dataset, val_dataset = random_split(\n","    full_train, [train_size, val_size],\n","    generator=torch.Generator().manual_seed(seed)\n",")\n","val_loader = DataLoader(val_dataset, batch_size=BS_VAL, shuffle=False, num_workers=2)\n","\n","test_dataset = torchvision.datasets.CIFAR100(\n","    './data', train=False, download=True, transform=transform_test\n",")\n","test_loader = DataLoader(test_dataset, batch_size=BS_VAL, shuffle=False, num_workers=2)\n","\n","\n","\n","# ── Instantiate TensorBoard writer ──────────────────────────────────────────────\n","from torch.utils.tensorboard import SummaryWriter\n","log_dir   = f\"./logs/FedAvg_lr{LR}_wd{WD}_bs{BS}\"\n","tb_writer = SummaryWriter(log_dir=log_dir)\n","\n","# ── B. CSV Logging Setup ────────────────────────────────────────────────────────\n","import csv, os\n","csv_path = './fedavg_results.csv'\n","\n","\n","\n","# ── B. CSV Logging Setup ────────────────────────────────────────────────────────\n","import csv, os\n","csv_path = './fedavg_results.csv'\n","if not os.path.exists(csv_path):\n","    with open(csv_path, 'w', newline='') as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\n","            'round',\n","            'val_loss', 'val_acc',\n","            'test_loss', 'test_acc'\n","        ])\n","\n","# Before your FedAvg loop: Instantiate the global model, loss, and client loaders once\n","\n","\n","# 6) Build all shardings\n","base = train_size // K\n","sizes = [base]*(K-1) + [train_size-base*(K-1)]\n","iid_shards = random_split(train_dataset, sizes,\n","                          generator=torch.Generator().manual_seed(seed))\n","\n","Nc_list   = [1, 5, 10, 50]\n","shardings = {'iid': iid_shards}\n","for Nc in Nc_list:\n","    shardings[f'non_iid_{Nc}'] = create_labelwise_shards(\n","        train_dataset, K, Nc, seed\n","    )\n","\n","cfg = {'lr': LR, 'weight_decay': WD, 'batch_size': BS, 'K': K, 'C': C}\n","\n","\n","\n","\n","\n","rng = np.random.default_rng(seed)\n","\n","\n","\n","\n","\n","# ── C. FedAvg Training Loop ─────────────────────────────────────────────────────\n","# ── F. FedAvg Experiment Loop with Drive Checkpoints ──────────────────────────\n","# Assumes `shardings`, `J_list`, `budget`, `load_checkpoint`, `save_checkpoint`, etc. are defined\n","\n","# ── D: Training Loop ────────────────────────────────────────────────────────────\n","all_results = {}\n","\n","for shard_key, shards in shardings.items():\n","    client_loaders = [\n","        DataLoader(shards[i], batch_size=BS, shuffle=True, num_workers=2)\n","        for i in range(K)\n","    ]\n","\n","    for J in J_list:\n","        ROUNDS_scaled = budget // J\n","        cfg.update({'J': J, 'ROUNDS': ROUNDS_scaled})\n","        summarize_run(cfg, client_loaders, test_loader, writer=tb_writer)\n","\n","        # Initialize model & optimizer, then resume if any\n","        model     = LELeNetCIFAR().to(device)\n","        optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=WD)\n","        criterion = nn.CrossEntropyLoss()\n","        start_round = load_checkpoint(\n","            model, optimizer,\n","            ckpt_dir=CKPT_DIR,\n","            shard_key=shard_key, J=J,\n","            resume=RESUME\n","        )\n","\n","        acc_hist = []\n","        print(f\"[{shard_key}|J={J}] Starting from round {start_round}/{ROUNDS_scaled}\")\n","\n","        for rnd in range(start_round, ROUNDS_scaled + 1):\n","            m = max(1, int(C * K))\n","            selected, _ = sample_clients_dirichlet(K, m, gamma='uniform', rng=rng)\n","\n","            # Local updates\n","            local_accuracies = []\n","            local_states, sizes = [], []\n","            for cid in selected:\n","                cm    = copy.deepcopy(model)\n","                opt_c = optim.SGD(cm.parameters(), lr=LR, momentum=0.9, weight_decay=WD)\n","                for _ in range(J):\n","                    train_loss, train_acc = train_one_epoch(cm, opt_c, criterion, client_loaders[cid])\n","                local_states.append(cm.state_dict())\n","                sizes.append(len(shards[cid]))\n","                local_accuracies.append(train_acc)\n","\n","\n","            # Compute local mean & std\n","            local_mean = np.mean(local_accuracies)\n","            local_std  = np.std(local_accuracies)\n","\n","            # Aggregate\n","            total = sum(sizes)\n","            new_st = {\n","                k: sum((sizes[i] / total) * local_states[i][k] for i in range(len(sizes)))\n","                for k in model.state_dict()\n","            }\n","            model.load_state_dict(new_st)\n","\n","            # Eval\n","            _, val_acc = eval_model(model, criterion, val_loader)\n","            acc_hist.append(val_acc)\n","\n","            # 4) Evaluation\n","            val_loss, val_acc   = eval_model(model, criterion, val_loader)\n","            test_loss, test_acc = eval_model(model, criterion, test_loader)\n","            acc_hist.append(test_acc)\n","\n","            # 5) Timing\n","            round_time = time.time() - round_start\n","\n","\n","\n","            # Logging\n","            # 6) CSV Logging\n","            with open(csv_path, 'a', newline='') as f:\n","                csv.writer(f).writerow([\n","                    shard_key, J, rnd,\n","                    f\"{round_time:.2f}\",\n","                    f\"{local_mean:.4f}\", f\"{local_std:.4f}\",\n","                    f\"{val_loss:.4f}\", f\"{val_acc:.4f}\",\n","                    f\"{test_loss:.4f}\", f\"{test_acc:.4f}\"\n","                ])\n","\n","            # 7) TensorBoard Logging\n","            tb_writer.add_scalar(f\"{shard_key}/J{J}_round_time\", round_time, rnd)\n","            tb_writer.add_scalar(f\"{shard_key}/J{J}_local_acc_mean\", local_mean, rnd)\n","            tb_writer.add_scalar(f\"{shard_key}/J{J}_local_acc_std\",  local_std,  rnd)\n","            tb_writer.add_scalar(f\"{shard_key}/J{J}_val_loss\",       val_loss,    rnd)\n","            tb_writer.add_scalar(f\"{shard_key}/J{J}_val_acc\",        val_acc,     rnd)\n","            tb_writer.add_scalar(f\"{shard_key}/J{J}_test_loss\",      test_loss,   rnd)\n","            tb_writer.add_scalar(f\"{shard_key}/J{J}_test_acc\",       test_acc,    rnd)\n","\n","            # 8) Print per-round summary\n","            print(\n","                f\"[{shard_key} | J={J}] Round {rnd}/{ROUNDS_scaled} \"\n","                f\"Time={round_time:.1f}s | \"\n","                f\"Local Acc={local_mean:.3f}±{local_std:.3f} | \"\n","                f\"Val Acc={val_acc:.3f} | Test Acc={test_acc:.3f}\"\n","            )\n","\n","\n","\n","            # Checkpoint every 20 rounds (or at first)\n","            if rnd == start_round or rnd % 20 == 0:\n","              # 1) Log a header with the current config and round\n","              print(f\"[{shard_key} | J={J}] Checkpointing at round {rnd}/{ROUNDS_scaled}\")\n","\n","              # 2) Print the latest metrics\n","              print(f\"    → Last Val Acc: {val_acc:.4f} | Last Val Loss: {val_loss:.4f}\")\n","              print(f\"    → Last Test Acc: {test_acc:.4f} | Last Test Loss: {test_loss:.4f}\")\n","\n","              # 3) Save the checkpoint\n","              save_checkpoint(\n","                  model, optimizer,\n","                  round_num=rnd,\n","                  ckpt_dir=CKPT_DIR,\n","                  shard_key=shard_key, J=J,\n","                  is_best=False\n","              )\n","              print(f\"[{shard_key} | J={J}] Checkpoint saved.\\n\")\n","\n","\n","        all_results[(shard_key, J)] = np.array(acc_hist)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMYuWSfTdWtt"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# 1) Load results\n","df = pd.read_csv('./fedavg_results.csv', names=['shard','J','round','val_acc'], header=0)\n","\n","# 2) Pivot final validation accuracy per (shard,J)\n","final = df.groupby(['shard','J']).last().reset_index()\n","pivot = final.pivot(index='shard', columns='J', values='val_acc')\n","print(\"Final Validation Accuracy:\")\n","display(pivot)\n","\n","# 3) Plot convergence curves for each (shard, J)\n","plt.figure(figsize=(10,6))\n","for (shard, J), group in df.groupby(['shard','J']):\n","    plt.plot(group['round'], group['val_acc'], label=f\"{shard}, J={J}\")\n","plt.xlabel('Federated Round')\n","plt.ylabel('Validation Accuracy')\n","plt.title('FedAvg Convergence: IID vs Non-IID, varying J')\n","plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n","plt.tight_layout()\n","plt.show()\n","\n","# 4) (Optional) Rolling average smoothing\n","plt.figure(figsize=(10,6))\n","for (shard, J), group in df.groupby(['shard','J']):\n","    sm = group['val_acc'].rolling(window=50, min_periods=1).mean()\n","    plt.plot(group['round'], sm, label=f\"{shard}, J={J}\")\n","plt.xlabel('Federated Round')\n","plt.ylabel('Smoothed Validation Acc')\n","plt.title('Smoothed Convergence Curves')\n","plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n","plt.tight_layout()\n","plt.show()\n","\n","# 5) Plot data‐distribution boxplots (to verify your non-IID splits)\n","from collections import Counter\n","\n","for key, shards in shardings.items():\n","    # count labels per client\n","    counts = [Counter([full_train[i][1] for i in subset]) for subset in shards]\n","    # convert to list of lists for boxplot\n","    per_class_counts = []\n","    for lbl in range(100):\n","        per_class_counts.append([c[lbl] for c in counts])\n","    plt.figure(figsize=(8,4))\n","    plt.boxplot(per_class_counts, whis=(5,95), showfliers=False)\n","    plt.title(f\"Label counts per client — {key}\")\n","    plt.xlabel(\"Class label\")\n","    plt.ylabel(\"Examples per client\")\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZm-lbwPdWvT"},"outputs":[],"source":[]},{"cell_type":"code","source":["#ATTENZIONE!!!!\n","#OCCHIO A RUNNAREEEE\n","\n","\n","\n","#utility function that deletes all checkpoints from the checkpoint folder\n","\n","def clear_checkpoints(ckpt_dir):\n","    \"\"\"\n","    Remove all checkpoint files in the specified dire ctory.\n","    \"\"\"\n","    removed = 0\n","    for fname in os.listdir(ckpt_dir):\n","        path = os.path.join(ckpt_dir, fname)\n","        if os.path.isfile(path):\n","            os.remove(path)\n","            removed += 1\n","    print(f\"[Checkpoint] Cleared {removed} files from {ckpt_dir}\")\n","\n","\n","clear_checkpoints(CKPT_DIR)\n"],"metadata":{"id":"00NdM_dr6vim"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Owzk--gFdWwB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5IgnGhEdWw1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1O3ub6UdWxr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1TE0jsvf4PTX7tN0Ngz_0NZfm8NoGgDgJ","timestamp":1753224018851},{"file_id":"1KYtrSgZWEprGxEteMTSH-NNBpr56IO0Y","timestamp":1753205697977}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}