{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0225dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved accuracy-vs-rounds plots: {'Nc=1': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_1.png', 'Nc=5': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_5.png', 'Nc=10': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_10.png', 'Nc=50': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_50.png', 'iid': 'figs\\\\auto_plots\\\\acc_vs_rounds_iid.png'}\n",
      "Saved final-accuracy-vs-J plot: figs\\auto_plots\\final_acc_vs_J.png\n",
      "Exported final per-run and aggregated CSVs to: figs\\auto_plots\\final_metrics_per_run.csv\n",
      "All done. Summary: {'Nc=1': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_1.png', 'Nc=5': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_5.png', 'Nc=10': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_10.png', 'Nc=50': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_50.png', 'iid': 'figs\\\\auto_plots\\\\acc_vs_rounds_iid.png', 'final_acc_vs_J': 'figs\\\\auto_plots\\\\final_acc_vs_J.png', 'final_metrics_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run.csv', 'aggregated_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_aggregated.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\AppData\\Local\\Temp\\ipykernel_30524\\4182015453.py:206: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  vals.append(float(row['mean']))\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp\\ipykernel_30524\\4182015453.py:207: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  errs.append(float(row['std']) if not np.isnan(row['std'].iloc[0]) else 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Nc=1': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_1.png',\n",
       " 'Nc=5': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_5.png',\n",
       " 'Nc=10': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_10.png',\n",
       " 'Nc=50': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_50.png',\n",
       " 'iid': 'figs\\\\auto_plots\\\\acc_vs_rounds_iid.png',\n",
       " 'final_acc_vs_J': 'figs\\\\auto_plots\\\\final_acc_vs_J.png',\n",
       " 'final_metrics_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run.csv',\n",
       " 'aggregated_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_aggregated.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture\n",
    "# Paste this cell in your notebook. It contains plotting utilities.\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Union, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 150, \"axes.grid\": True})\n",
    "\n",
    "# -------------------------\n",
    "# Helper parsing functions\n",
    "# -------------------------\n",
    "def parse_run_name(name: str) -> Tuple[str, Union[int,str]]:\n",
    "    \"\"\"\n",
    "    Parse a run name like:\n",
    "      'test_sampled_uniform_1_J_4' -> shard_key='sampled_uniform_1', J=4\n",
    "      'test_iid_J_4'              -> shard_key='iid', J=4\n",
    "      'test_non_iid_10_J_8'      -> shard_key='non_iid_10', J=8\n",
    "    Returns (shard_key, J)\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return (\"unknown\", -1)\n",
    "    m = re.match(r'^(?:test_)?(.+)_J_(\\d+)$', name)\n",
    "    if m:\n",
    "        shard_key = m.group(1)\n",
    "        J = int(m.group(2))\n",
    "        return shard_key, J\n",
    "    # fallback: try to find _J_ anywhere\n",
    "    m2 = re.search(r'_J_(\\d+)', name)\n",
    "    if m2:\n",
    "        J = int(m2.group(1))\n",
    "        # try to extract shard_key before _J_\n",
    "        before = name.split('_J_')[0]\n",
    "        if before.startswith(\"test_\"):\n",
    "            shard_key = before[len(\"test_\"):]\n",
    "        else:\n",
    "            shard_key = before\n",
    "        return shard_key, J\n",
    "    return (name, -1)\n",
    "\n",
    "def shardkey_to_Nc_label(shard_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert shard_key to a compact Nc label:\n",
    "      'sampled_uniform_1' -> 'Nc=1'\n",
    "      'sampled_uniform_50' -> 'Nc=50'\n",
    "      'iid' -> 'iid'\n",
    "      fallback returns shard_key\n",
    "    \"\"\"\n",
    "    if shard_key is None:\n",
    "        return 'unknown'\n",
    "    if shard_key.lower().startswith('iid'):\n",
    "        return 'iid'\n",
    "    # try to find trailing number\n",
    "    m = re.search(r'_(\\d+)$', shard_key)\n",
    "    if m:\n",
    "        return f\"Nc={m.group(1)}\"\n",
    "    # try internal number\n",
    "    m2 = re.search(r'(\\d+)', shard_key)\n",
    "    if m2:\n",
    "        return f\"Nc={m2.group(1)}\"\n",
    "    return shard_key\n",
    "\n",
    "# -------------------------\n",
    "# Loading & preprocessing\n",
    "# -------------------------\n",
    "def load_experiment_csv(csv_path: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV and add parsed columns:\n",
    "      - 'shard_key'  (e.g., sampled_uniform_1 or iid)\n",
    "      - 'Nc_label'   (e.g., Nc=1 or iid)\n",
    "      - 'J'          (int)\n",
    "    Returns dataframe with these columns appended.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # normalize column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if 'name' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain column 'name' (run identifier).\")\n",
    "    # parse\n",
    "    parsed = df['name'].apply(parse_run_name)\n",
    "    df['shard_key'] = parsed.apply(lambda x: x[0])\n",
    "    df['J'] = parsed.apply(lambda x: x[1])\n",
    "    df['Nc_label'] = df['shard_key'].apply(shardkey_to_Nc_label)\n",
    "    # ensure numeric columns are floats\n",
    "    for col in ['round','val_loss','val_acc','test_loss','test_acc','train_loss','train_acc','local_train_mean','local_train_std']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Aggregation helpers\n",
    "# -------------------------\n",
    "def final_metrics_per_run(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For every unique 'name' (run) return the last-round metrics.\n",
    "    Returns DataFrame indexed by 'name' with columns including 'test_acc', 'J', 'Nc_label', 'shard_key', 'round'\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    for name, group in df.groupby('name'):\n",
    "        # pick the row with max round (if round exists), else last row\n",
    "        if 'round' in group.columns and not group['round'].isna().all():\n",
    "            row = group.loc[group['round'].idxmax()]\n",
    "        else:\n",
    "            row = group.iloc[-1]\n",
    "        d = {\n",
    "            'name': name,\n",
    "            'shard_key': row.get('shard_key'),\n",
    "            'Nc_label': row.get('Nc_label'),\n",
    "            'J': int(row.get('J', -1)),\n",
    "            'final_round': row.get('round'),\n",
    "            'test_acc': row.get('test_acc'),\n",
    "            'val_acc': row.get('val_acc'),\n",
    "            'test_loss': row.get('test_loss'),\n",
    "            'val_loss': row.get('val_loss')\n",
    "        }\n",
    "        out_rows.append(d)\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    return out_df\n",
    "\n",
    "# -------------------------\n",
    "# Plotting / Reporting\n",
    "# -------------------------\n",
    "def ensure_outdir(out_dir: Union[str, Path]):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def plot_accuracy_vs_rounds(df: pd.DataFrame, out_dir: str,\n",
    "                            nc_order: Optional[Sequence[str]] = None,\n",
    "                            J_list: Optional[Sequence[int]] = (4,8,16),\n",
    "                            smoothing: int = 1):\n",
    "    \"\"\"\n",
    "    Plot test accuracy vs rounds grouped by Nc label.\n",
    "    - df: loaded DataFrame from load_experiment_csv\n",
    "    - out_dir: directory to save PNGs\n",
    "    - nc_order: list of Nc_label strings to iterate in order, e.g. ['Nc=1','Nc=5','Nc=10','Nc=50','iid']\n",
    "    - smoothing: optional moving average kernel (integer)\n",
    "    \"\"\"\n",
    "    ensure_outdir(out_dir)\n",
    "    if nc_order is None:\n",
    "        nc_order = sorted(df['Nc_label'].unique(), key=lambda x: (x!='iid', x))\n",
    "    summary = {}\n",
    "    for nc in nc_order:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        ax = plt.gca()\n",
    "        subset = df[df['Nc_label']==nc]\n",
    "        if subset.empty:\n",
    "            print(f\"Warning: no data for {nc}, skipping.\")\n",
    "            continue\n",
    "        for J in J_list:\n",
    "            sel = subset[subset['J']==J]\n",
    "            if sel.empty:\n",
    "                continue\n",
    "            # group across runs (names) by round -> mean/std\n",
    "            grouped = sel.groupby('round')['test_acc'].agg(['mean','std','count']).reset_index()\n",
    "            if grouped.empty:\n",
    "                continue\n",
    "            x = grouped['round'].values\n",
    "            y = grouped['mean'].values\n",
    "            if smoothing > 1:\n",
    "                # simple moving average\n",
    "                kernel = np.ones(smoothing) / smoothing\n",
    "                y = np.convolve(y, kernel, mode='same')\n",
    "            ax.plot(x, y, label=f\"J={J}\")\n",
    "            # shaded std if available\n",
    "            if 'std' in grouped.columns and not grouped['std'].isna().all():\n",
    "                std = grouped['std'].values\n",
    "                ax.fill_between(x, y-std, y+std, alpha=0.2)\n",
    "        ax.set_title(f\"Test accuracy vs rounds — {nc}\")\n",
    "        ax.set_xlabel(\"Round\")\n",
    "        ax.set_ylabel(\"Test accuracy\")\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "        plt.tight_layout()\n",
    "        out_file = Path(out_dir)/f\"acc_vs_rounds_{nc.replace('=','_')}.png\"\n",
    "        plt.savefig(out_file)\n",
    "        plt.close()\n",
    "        summary[nc] = str(out_file)\n",
    "    print(\"Saved accuracy-vs-rounds plots:\", summary)\n",
    "    return summary\n",
    "\n",
    "def plot_final_acc_vs_J(df: pd.DataFrame, out_dir: str,\n",
    "                        nc_order: Optional[Sequence[str]] = None,\n",
    "                        J_list: Optional[Sequence[int]] = (4,8,16)):\n",
    "    \"\"\"\n",
    "    Bar plot showing final test accuracy (mean ± std) vs J for each Nc.\n",
    "    \"\"\"\n",
    "    ensure_outdir(out_dir)\n",
    "    final = final_metrics_per_run(df)\n",
    "    # aggregate per (Nc_label, J)\n",
    "    agg = final.groupby(['Nc_label','J'])['test_acc'].agg(['mean','std','count']).reset_index()\n",
    "    if nc_order is None:\n",
    "        nc_order = sorted(agg['Nc_label'].unique(), key=lambda x: (x!='iid', x))\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    width = 0.15\n",
    "    x_positions = np.arange(len(nc_order))\n",
    "    offsets = np.linspace(-width, width, num=len(J_list))\n",
    "    for i, J in enumerate(J_list):\n",
    "        vals = []\n",
    "        errs = []\n",
    "        for nc in nc_order:\n",
    "            row = agg[(agg['Nc_label']==nc) & (agg['J']==J)]\n",
    "            if not row.empty:\n",
    "                vals.append(float(row['mean']))\n",
    "                errs.append(float(row['std']) if not np.isnan(row['std'].iloc[0]) else 0.0)\n",
    "            else:\n",
    "                vals.append(0.0)\n",
    "                errs.append(0.0)\n",
    "        ax.bar(x_positions + offsets[i], vals, width=width, yerr=errs, label=f\"J={J}\", capsize=3)\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(nc_order)\n",
    "    ax.set_ylim(0,1.0)\n",
    "    ax.set_ylabel(\"Final test accuracy (mean ± std)\")\n",
    "    ax.set_title(\"Final test accuracy vs J for each Nc\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    out_file = Path(out_dir)/\"final_acc_vs_J.png\"\n",
    "    plt.savefig(out_file)\n",
    "    plt.close()\n",
    "    print(\"Saved final-accuracy-vs-J plot:\", out_file)\n",
    "    return str(out_file)\n",
    "\n",
    "def export_final_metrics_csv(df: pd.DataFrame, out_path: Union[str, Path]):\n",
    "    \"\"\"\n",
    "    Export a CSV with final metrics per run and aggregated stats per (Nc,J).\n",
    "    \"\"\"\n",
    "    final = final_metrics_per_run(df)\n",
    "    # save per-run final\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    final.to_csv(out_path, index=False)\n",
    "    # also save aggregated summary\n",
    "    agg = final.groupby(['Nc_label','J'])['test_acc'].agg(['mean','std','count']).reset_index()\n",
    "    agg.to_csv(out_path.parent / (out_path.stem + \"_aggregated.csv\"), index=False)\n",
    "    print(\"Exported final per-run and aggregated CSVs to:\", out_path)\n",
    "    return final, agg\n",
    "\n",
    "# -------------------------\n",
    "# Optional: heatmaps / per-client plots (if you have per-client arrays)\n",
    "# -------------------------\n",
    "def plot_shard_heatmap(client_class_counts: np.ndarray,\n",
    "                       out_file: Union[str, Path],\n",
    "                       client_labels: Optional[Sequence[str]] = None,\n",
    "                       class_labels: Optional[Sequence[str]] = None,\n",
    "                       cmap: str = \"viridis\"):\n",
    "    \"\"\"\n",
    "    client_class_counts: 2D array shape (n_clients, n_classes) with counts.\n",
    "    \"\"\"\n",
    "    ensure_outdir(Path(out_file).parent)\n",
    "    counts = np.array(client_class_counts)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(counts, aspect='auto', interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar(label=\"sample count\")\n",
    "    if client_labels is not None:\n",
    "        # too many ticks -> subsample\n",
    "        plt.yticks(np.linspace(0, counts.shape[0]-1, min(10, counts.shape[0])).astype(int),\n",
    "                   [client_labels[i] for i in np.linspace(0, counts.shape[0]-1, min(10, counts.shape[0])).astype(int)])\n",
    "    if class_labels is not None:\n",
    "        plt.xticks(np.linspace(0, counts.shape[1]-1, min(10, counts.shape[1])).astype(int),\n",
    "                   [class_labels[i] for i in np.linspace(0, counts.shape[1]-1, min(10, counts.shape[1])).astype(int)],\n",
    "                   rotation=45, ha='right')\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Client\")\n",
    "    plt.title(\"Client × Class sample counts\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file)\n",
    "    plt.close()\n",
    "    print(\"Saved shard heatmap to\", out_file)\n",
    "    return str(out_file)\n",
    "\n",
    "def plot_per_client_accuracy_hist(per_client_acc: np.ndarray, out_file: Union[str, Path],\n",
    "                                  bins: int = 20):\n",
    "    ensure_outdir(Path(out_file).parent)\n",
    "    arr = np.array(per_client_acc)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(arr, bins=bins, edgecolor='k')\n",
    "    plt.xlabel(\"Per-client test accuracy\")\n",
    "    plt.ylabel(\"Number of clients\")\n",
    "    plt.title(\"Per-client accuracy distribution (final round)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file)\n",
    "    plt.close()\n",
    "    print(\"Saved per-client histogram to\", out_file)\n",
    "    return str(out_file)\n",
    "\n",
    "# -------------------------\n",
    "# Orchestrator: produce all standard plots\n",
    "# -------------------------\n",
    "def generate_all_plots(csv_path: str, out_dir: str,\n",
    "                       nc_order: Optional[Sequence[str]] = None,\n",
    "                       J_list: Optional[Sequence[int]] = (4,8,16),\n",
    "                       smoothing: int = 1):\n",
    "    \"\"\"\n",
    "    Top-level convenience function to load the CSV and produce:\n",
    "      - accuracy vs rounds per Nc (one file per Nc)\n",
    "      - final accuracy vs J (single panel)\n",
    "      - export final metrics CSVs\n",
    "    Returns a dict of saved file paths.\n",
    "    \"\"\"\n",
    "    df = load_experiment_csv(csv_path)\n",
    "    ensure_outdir(out_dir)\n",
    "    # default ordering for Nc\n",
    "    if nc_order is None:\n",
    "        # prefer Nc=1,5,10,50, iid ordering if present\n",
    "        possible = ['Nc=1','Nc=5','Nc=10','Nc=50','iid']\n",
    "        present = [x for x in possible if x in df['Nc_label'].unique()]\n",
    "        other = []\n",
    "        nc_order = present + other\n",
    "    summary_paths = {}\n",
    "    summary_paths.update(plot_accuracy_vs_rounds(df, out_dir, nc_order=nc_order, J_list=J_list, smoothing=smoothing))\n",
    "    summary_paths['final_acc_vs_J'] = plot_final_acc_vs_J(df, out_dir, nc_order=nc_order, J_list=J_list)\n",
    "    final_df, agg_df = export_final_metrics_csv(df, Path(out_dir)/\"final_metrics_per_run.csv\")\n",
    "    summary_paths['final_metrics_csv'] = str(Path(out_dir)/\"final_metrics_per_run.csv\")\n",
    "    summary_paths['aggregated_csv'] = str(Path(out_dir)/\"final_metrics_per_run_aggregated.csv\")\n",
    "    print(\"All done. Summary:\", summary_paths)\n",
    "    return summary_paths\n",
    "\n",
    "# -------------------------\n",
    "# Example usage (uncomment & edit paths)\n",
    "# -------------------------\n",
    "CSV_PATH = \"le_nouveaux_log.csv\"   # <--- set your CSV path\n",
    "OUT_DIR = \"figs/auto_plots\"\n",
    "generate_all_plots(CSV_PATH, OUT_DIR, smoothing=3)\n",
    "#\n",
    "# If you have per-client arrays (numpy) available, call:\n",
    "#plot_shard_heatmap(my_client_class_counts, \"figs/auto_plots/shard_heatmap.png\")\n",
    "#plot_per_client_accuracy_hist(my_per_client_acc_array, \"figs/auto_plots/per_client_hist.png\")\n",
    "#\n",
    "# To inspect a quick summary table:\n",
    "# df = load_experiment_csv(CSV_PATH)\n",
    "# final = final_metrics_per_run(df)\n",
    "# print(final.groupby(['Nc_label','J'])['test_acc'].agg(['mean','std','count']).reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db8f2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved accuracy-vs-rounds plots: {'Nc=1': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_1.png', 'Nc=5': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_5.png', 'Nc=10': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_10.png', 'Nc=50': 'figs\\\\auto_plots\\\\acc_vs_rounds_Nc_50.png', 'iid': 'figs\\\\auto_plots\\\\acc_vs_rounds_iid.png'}\n",
      "Saved final-accuracy-vs-J plot: figs\\auto_plots\\final_acc_vs_J.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\AppData\\Local\\Temp\\ipykernel_30524\\4182015453.py:206: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  vals.append(float(row['mean']))\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp\\ipykernel_30524\\4182015453.py:207: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  errs.append(float(row['std']) if not np.isnan(row['std'].iloc[0]) else 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved loss-vs-rounds plots: {'Nc=1': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_1.png', 'Nc=5': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_5.png', 'Nc=10': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_10.png', 'Nc=50': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_50.png', 'iid': 'figs\\\\auto_plots\\\\loss_vs_rounds_iid.png'}\n",
      "Saved final-loss-vs-J plot: figs\\auto_plots\\final_loss_vs_J.png\n",
      "Exported final per-run and aggregated CSVs to: figs\\auto_plots\\final_metrics_per_run_with_loss.csv\n",
      "All done. Summary: {'Nc=1': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_1.png', 'Nc=5': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_5.png', 'Nc=10': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_10.png', 'Nc=50': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_50.png', 'iid': 'figs\\\\auto_plots\\\\loss_vs_rounds_iid.png', 'final_acc_vs_J': 'figs\\\\auto_plots\\\\final_acc_vs_J.png', 'final_loss_vs_J': 'figs\\\\auto_plots\\\\final_loss_vs_J.png', 'final_metrics_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_with_loss.csv', 'aggregated_acc_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_with_loss_aggregated_acc.csv', 'aggregated_loss_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_with_loss_aggregated_loss.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\AppData\\Local\\Temp\\ipykernel_30524\\3709557724.py:77: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  vals.append(float(row['mean']))\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp\\ipykernel_30524\\3709557724.py:78: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  errs.append(float(row['std']) if not np.isnan(row['std'].iloc[0]) else 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Nc=1': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_1.png',\n",
       " 'Nc=5': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_5.png',\n",
       " 'Nc=10': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_10.png',\n",
       " 'Nc=50': 'figs\\\\auto_plots\\\\loss_vs_rounds_Nc_50.png',\n",
       " 'iid': 'figs\\\\auto_plots\\\\loss_vs_rounds_iid.png',\n",
       " 'final_acc_vs_J': 'figs\\\\auto_plots\\\\final_acc_vs_J.png',\n",
       " 'final_loss_vs_J': 'figs\\\\auto_plots\\\\final_loss_vs_J.png',\n",
       " 'final_metrics_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_with_loss.csv',\n",
       " 'aggregated_acc_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_with_loss_aggregated_acc.csv',\n",
       " 'aggregated_loss_csv': 'figs\\\\auto_plots\\\\final_metrics_per_run_with_loss_aggregated_loss.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Loss plotting utilities (drop-in)\n",
    "# -------------------------\n",
    "def plot_loss_vs_rounds(df: pd.DataFrame, out_dir: str,\n",
    "                        nc_order: Optional[Sequence[str]] = None,\n",
    "                        J_list: Optional[Sequence[int]] = (4,8,16),\n",
    "                        smoothing: int = 1):\n",
    "    \"\"\"\n",
    "    Plot test loss vs rounds grouped by Nc label.\n",
    "    - df: DataFrame from load_experiment_csv\n",
    "    - out_dir: directory to save PNGs\n",
    "    - nc_order: list of Nc_label strings to iterate in order\n",
    "    - smoothing: optional moving average kernel (integer)\n",
    "    \"\"\"\n",
    "    ensure_outdir(out_dir)\n",
    "    if nc_order is None:\n",
    "        nc_order = sorted(df['Nc_label'].unique(), key=lambda x: (x!='iid', x))\n",
    "    summary = {}\n",
    "    for nc in nc_order:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        ax = plt.gca()\n",
    "        subset = df[df['Nc_label']==nc]\n",
    "        if subset.empty:\n",
    "            print(f\"Warning: no data for {nc}, skipping.\")\n",
    "            continue\n",
    "        for J in J_list:\n",
    "            sel = subset[subset['J']==J]\n",
    "            if sel.empty:\n",
    "                continue\n",
    "            grouped = sel.groupby('round')['test_loss'].agg(['mean','std','count']).reset_index()\n",
    "            if grouped.empty:\n",
    "                continue\n",
    "            x = grouped['round'].values\n",
    "            y = grouped['mean'].values\n",
    "            if smoothing > 1:\n",
    "                kernel = np.ones(smoothing) / smoothing\n",
    "                y = np.convolve(y, kernel, mode='same')\n",
    "            ax.plot(x, y, label=f\"J={J}\")\n",
    "            if 'std' in grouped.columns and not grouped['std'].isna().all():\n",
    "                std = grouped['std'].values\n",
    "                ax.fill_between(x, y-std, y+std, alpha=0.2)\n",
    "        ax.set_title(f\"Test loss vs rounds — {nc}\")\n",
    "        ax.set_xlabel(\"Round\")\n",
    "        ax.set_ylabel(\"Test loss\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        out_file = Path(out_dir)/f\"loss_vs_rounds_{nc.replace('=','_')}.png\"\n",
    "        plt.savefig(out_file)\n",
    "        plt.close()\n",
    "        summary[nc] = str(out_file)\n",
    "    print(\"Saved loss-vs-rounds plots:\", summary)\n",
    "    return summary\n",
    "\n",
    "def plot_final_loss_vs_J(df: pd.DataFrame, out_dir: str,\n",
    "                         nc_order: Optional[Sequence[str]] = None,\n",
    "                         J_list: Optional[Sequence[int]] = (4,8,16)):\n",
    "    \"\"\"\n",
    "    Bar plot showing final test loss (mean ± std) vs J for each Nc.\n",
    "    \"\"\"\n",
    "    ensure_outdir(out_dir)\n",
    "    final = final_metrics_per_run(df)\n",
    "    # aggregate per (Nc_label, J)\n",
    "    # note: final_metrics_per_run returns test_loss in addition to test_acc\n",
    "    agg = final.groupby(['Nc_label','J'])['test_loss'].agg(['mean','std','count']).reset_index()\n",
    "    if nc_order is None:\n",
    "        nc_order = sorted(agg['Nc_label'].unique(), key=lambda x: (x!='iid', x))\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    width = 0.15\n",
    "    x_positions = np.arange(len(nc_order))\n",
    "    offsets = np.linspace(-width, width, num=len(J_list))\n",
    "    for i, J in enumerate(J_list):\n",
    "        vals = []\n",
    "        errs = []\n",
    "        for nc in nc_order:\n",
    "            row = agg[(agg['Nc_label']==nc) & (agg['J']==J)]\n",
    "            if not row.empty:\n",
    "                vals.append(float(row['mean']))\n",
    "                errs.append(float(row['std']) if not np.isnan(row['std'].iloc[0]) else 0.0)\n",
    "            else:\n",
    "                vals.append(0.0)\n",
    "                errs.append(0.0)\n",
    "        ax.bar(x_positions + offsets[i], vals, width=width, yerr=errs, label=f\"J={J}\", capsize=3)\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(nc_order)\n",
    "    ax.set_ylabel(\"Final test loss (mean ± std)\")\n",
    "    ax.set_title(\"Final test loss vs J for each Nc\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    out_file = Path(out_dir)/\"final_loss_vs_J.png\"\n",
    "    plt.savefig(out_file)\n",
    "    plt.close()\n",
    "    print(\"Saved final-loss-vs-J plot:\", out_file)\n",
    "    return str(out_file)\n",
    "\n",
    "def export_final_metrics_csv_with_loss(df: pd.DataFrame, out_path: Union[str, Path]):\n",
    "    \"\"\"\n",
    "    Export a CSV with final metrics per run including loss and aggregated stats per (Nc,J).\n",
    "    \"\"\"\n",
    "    final = final_metrics_per_run(df)\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    final.to_csv(out_path, index=False)\n",
    "    # aggregated summary for both acc and loss\n",
    "    agg_acc = final.groupby(['Nc_label','J'])['test_acc'].agg(['mean','std','count']).reset_index()\n",
    "    agg_loss = final.groupby(['Nc_label','J'])['test_loss'].agg(['mean','std','count']).reset_index()\n",
    "    agg_acc.to_csv(out_path.parent / (out_path.stem + \"_aggregated_acc.csv\"), index=False)\n",
    "    agg_loss.to_csv(out_path.parent / (out_path.stem + \"_aggregated_loss.csv\"), index=False)\n",
    "    print(\"Exported final per-run and aggregated CSVs to:\", out_path)\n",
    "    return final, agg_acc, agg_loss\n",
    "\n",
    "def generate_all_plots_with_loss(csv_path: str, out_dir: str,\n",
    "                                 nc_order: Optional[Sequence[str]] = None,\n",
    "                                 J_list: Optional[Sequence[int]] = (4,8,16),\n",
    "                                 smoothing: int = 1):\n",
    "    \"\"\"\n",
    "    Convenience wrapper to create accuracy + loss plots and exports.\n",
    "    \"\"\"\n",
    "    df = load_experiment_csv(csv_path)\n",
    "    ensure_outdir(out_dir)\n",
    "    if nc_order is None:\n",
    "        possible = ['Nc=1','Nc=5','Nc=10','Nc=50','iid']\n",
    "        present = [x for x in possible if x in df['Nc_label'].unique()]\n",
    "        other = []\n",
    "        nc_order = present + other\n",
    "    summary_paths = {}\n",
    "    # accuracy\n",
    "    summary_paths.update(plot_accuracy_vs_rounds(df, out_dir, nc_order=nc_order, J_list=J_list, smoothing=smoothing))\n",
    "    summary_paths['final_acc_vs_J'] = plot_final_acc_vs_J(df, out_dir, nc_order=nc_order, J_list=J_list)\n",
    "    # loss\n",
    "    summary_paths.update(plot_loss_vs_rounds(df, out_dir, nc_order=nc_order, J_list=J_list, smoothing=smoothing))\n",
    "    summary_paths['final_loss_vs_J'] = plot_final_loss_vs_J(df, out_dir, nc_order=nc_order, J_list=J_list)\n",
    "    # exports\n",
    "    final_df, agg_acc, agg_loss = export_final_metrics_csv_with_loss(df, Path(out_dir)/\"final_metrics_per_run_with_loss.csv\")\n",
    "    summary_paths['final_metrics_csv'] = str(Path(out_dir)/\"final_metrics_per_run_with_loss.csv\")\n",
    "    summary_paths['aggregated_acc_csv'] = str(Path(out_dir)/\"final_metrics_per_run_with_loss_aggregated_acc.csv\")\n",
    "    summary_paths['aggregated_loss_csv'] = str(Path(out_dir)/\"final_metrics_per_run_with_loss_aggregated_loss.csv\")\n",
    "    print(\"All done. Summary:\", summary_paths)\n",
    "    return summary_paths\n",
    "\n",
    "# -------------------------\n",
    "# Example usage (uncomment & edit paths)\n",
    "# -------------------------\n",
    "CSV_PATH = \"le_nouveaux_log.csv\"   # <--- set your CSV path\n",
    "OUT_DIR = \"figs/auto_plots\"\n",
    "generate_all_plots_with_loss(CSV_PATH, OUT_DIR, smoothing=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0a783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185bdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fb5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-machine-learning-labs-MHGu8hrl-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
