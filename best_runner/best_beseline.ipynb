{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lGxnD3UbdWmb"
      },
      "outputs": [],
      "source": [
        "# ── A. Imports & Reproducibility ────────────────────────────────────────────────\n",
        "import os, copy\n",
        "import csv                                                  # For result logging :contentReference[oaicite:0]{index=0}\n",
        "import random                                               # For seeding :contentReference[oaicite:1]{index=1}\n",
        "import numpy as np                                          # For numeric ops :contentReference[oaicite:2]{index=2}\n",
        "import torch                                               # Core PyTorch :contentReference[oaicite:3]{index=3}\n",
        "import torch.nn as nn                                       # Neural-net modules :contentReference[oaicite:4]{index=4}\n",
        "import torch.nn.functional as F                             # Functional API :contentReference[oaicite:5]{index=5}\n",
        "import torch.optim as optim                                 # Optimizers :contentReference[oaicite:6]{index=6}\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR      # Scheduler :contentReference[oaicite:7]{index=7}\n",
        "from torch.utils.data import DataLoader, random_split       # Data loaders & splits :contentReference[oaicite:8]{index=8}\n",
        "import torchvision                                          # Datasets & transforms :contentReference[oaicite:9]{index=9}\n",
        "import torchvision.transforms as T                          # Transforms :contentReference[oaicite:10]{index=10}\n",
        "from torch.utils.tensorboard import SummaryWriter           # TensorBoard logging :contentReference[oaicite:11]{index=11}\n",
        "import matplotlib.pyplot as plt                             # Plotting :contentReference[oaicite:12]{index=12}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Q3SwyBdWnP",
        "outputId": "27837457-f0b2-4966-a0be-babd9a9fbedd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Seed everything for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# ── B. Device ───────────────────────────────────────────────────────────────────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")                             # Confirm GPU vs CPU :contentReference[oaicite:13]{index=13}\n",
        "\n",
        "\n",
        "\n",
        "# ── C. Data Preparation ─────────────────────────────────────────────────────────\n",
        "# Transforms\n",
        "transform_train = T.Compose([\n",
        "    T.RandomCrop(32, padding=4), T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
        "])\n",
        "transform_test = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bF8TWhdImubH"
      },
      "outputs": [],
      "source": [
        "import glob, torch, os\n",
        "\n",
        "\n",
        "def latest_ckpt(dirpath, pattern=\"last_ckpt__round_*.pth\"):\n",
        "    paths = glob.glob(os.path.join(dirpath, pattern))\n",
        "    if not paths:\n",
        "        return None\n",
        "    paths.sort(key=lambda p: int(p.rsplit(\"_\", 1)[1].split(\".\")[0]))\n",
        "    return paths[-1]\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, ckpt_dir, resume=True,name=\"\"):\n",
        "    if not resume:\n",
        "        print(\"[Checkpoint] Starting training from scratch.\")\n",
        "        return 1\n",
        "    if name:\n",
        "        pattern='last_ckpt_'+name+'_round_*.pth'\n",
        "        ckpt_path = latest_ckpt(ckpt_dir,pattern)\n",
        "    else:\n",
        "        ckpt_path = latest_ckpt(ckpt_dir)\n",
        "    if ckpt_path is None:\n",
        "        print(\"[Checkpoint] No checkpoint found; training from scratch.\")\n",
        "        return 1\n",
        "    # Load checkpoint tensors onto CPU to preserve RNG state tensor\n",
        "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "    model.load_state_dict(ckpt['model_state'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "    # Restore CPU RNG state\n",
        "    rng_state = ckpt['rng_state']\n",
        "    if rng_state.device.type != 'cpu':\n",
        "        rng_state = rng_state.cpu()\n",
        "    torch.set_rng_state(rng_state)\n",
        "    print(f\"[Checkpoint] Resumed from round {ckpt['round']} (loaded {os.path.basename(ckpt_path)})\")\n",
        "    return ckpt['round'] + 1\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, round_num, ckpt_dir,personalized_par_string=\"\", is_best=False):\n",
        "    print(f\"[Checkpoint] Saving round {round_num}...\")\n",
        "    state = {\n",
        "        'round': round_num,\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'rng_state': torch.get_rng_state(),\n",
        "    }\n",
        "    fname = f\"{'best' if is_best else 'last'}_ckpt_{personalized_par_string}_round_{round_num}.pth\"\n",
        "    half_name=f\"last_ckpt_{personalized_par_string}_round_\"\n",
        "    if is_best:\n",
        "        torch.save(model.state_dict(), os.path.join(ckpt_dir,fname))\n",
        "    else:\n",
        "            torch.save(state, os.path.join(ckpt_dir, fname))\n",
        "\n",
        "    print(f\"[Checkpoint] Done saving to {fname}\")\n",
        "\n",
        "    for existing in os.listdir(ckpt_dir):\n",
        "         existing_path = os.path.join(ckpt_dir, existing)\n",
        "         if (\n",
        "             existing.endswith('.pth')\n",
        "             and existing != fname\n",
        "             and 'best' not in existing\n",
        "             and half_name in existing\n",
        "         ):\n",
        "             os.remove(existing_path)\n",
        "             print(f\"  Deleted: {existing}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def load_checkpoint(model, optimizer, ckpt_dir, resume=True):\n",
        "#     if not resume:\n",
        "#         print(\"[Checkpoint] Starting training from scratch.\")\n",
        "#         return 1\n",
        "#     ckpt_path = latest_ckpt(ckpt_dir,'last_ckpt_round_lambda_\\d_*.pth')  #path to ignore first number that is parameter of lamda\n",
        "#     if ckpt_path is None:\n",
        "#         print(\"[Checkpoint] No checkpoint found; training from scratch.\")\n",
        "#         return 1\n",
        "#     # Load checkpoint tensors onto CPU to preserve RNG state tensor\n",
        "#     ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "#     model.load_state_dict(ckpt['model_state'])\n",
        "#     optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "#     # Restore CPU RNG state\n",
        "#     rng_state = ckpt['rng_state']\n",
        "#     if rng_state.device.type != 'cpu':\n",
        "#         rng_state = rng_state.cpu()\n",
        "#     torch.set_rng_state(rng_state)\n",
        "#     print(f\"[Checkpoint] Resumed from round {ckpt['round']} (loaded {os.path.basename(ckpt_path)})\")\n",
        "#     return ckpt['round'] + 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1KsmNAdjdWqH",
        "outputId": "79b45e48-5862-4769-fa70-dd9659401996"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:37<00:00, 4.47MB/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'la zanzara concordo'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ── C. Data Preparation ─────────────────────────────────────────────────────────\n",
        "# Transforms (as before)…\n",
        "\n",
        "# Download full CIFAR‑100 training set\n",
        "full_train = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "\n",
        "# 1) Centralized validation split\n",
        "val_size   = 5000\n",
        "train_size = len(full_train) - val_size\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_train,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(seed)\n",
        ")\n",
        "\n",
        "train_loader_all = DataLoader(\n",
        "    train_dataset, batch_size=256, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "\n",
        "# ── C.1 Build validation loader ───────────────────────────────\n",
        "BS_VAL = 256\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BS_VAL,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        " ###chang ehere \n",
        "\n",
        "# 2) IID sharding of the remaining train_dataset into K=100 clients\n",
        "K = 100\n",
        "base = train_size // K\n",
        "sizes = [base] * (K - 1) + [train_size - base * (K - 1)]\n",
        "shards = random_split(\n",
        "    train_dataset,\n",
        "    sizes,\n",
        "    generator=torch.Generator().manual_seed(seed)\n",
        "    )\n",
        "\n",
        "\n",
        "# 3) Global test set (unchanged)\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "bs_test = 256\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=bs_test, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "# 4) (Later) you can build per-client loaders:\n",
        "# client_loaders = [\n",
        "#     DataLoader(shards[i], batch_size=bs, shuffle=True, num_workers=2)\n",
        "#     for i in range(K)\n",
        "# ]\n",
        "\n",
        "\"la zanzara concordo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9zukJAc0dWq0"
      },
      "outputs": [],
      "source": [
        "# ── D. Model Definition ─────────────────────────────────────────────────────────\n",
        "class LELeNetCIFAR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1   = nn.Linear(64*8*8, 384)\n",
        "        self.fc2   = nn.Linear(384, 192)\n",
        "        self.fc3   = nn.Linear(192, 100)\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x)); x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bnOGpNdtw-X0"
      },
      "outputs": [],
      "source": [
        "# ── E. Utilities: Train/Eval & Checkpointing ────────────────────────────────────\n",
        "def train_one_epoch(model, optimizer, criterion, loader):\n",
        "    model.train()\n",
        "    running_loss = correct = total = 0\n",
        "    for imgs, lbls in loader:\n",
        "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(imgs)\n",
        "        loss = criterion(out, lbls)\n",
        "        loss.backward(); optimizer.step()\n",
        "        running_loss += loss.item()*imgs.size(0)\n",
        "        correct += out.argmax(1).eq(lbls).sum().item()\n",
        "        total += lbls.size(0)\n",
        "    return running_loss/total, correct/total\n",
        "\n",
        "def eval_model(model, criterion, loader):\n",
        "    model.eval()\n",
        "    running_loss = correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            out = model(imgs); loss = criterion(out, lbls)\n",
        "            running_loss += loss.item()*imgs.size(0)\n",
        "            correct += out.argmax(1).eq(lbls).sum().item()\n",
        "            total += lbls.size(0)\n",
        "    return running_loss/total, correct/total\n",
        "\n",
        "\n",
        "def sample_clients_dirichlet(K, m, gamma, rng):\n",
        "    \"\"\"\n",
        "    Sample m clients out of K with probabilities drawn from a Dirichlet(gamma) distribution.\n",
        "    Returns:\n",
        "      selected: list of client indices\n",
        "      p:      numpy array of length K with the sampling probabilities\n",
        "    \"\"\"\n",
        "    if gamma == 'uniform':\n",
        "        p = np.ones(K) / K\n",
        "    else:\n",
        "        alpha = np.ones(K) * gamma\n",
        "        p = rng.dirichlet(alpha)\n",
        "    selected = rng.choice(K, size=m, replace=False, p=p)\n",
        "    return selected.tolist(), p\n",
        "\n",
        "#unused below\n",
        "def sample_clients_dirichlet2(K, m, gamma, rng=None):  #check which one to use\n",
        "    \"\"\"\n",
        "    Sample m clients out of K with probabilities drawn from a Dirichlet(gamma) distribution.\n",
        "    Returns:\n",
        "      selected: list of client indices\n",
        "      p:      numpy array of length K with the sampling probabilities\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    if gamma == 'uniform':\n",
        "        p = np.ones(K) / K\n",
        "        selected = rng.choice(K, size=m, replace=False)\n",
        "    else:\n",
        "        alpha = np.ones(K) * gamma\n",
        "        p = rng.dirichlet(alpha)\n",
        "        p = p / p.sum()                          # ensure sum=1\n",
        "        selected = rng.choice(K, size=m, replace=False, p=p)\n",
        "    return selected.tolist(), p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cHwkabwS0v9L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import copy\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "def clean_results_history(results_file_name,new_file_name):\n",
        "    input_file = results_file_name\n",
        "    output_file=new_file_name\n",
        "\n",
        "\n",
        "    # Read and clean lines\n",
        "    with open(input_file, \"r\") as f:\n",
        "        lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    filtered = []\n",
        "    last_seen_index = float('inf')  # Start with a very large number\n",
        "    header = lines[0]\n",
        "    data_lines = lines[1:]\n",
        "\n",
        "    # Iterate in reverse\n",
        "    for line in reversed(data_lines):\n",
        "        current_index = int(line.split(',')[0])\n",
        "        if current_index < last_seen_index:\n",
        "            filtered.append(line)\n",
        "            last_seen_index = current_index\n",
        "        else:\n",
        "            # Skip this line, as its index is higher than the next one\n",
        "            continue\n",
        "\n",
        "    # Reverse again to restore original order (except removed lines)\n",
        "    filtered.reverse()\n",
        "\n",
        "    # Write to output\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(header+\"\\n\")\n",
        "\n",
        "        f.write('\\n'.join(filtered))\n",
        "\n",
        "    print(f\"Filtered results written to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "def get_results(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        acc_col = f\"{split}_acc\"\n",
        "        loss_col = f\"{split}_loss\"\n",
        "\n",
        "        max_acc = df[acc_col].max()\n",
        "        max_idx = df[acc_col].idxmax()\n",
        "\n",
        "        max_round = df.loc[max_idx, 'round']\n",
        "        loss_at_max = df.loc[max_idx, loss_col]\n",
        "\n",
        "        results[split] = {\n",
        "            'max_acc': max_acc,\n",
        "            'round': int(max_round),\n",
        "            'loss_at_max': loss_at_max\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def write_final_results(name, params, csv_path='results_log.csv', results_csv_path='global_results.csv'):\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    results = get_results(csv_path)\n",
        "\n",
        "    row = {\n",
        "        'timestamp': timestamp,\n",
        "        'model_name': name,\n",
        "        'parameters': json.dumps(params),\n",
        "        'train_max_acc': results['train']['max_acc'],\n",
        "        'train_round': results['train']['round'],\n",
        "        'train_loss': results['train']['loss_at_max'],\n",
        "        'val_max_acc': results['val']['max_acc'],\n",
        "        'val_round': results['val']['round'],\n",
        "        'val_loss': results['val']['loss_at_max'],\n",
        "        'test_max_acc': results['test']['max_acc'],\n",
        "        'test_round': results['test']['round'],\n",
        "        'test_loss': results['test']['loss_at_max'],\n",
        "    }\n",
        "\n",
        "    file_exists = os.path.exists(results_csv_path)\n",
        "\n",
        "    with open(results_csv_path, 'a', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row)\n",
        "\n",
        "rng = np.random.default_rng(seed)\n",
        "\n",
        "def run_federated_training(\n",
        "    global_model,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    client_loaders,\n",
        "    train_loader_all,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    shards,\n",
        "    params,\n",
        "    CKPT_DIR=\"./fl_checkpoints\",\n",
        "    name=\"fed\"\n",
        "):\n",
        "    # Prepare CSV logging path per gamma\n",
        "    csv_path=f'./fedavg_results.csv'\n",
        "\n",
        "    gamma     = params['GAMMA']\n",
        "    K         = params['K']\n",
        "    C         = params['C']\n",
        "    J         = params['J']\n",
        "    ROUNDS    = params['ROUNDS']\n",
        "    BS        = params['batch_size']\n",
        "    LR        = params['lr']\n",
        "    WD        = params['weight_decay']\n",
        "    momentum  = params['momentum']\n",
        "    \n",
        "\n",
        "\n",
        "    csv_path_res = f'./'+name+'_results.csv'\n",
        "    if not os.path.exists(csv_path):\n",
        "        with open(csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['round', 'val_loss', 'val_acc', 'test_loss', 'test_acc','train_loss', 'train_acc'])\n",
        "\n",
        "\n",
        "\n",
        "    start_round = load_checkpoint(global_model, optimizer, CKPT_DIR, resume=True,name=name)\n",
        "    best_accuracy = 0\n",
        "\n",
        "    print(f\"[Training] Starting gamma = {gamma} from round {start_round} to {ROUNDS}\")\n",
        "\n",
        "    best_model_state = None\n",
        "    best_round=1\n",
        "\n",
        "    for rnd in range(start_round, ROUNDS + 1):\n",
        "        # 1) Sample clients\n",
        "        m = max(1, int(C * K)) # → 10 clients per round when K=100, C=0.1\n",
        "\n",
        "        selected, _ = sample_clients_dirichlet(K, m, gamma, rng )\n",
        "\n",
        "        local_states, sizes = [], []\n",
        "        for i in selected:\n",
        "            # 2a) Copy global model\n",
        "            client_model = copy.deepcopy(global_model)\n",
        "            client_model.train()   ####to delete???\n",
        "            client_opt = optim.SGD(client_model.parameters(), lr=LR, momentum=0.9, weight_decay=WD)\n",
        "            # 2b) Local training for J epochs\n",
        "            for _ in range(J):\n",
        "                train_one_epoch(client_model, client_opt, criterion, client_loaders[i])\n",
        "            local_states.append(client_model.state_dict())\n",
        "            sizes.append(len(shards[i]))\n",
        "        # 3) Weighted aggregation\n",
        "        total_size = sum(sizes)\n",
        "        new_state = {\n",
        "            k: sum((sizes[j] / total_size) * local_states[j][k] for j in range(len(sizes)))\n",
        "            for k in global_model.state_dict()\n",
        "        }\n",
        "        global_model.load_state_dict(new_state)\n",
        "         # 4) Global evaluation\n",
        "        train_loss, train_acc = eval_model(global_model, criterion, train_loader_all)\n",
        "        val_loss, val_acc = eval_model(global_model, criterion, val_loader)\n",
        "        test_loss, test_acc = eval_model(global_model, criterion, test_loader)\n",
        "\n",
        "\n",
        "\n",
        "        with open(csv_path, 'a', newline='') as f:\n",
        "            csv.writer(f).writerow([\n",
        "                rnd,\n",
        "                f\"{val_loss:.4f}\", f\"{val_acc:.4f}\",\n",
        "                f\"{test_loss:.4f}\", f\"{test_acc:.4f}\",\n",
        "                 f\"{train_loss:.4f}\",  f\"{train_acc:.4f}\"\n",
        "            ])\n",
        "\n",
        "\n",
        "\n",
        "        if test_acc > best_accuracy:\n",
        "            best_accuracy = test_acc\n",
        "            best_model_state = copy.deepcopy(global_model.state_dict())\n",
        "            best_round=rnd\n",
        "\n",
        "\n",
        "        # if val_acc > best_accuracy:\n",
        "        #     best_accuracy = val_acc\n",
        "        #     save_checkpoint(global_model, optimizer, rnd, CKPT_DIR, is_best=True)\n",
        "\n",
        "        if rnd % 3 == 0 or rnd == 1:\n",
        "             print(f\"Checkpointing round {rnd} (gamma={gamma})\")\n",
        "             save_checkpoint(global_model, optimizer, rnd, CKPT_DIR,personalized_par_string=name, is_best=False)\n",
        "             print(f\"Round {rnd}/{ROUNDS} | Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        global_model.load_state_dict(best_model_state)  # Load best weights back into the model\n",
        "        save_checkpoint(\n",
        "            model=global_model,\n",
        "            optimizer=optimizer,\n",
        "            round_num=best_round,\n",
        "            ckpt_dir=CKPT_DIR,\n",
        "            personalized_par_string=name,\n",
        "            is_best=True\n",
        "        )\n",
        "\n",
        "    clean_results_history(csv_path,csv_path_res)\n",
        "\n",
        "\n",
        "    write_final_results(name,params,csv_path_res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # if best_model_state is not None:\n",
        "    #     model_path = os.path.join(CKPT_DIR, f\"best_model_gamma_{gamma}.pth\")\n",
        "    #     torch.save(best_model_state, model_path)\n",
        "    #     print(f\"[Saved] Best model for gamma={gamma} with Val Acc = {best_accuracy:.4f} → {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "def create_labelwise_shards(dataset, K, Nc, seed=42):\n",
        "    # 1) Group indices by label\n",
        "    label2idx = defaultdict(list)\n",
        "    for idx, (_, lbl) in enumerate(dataset):\n",
        "        label2idx[lbl].append(idx)\n",
        "\n",
        "    # 2) Shuffle each label’s pool\n",
        "    rng = random.Random(seed)\n",
        "    for lbl in label2idx:\n",
        "        rng.shuffle(label2idx[lbl])\n",
        "\n",
        "    # 3) Prepare an iterator per label\n",
        "    pointers = {lbl: 0 for lbl in label2idx}\n",
        "\n",
        "    # 4) Build shards\n",
        "    samples_per_client = len(dataset) // K\n",
        "    shards_idx = []\n",
        "    labels_cycle = list(label2idx.keys())\n",
        "\n",
        "    for client_id in range(K):\n",
        "        client_idxs = []\n",
        "        # Rotate start point so clients don’t always pick the same first label\n",
        "        rng.shuffle(labels_cycle)\n",
        "        for lbl in labels_cycle:\n",
        "            if len(client_idxs) >= samples_per_client:\n",
        "                break\n",
        "            # How many to take from this label\n",
        "            needed = samples_per_client - len(client_idxs)\n",
        "            available = len(label2idx[lbl]) - pointers[lbl]\n",
        "            take = min(needed, available)\n",
        "            if take > 0:\n",
        "                start = pointers[lbl]\n",
        "                end   = start + take\n",
        "                client_idxs.extend(label2idx[lbl][start:end])\n",
        "                pointers[lbl] += take\n",
        "        # If we still haven’t reached samples_per_client (rare), fill randomly\n",
        "        if len(client_idxs) < samples_per_client:\n",
        "            all_remaining = [i for lbl in label2idx\n",
        "                                 for i in label2idx[lbl][pointers[lbl]:]]\n",
        "            client_idxs.extend(rng.sample(all_remaining,\n",
        "                                          samples_per_client - len(client_idxs)))\n",
        "        shards_idx.append(client_idxs)\n",
        "\n",
        "    return [Subset(dataset, idxs) for idxs in shards_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAGLQObvdWs7",
        "outputId": "ff75a994-b415-48e3-c2b2-137cbcf81409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Checkpoint] No checkpoint found; training from scratch.\n",
            "[Training] Starting gamma = uniform from round 1 to 200\n",
            "Checkpointing round 1 (gamma=uniform)\n",
            "[Checkpoint] Saving round 1...\n",
            "[Checkpoint] Done saving to last_ckpt_hetetogeneous_shard_iid_J_4_round_1.pth\n",
            "Round 1/200 | Val Acc: 0.0082, Test Acc: 0.0092\n",
            "Checkpointing round 3 (gamma=uniform)\n",
            "[Checkpoint] Saving round 3...\n",
            "[Checkpoint] Done saving to last_ckpt_hetetogeneous_shard_iid_J_4_round_3.pth\n",
            "  Deleted: last_ckpt_hetetogeneous_shard_iid_J_4_round_1.pth\n",
            "Round 3/200 | Val Acc: 0.0124, Test Acc: 0.0101\n",
            "Checkpointing round 6 (gamma=uniform)\n",
            "[Checkpoint] Saving round 6...\n",
            "[Checkpoint] Done saving to last_ckpt_hetetogeneous_shard_iid_J_4_round_6.pth\n",
            "  Deleted: last_ckpt_hetetogeneous_shard_iid_J_4_round_3.pth\n",
            "Round 6/200 | Val Acc: 0.0230, Test Acc: 0.0211\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ── A. Mount Google Drive ─────────────────────────────────────────────────────\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    # If import succeeds, we are likely in Colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    # If import fails, we are likely not in Colab\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    CKPT_DIR = '/content/drive/MyDrive/fl_checkpoints'\n",
        "else:\n",
        "    CKPT_DIR = './fl_checkpoints'\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set this to True to resume from the last checkpoint; False to start from scratch\n",
        "RESUME = True\n",
        "\n",
        "\n",
        "\n",
        "# ── A. FedAvg Hyperparameters ───────────────────────────────────────────────────\n",
        "# Fixed FL parameters\n",
        "K      = 100      # total clients\n",
        "C      = 0.1      # fraction of clients sampled per round\n",
        "J      = 4        # local epochs per client\n",
        "ROUNDS = 200  #2000     # total communication rounds\n",
        "\n",
        "# Optimizer hyperparameters (constant, no schedule)\n",
        "LR     = 0.01     # fixed learning rate\n",
        "WD     = 1e-4     # weight decay\n",
        "BS     = 128      # per-client batch size\n",
        "momentum=0.9\n",
        "\n",
        "#gamma value for dirichlet client selection\n",
        "\n",
        "\n",
        "budget   = J * ROUNDS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Before your FedAvg loop: Instantiate the global model, loss, and client loaders once\n",
        "\n",
        "# Instantiate model, optimizer, loss, and client loaders\n",
        "global_model   = LELeNetCIFAR().to(device)\n",
        "optimizer      = optim.SGD(global_model.parameters(), lr=LR, momentum=momentum, weight_decay=WD)\n",
        "criterion      = nn.CrossEntropyLoss()\n",
        "client_loaders = [\n",
        "    DataLoader(shards[i], batch_size=BS, shuffle=True, num_workers=2)\n",
        "    for i in range(K)\n",
        "]  ##should i out in the for loop below ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gammas = [0.01]   #[0.01, 0.1, 0.5, 1, 10, 100]\n",
        "\n",
        "\n",
        "\n",
        "base = train_size // K\n",
        "sizes = [base]*(K-1) + [train_size-base*(K-1)]\n",
        "iid_shards = random_split(train_dataset, sizes,\n",
        "                        generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "Nc_list   = [1, 5, 10, 50]\n",
        "shardings = {'iid': iid_shards}\n",
        "\n",
        "for Nc in Nc_list:\n",
        "    shardings[f'non_iid_{Nc}'] = create_labelwise_shards(\n",
        "            train_dataset, K, Nc, seed\n",
        "    )\n",
        "\n",
        "\n",
        "#gamma value for dirichlet client selection\n",
        "gamma='uniform'   #gamma uniform since i'm not testing that \n",
        "J_list   = [4, 8, 16]\n",
        "for shard_key, shards in shardings.items():\n",
        "    client_loaders = [\n",
        "        DataLoader(shards[i], batch_size=BS, shuffle=True, num_workers=2)\n",
        "        for i in range(K)\n",
        "    ]\n",
        "    for J in J_list: #the other J that is outside , it's used onlu to calc the budget\n",
        "\n",
        "    \n",
        "\n",
        "        ROUNDS = budget // J  ##rounds scaled \n",
        "        # Reinitialize model and optimizer per gamma\n",
        "        model = LELeNetCIFAR().to(device)\n",
        "        opt = optim.SGD(model.parameters(), lr=LR, momentum=momentum, weight_decay=WD)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        params= {   ##probably needed to differenciate parameter between client and gloabal model\n",
        "        'lr':           LR,\n",
        "        'weight_decay': WD,\n",
        "        'batch_size':   BS,\n",
        "        'K':            K,\n",
        "        'C':            C,\n",
        "        'J':            J,\n",
        "        'ROUNDS':       ROUNDS,\n",
        "        'GAMMA':gamma,\n",
        "        \"momentum\": momentum\n",
        "        }\n",
        "\n",
        "        ##new_part\n",
        "\n",
        "\n",
        "        ##new_part\n",
        "\n",
        "            \n",
        "        run_federated_training(\n",
        "            name=\"hetetogeneous_shard_{}_J_{}\".format(shard_key,J),\n",
        "            global_model=model,\n",
        "            optimizer=opt,\n",
        "            criterion=loss_fn,\n",
        "            client_loaders=client_loaders,\n",
        "            val_loader=val_loader,\n",
        "            test_loader=test_loader,\n",
        "            train_loader_all=train_loader_all,\n",
        "            shards=shards,\n",
        "            CKPT_DIR=CKPT_DIR,params=params\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mYs93FL0v9M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('global_results.csv')\n",
        "print(df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWBCZ6Mv0v9N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of gamma values and corresponding file names\n",
        "gammas = [0.01,0.1, 0.5, 1, 10]\n",
        "base_path = './fedavg_results_gamma_{}.csv'\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Load and plot each CSV\n",
        "for gamma in gammas:\n",
        "    file_path = base_path.format(gamma)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    plt.plot(df['round'], df['test_acc'], label=f'γ = {gamma}')\n",
        "    print(f\"Gamma {gamma}: Max Test Acc = {df['test_acc'].max():.4f}\")\n",
        "\n",
        "\n",
        "# Configure plot\n",
        "plt.xlabel('Federated Round')\n",
        "plt.ylabel('Global Test Accuracy')\n",
        "plt.title('FedAvg CIFAR-100: Test Accuracy vs Rounds for Different γ')\n",
        "plt.grid(True)\n",
        "plt.legend(title=\"Gamma\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZm-lbwPdWvT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owzk--gFdWwB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5IgnGhEdWw1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1O3ub6UdWxr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advanced-machine-learning-labs-ROHCwjr8-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
