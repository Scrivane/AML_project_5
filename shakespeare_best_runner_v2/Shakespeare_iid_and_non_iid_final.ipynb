{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Shakespeare dataset (this may take a few seconds)...\n",
            "Train size: 3803542, Val size: 211308, Vocab size: 80\n"
          ]
        }
      ],
      "source": [
        "# ---------------- Full runnable Colab cell (copy-paste) ----------------\n",
        "import random, math, time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "multiple_device=False\n",
        "# Limit this notebook to 30% compute\n",
        "if multiple_device==True:\n",
        "    os.environ[\"CUDA_MPS_ACTIVE_THREAD_PERCENTAGE\"] = \"40\"\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "if multiple_device==True:\n",
        "    torch.cuda.set_per_process_memory_fraction(0.4, device=0)\n",
        "\n",
        "\n",
        "# ---------------- settings (tune these) ----------------\n",
        "K = 100                    # number of clients\n",
        "C = 0.1                    # fraction of clients per round\n",
        "CLIENTS_PER_ROUND = max(1, int(math.ceil(C * K)))\n",
        "ROUNDS = 200                # number of federated rounds per experiment\n",
        "LOCAL_STEPS_LIST = [1, 4, 8, 16]\n",
        "BASE_J = 4\n",
        "CLIENT_BATCH = 16\n",
        "EMB_DIM = 32\n",
        "HIDDEN_DIM = 128\n",
        "LR_LOCAL = 1e-3\n",
        "SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "GAMMAS = [0.01, 0.1, 1.0, 10.0]  # dirichlet concentration values for skewed scheme\n",
        "log_every = 1      # print participation summary every `log_every` rounds\n",
        "topk = 5\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ---------------- load dataset & build vocab ----------------\n",
        "def iid_shards_by_chunk_indices(dataset_len, num_clients=100, seed=42):\n",
        "    idxs = list(range(dataset_len))\n",
        "    random.Random(seed).shuffle(idxs)\n",
        "    shards = [[] for _ in range(num_clients)]\n",
        "    for i, idx in enumerate(idxs):\n",
        "        shards[i % num_clients].append(idx)\n",
        "    return shards\n",
        "if \"train_ds\" not in globals() or \"val_ds\" not in globals() or \"test_ds\" not in globals():\n",
        "    print(\"Loading Shakespeare dataset (this may take a few seconds)...\")\n",
        "    hf = load_dataset(\"flwrlabs/shakespeare\", split=\"train\").shuffle(seed=SEED)\n",
        "   \n",
        "\n",
        "\n",
        "    # train/val split\n",
        "    parts = hf.train_test_split(test_size=0.1, seed=SEED)  # 10% for val+test\n",
        "    train_hf = parts['train']\n",
        "\n",
        "    # split the 10% leftover into val and test (50-50)\n",
        "    val_test_hf = parts['test'].train_test_split(test_size=0.5, seed=SEED)\n",
        "    val_hf = val_test_hf['train']\n",
        "    test_hf = val_test_hf['test']\n",
        "\n",
        "    # build vocab from training split\n",
        "    chars = sorted({c for txt in train_hf['x'] for c in txt} | set(train_hf['y']))\n",
        "    PAD_TOKEN = \"<pad>\"\n",
        "    vocab = [PAD_TOKEN] + chars\n",
        "    char2idx = {c: i for i, c in enumerate(vocab)}\n",
        "    idx2char = {i: c for c, i in char2idx.items()}\n",
        "    VOCAB_SIZE = len(vocab)\n",
        "    print(f\"Train size: {len(train_hf)}, Val size: {len(val_hf)}, Vocab size: {VOCAB_SIZE}\")\n",
        "\n",
        "    # dataset wrapper\n",
        "    class ShakespeareDataset(Dataset):\n",
        "        def __init__(self, hf_dataset, char2idx):\n",
        "            self.hf = hf_dataset\n",
        "            self.char2idx = char2idx\n",
        "        def __len__(self):\n",
        "            return len(self.hf)\n",
        "        def __getitem__(self, idx):\n",
        "            item = self.hf[int(idx)]\n",
        "            x_str, y_str = item['x'], item['y']\n",
        "            x_idx = torch.tensor([self.char2idx[ch] for ch in x_str], dtype=torch.long)\n",
        "            y_idx = torch.tensor(self.char2idx[y_str], dtype=torch.long)\n",
        "            return x_idx, y_idx\n",
        "\n",
        "    train_ds = ShakespeareDataset(train_hf, char2idx)\n",
        "    val_ds = ShakespeareDataset(val_hf, char2idx)\n",
        "        # wrap test dataset\n",
        "    test_ds = ShakespeareDataset(test_hf, char2idx)\n",
        "\n",
        "    \n",
        "else:\n",
        "    print(\"Shakespeare dataset already loaded; skipping reload.\")\n",
        "\n",
        "\n",
        "# ---------------- model definition ----------------\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=EMB_DIM, hid=HIDDEN_DIM, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n",
        "        self.fc = nn.Linear(hid, vocab_size)\n",
        "    def forward(self, x):\n",
        "        e = self.embed(x)\n",
        "        out, _ = self.lstm(e)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "# ---------------- evaluate helper ----------------\n",
        "def evaluate(model, dataset, device=DEVICE, batch_size=256):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss = 0.0; total = 0; correct = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += xb.size(0)\n",
        "    return total_loss / total, correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XAOd3qTCGNE7",
        "outputId": "beedb14b-72d4-42be-d048-2185d4f8a161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kk\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 426\u001b[39m\n\u001b[32m    424\u001b[39m kappa,cappa=create_label_shards_by_role(train_ds, K, \u001b[32m50\u001b[39m, seed=SEED)\n\u001b[32m    425\u001b[39m kappa1,cappa1=create_label_shards(train_ds, idx2char, K, \u001b[32m50\u001b[39m, seed=SEED)\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkappa\u001b[39m\u001b[33m\"\u001b[39m, kappa)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1481\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1523\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1324\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1961\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/advanced-machine-learning-labs-ROHCwjr8-py3.12/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/advanced-machine-learning-labs-ROHCwjr8-py3.12/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "# ---------------- client local update (expects dataset returning (x_tensor,y_tensor)) ----------------\n",
        "def client_update(global_state, client_idx_list, train_dataset, steps, batch_size=CLIENT_BATCH, lr=LR_LOCAL, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Local client training starting from global_state.\n",
        "    Returns (state_cpu_dict, num_examples).\n",
        "    Assumes train_dataset[i] returns (x_tensor, y_tensor).\n",
        "    \"\"\"\n",
        "    if len(client_idx_list) == 0:\n",
        "        return None, 0\n",
        "\n",
        "    # DataLoader for this client's subset\n",
        "    subset = Subset(train_dataset, client_idx_list)\n",
        "    loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # local model init\n",
        "    local_model = CharLSTM(VOCAB_SIZE).to(device)\n",
        "    local_model.load_state_dict(global_state)\n",
        "    opt = optim.Adam(local_model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    it = iter(loader)\n",
        "    steps_done = 0\n",
        "    while steps_done < steps:\n",
        "        try:\n",
        "            xb, yb = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            xb, yb = next(it)\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = local_model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=1.0)\n",
        "        opt.step()\n",
        "        steps_done += 1\n",
        "\n",
        "    state_cpu = {k: v.cpu().clone() for k, v in local_model.state_dict().items()}\n",
        "    return state_cpu, len(client_idx_list)\n",
        "\n",
        "# ---------------- aggregation helper ----------------\n",
        "def aggregate_states(local_states, local_sizes):\n",
        "    \"\"\"Weighted average of CPU state dicts.\"\"\"\n",
        "    new_state = {}\n",
        "    total = float(sum(local_sizes))\n",
        "    keys = local_states[0].keys()\n",
        "    for k in keys:\n",
        "        new_state[k] = torch.zeros_like(local_states[0][k], dtype=local_states[0][k].dtype)\n",
        "        for s, sz in zip(local_states, local_sizes):\n",
        "            new_state[k] += s[k] * (sz / total)\n",
        "    return new_state\n",
        "\n",
        "# ---------------- sampling function (Dirichlet skew) ----------------\n",
        "def sample_skewed_clients(K, C, gamma):\n",
        "    \"\"\"\n",
        "    Draw Dirichlet probabilities and sample clients without replacement.\n",
        "    Defensively handles numerical issues by falling back to uniform p if needed.\n",
        "    Returns (selected_list, pvec)\n",
        "    \"\"\"\n",
        "    if gamma is None or not (isinstance(gamma, (int, float)) and gamma > 0):\n",
        "        raise ValueError(\"gamma must be a positive float for skewed sampling\")\n",
        "\n",
        "    p = np.random.dirichlet([gamma] * K)\n",
        "    # defensive normalization / fallback\n",
        "    if np.isnan(p).any() or p.sum() == 0:\n",
        "        p = np.ones(K) / K\n",
        "    else:\n",
        "        p = p / p.sum()\n",
        "\n",
        "    num_sel = max(1, int(math.ceil(C * K)))\n",
        "    selected = list(np.random.choice(np.arange(K), size=num_sel, replace=False, p=p))\n",
        "    return selected, p\n",
        "\n",
        "# ---------------- optional: non-iid shard creator (by target label count Nc) ----------------\n",
        "def create_label_shards(train_dataset, idx2char, K, Nc, seed=SEED):\n",
        "    \"\"\"\n",
        "    Create client shards where each client has at most Nc distinct target labels.\n",
        "    Returns client_indices list-of-lists and clients_allowed (labels per client).\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    all_labels = list(idx2char.values())  # char list\n",
        "    clients_allowed = []\n",
        "    for k in range(K):\n",
        "        allowed = random.sample(all_labels, min(Nc, len(all_labels)))\n",
        "        clients_allowed.append(set(allowed))\n",
        "\n",
        "    # Ensure each label appears at least once\n",
        "    label_to_clients = defaultdict(list)\n",
        "    for k, labels in enumerate(clients_allowed):\n",
        "        for lab in labels:\n",
        "            label_to_clients[lab].append(k)\n",
        "    for lab in all_labels:\n",
        "        if len(label_to_clients[lab]) == 0:\n",
        "            k = random.randrange(K)\n",
        "            clients_allowed[k].add(lab)\n",
        "            label_to_clients[lab].append(k)\n",
        "\n",
        "    # assign examples to clients that have that label in their allowed set\n",
        "    client_indices = [[] for _ in range(K)]\n",
        "    for i in range(len(train_dataset)):\n",
        "        _, y = train_dataset[i]\n",
        "        label_char = idx2char[int(y.item())]\n",
        "        eligible = label_to_clients[label_char]\n",
        "        chosen = random.choice(eligible)\n",
        "        client_indices[chosen].append(i)\n",
        "    return client_indices, clients_allowed\n",
        "\n",
        "\n",
        "def create_label_shards_by_role(train_dataset, K, Nc, seed=SEED):\n",
        "    print(\"kk\")\n",
        "\n",
        "    play_titles =     ['ALLS_WELL_THAT_ENDS_WELL', 'AS_YOU_LIKE_IT', 'THE_COMEDY_OF_ERRORS', 'LOVE_S_LABOUR_S_LOST', 'MEASURE_FOR_MEASURE', 'THE_MERCHANT_OF_VENICE', 'THE_MERRY_WIVES_OF_WINDSOR', 'A_MIDSUMMER_NIGHT_S_DREAM', 'MUCH_ADO_ABOUT_NOTHING', 'THE_TRAGEDY_OF_HAMLET__PRINCE_OF_DENMARK', 'THE_TAMING_OF_THE_SHREW', 'THE_TEMPEST', 'TWELFTH_NIGHT__OR__WHAT_YOU_WILL', 'THE_TWO_GENTLEMEN_OF_VERONA', 'THE_WINTER_S_TALE', 'KING_JOHN', 'SECOND_PART_OF_KING_HENRY_IV', 'THE_LIFE_OF_KING_HENRY_THE_FIFTH', 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH', 'THE_FIRST_PART_OF_HENRY_THE_SIXTH', 'THE_SECOND_PART_OF_KING_HENRY_THE_SIXTH', 'THE_THIRD_PART_OF_KING_HENRY_THE_SIXTH', 'KING_RICHARD_THE_SECOND', 'KING_RICHARD_III', 'KING_HENRY_THE_EIGHTH', 'THE_TRAGEDY_OF_ANTONY_AND_CLEOPATRA', 'THE_TRAGEDY_OF_CORIOLANUS', 'CYMBELINE', 'THE_TRAGEDY_OF_JULIUS_CAESAR', 'THE_TRAGEDY_OF_KING_LEAR', 'THE_TRAGEDY_OF_MACBETH', 'THE_TRAGEDY_OF_OTHELLO__MOOR_OF_VENICE', 'THE_TRAGEDY_OF_ROMEO_AND_JULIET', 'THE_LIFE_OF_TIMON_OF_ATHENS', 'THE_TRAGEDY_OF_TITUS_ANDRONICUS', 'THE_HISTORY_OF_TROILUS_AND_CRESSIDA']\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    role_index_map = {}\n",
        "    for idx, r in enumerate(train_dataset.hf[\"character_id\"]):\n",
        "        match = next((title for title in play_titles if r.startswith(title)), None)\n",
        "        if match:\n",
        "            stripped=r[len(match):].lstrip(\"_\")\n",
        "            if(stripped.startswith(\"OR__WHAT_YOU_WILL\")):   \n",
        "                print(\"match\", match)\n",
        "   \n",
        "        \n",
        "            if stripped not in role_index_map:\n",
        "                role_index_map[stripped] = []\n",
        "            role_index_map[stripped].append(idx)\n",
        "        else:\n",
        "            raise ValueError(f\"Role '{r}' does not start with any known play title.\")\n",
        "    \n",
        "\n",
        "    sorted_roles = sorted(role_index_map.keys(), key=lambda r: len(role_index_map[r]), reverse=True)\n",
        "\n",
        "    # 3. Assign roles to clients round-robin until each client has Nc classes\n",
        "    clients_allowed = [set() for _ in range(K)]\n",
        "    client_ptr = 0\n",
        "    num_roles = len(sorted_roles)\n",
        "    role_idx = 0\n",
        "    while not all(len(c) >= Nc for c in clients_allowed):\n",
        "        role = sorted_roles[role_idx % num_roles]  # cycle through roles if needed\n",
        "\n",
        "        # assign role only if client still needs roles\n",
        "        if len(clients_allowed[client_ptr]) < Nc:\n",
        "            clients_allowed[client_ptr].add(role)\n",
        "\n",
        "    # move to next client\n",
        "        client_ptr = (client_ptr + 1) % K\n",
        "        role_idx += 1\n",
        "\n",
        "        \n",
        "    # 4. Build reverse mapping role â†’ eligible clients\n",
        "    role_to_clients = defaultdict(list)\n",
        "    for k, roles in enumerate(clients_allowed):\n",
        "        for role in roles:\n",
        "            role_to_clients[role].append(k)\n",
        "\n",
        "    # 5. Assign dataset indices to clients\n",
        "    client_indices = [[] for _ in range(K)]\n",
        "    for role, indices in role_index_map.items():\n",
        "        if role not in role_to_clients:\n",
        "            continue  # skip if role not assigned\n",
        "        eligible = role_to_clients[role]\n",
        "        for client_id in eligible:\n",
        "            client_indices[client_id].extend(indices)\n",
        "\n",
        "    \n",
        "    rng = random.Random(seed)\n",
        "    \n",
        "    for client_list in client_indices:\n",
        "        rng.shuffle(client_list)\n",
        "\n",
        "    return client_indices, clients_allowed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- main experiment loop: uniform vs skewed ----------------\n",
        "\n",
        "\n",
        "def run_global(\n",
        "    J,\n",
        "      shards,\n",
        "    gamma=\"uniform\",        # \"uniform\" or float\n",
        "    K=K,\n",
        "    C=C,\n",
        "    rounds_global=ROUNDS,\n",
        "    client_batch=CLIENT_BATCH,\n",
        "    lr_local=LR_LOCAL,\n",
        "    device=DEVICE,\n",
        "    log_every=log_every,\n",
        "    name=\"shakespeare\",\n",
        "    topk=topk,\n",
        "    plot=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Run FedAvg with a single gamma and local steps J.\n",
        "\n",
        "    Args:\n",
        "        J (int): Number of local steps per client.\n",
        "        gamma (str|float): \"uniform\" for uniform sampling, or positive float for skewed sampling.\n",
        "        plot (bool): Whether to plot validation metrics and selection counts.\n",
        "\n",
        "    Returns:\n",
        "        dict: Results dictionary with losses, accuracies, selection counts, etc.\n",
        "    \"\"\"\n",
        "    # check gamma validity\n",
        "    if gamma != \"uniform\" and not (isinstance(gamma, (int, float)) and gamma > 0):\n",
        "        raise ValueError(\"gamma must be 'uniform' or a positive float\")\n",
        "\n",
        "    # rounds adjusted to keep total local updates ~ constant\n",
        "   \n",
        "    scheme_name = f\"uniform_J{J}\" if gamma == \"uniform\" else f\"skewed_gamma{gamma}_J{J}\"\n",
        "\n",
        "    print(f\"\\n=== Experiment: {scheme_name} | rounds={rounds_global}, local_steps={J} ===\")\n",
        "\n",
        "    # fresh global model\n",
        "    global_model = CharLSTM(VOCAB_SIZE).to(device)\n",
        "    global_state = global_model.state_dict()\n",
        "    val_losses, val_accs = [], []\n",
        "    sel_counts = np.zeros(K, dtype=int)\n",
        "    pvec = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ckpt_dir=\"checkpoint_shakespeare\"\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    start_round=1\n",
        "\n",
        "\n",
        "    ckpt_files = [f for f in os.listdir(ckpt_dir) if f.startswith(f\"{name}_round_\")]\n",
        "    if ckpt_files:\n",
        "            latest = max(ckpt_files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "            resume_ckpt = os.path.join(ckpt_dir, latest)\n",
        "            print(f\"Resuming from {resume_ckpt}\")\n",
        "            ckpt = torch.load(resume_ckpt, map_location=device, weights_only=False)\n",
        "\n",
        "            # restore model\n",
        "            global_model.load_state_dict(ckpt[\"model_state\"])\n",
        "            global_state = global_model.state_dict()\n",
        "\n",
        "            # restore histories\n",
        "            val_losses = ckpt[\"val_losses\"]\n",
        "            val_accs = ckpt[\"val_accs\"]\n",
        "            sel_counts = np.array(ckpt[\"sel_counts\"])\n",
        "            pvec = ckpt[\"pvec\"]\n",
        "\n",
        "            # restore RNG states\n",
        "            rng_state = ckpt[\"rng_state\"]\n",
        "            if isinstance(rng_state, torch.ByteTensor):\n",
        "                rng_state = rng_state.clone().detach().cpu()\n",
        "            else:\n",
        "                rng_state = torch.as_tensor(rng_state, dtype=torch.uint8, device=\"cpu\")\n",
        "            torch.set_rng_state(rng_state)\n",
        "\n",
        "            if torch.cuda.is_available() and ckpt.get(\"cuda_rng_state\") is not None:\n",
        "                cuda_states = []\n",
        "                for s in ckpt[\"cuda_rng_state\"]:\n",
        "                    if isinstance(s, torch.ByteTensor):\n",
        "                        s = s.clone().detach().cpu()\n",
        "                    else:\n",
        "                        s = torch.as_tensor(s, dtype=torch.uint8, device=\"cpu\")\n",
        "                    cuda_states.append(s)\n",
        "                torch.cuda.set_rng_state_all(cuda_states)\n",
        "\n",
        "            # figure out resume round\n",
        "            start_round = int(latest.split(\"_\")[-1].split(\".\")[0]) + 1\n",
        "            print(f\"Resuming at round {start_round}\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    for r in range(start_round, rounds_global + 1):\n",
        "        # --- client sampling ---\n",
        "        if gamma == \"uniform\":\n",
        "            selected = random.sample(range(K), CLIENTS_PER_ROUND)\n",
        "            pvec = np.ones(K) / K\n",
        "        else:\n",
        "            selected, pvec = sample_skewed_clients(K, C, gamma)\n",
        "            if pvec is None or np.isnan(pvec).any() or pvec.sum() == 0:\n",
        "                pvec = np.ones(K) / K\n",
        "                selected = random.sample(range(K), CLIENTS_PER_ROUND)\n",
        "\n",
        "        for s in selected:\n",
        "            sel_counts[s] += 1\n",
        "\n",
        "        # --- logging ---\n",
        "        \"\"\"if (r % log_every) == 0:\n",
        "            #p_min, p_max, p_sum = float(np.min(pvec)), float(np.max(pvec)), float(np.sum(pvec))\n",
        "             selected\n",
        "            print(f\"[{scheme_name}] Round {r}/{rounds_global}  p_min={p_min:.4e}, p_max={p_max:.4e}, p_sum={p_sum:.4f}\")\n",
        "            print(\"  selected this round (first 20):\", selected[:20], f\"(total={len(selected)})\")\n",
        "            print(\"-\" * 60) \"\"\"\n",
        "\n",
        "        # --- local updates ---\n",
        "        local_states, local_sizes = [], []\n",
        "        for cid in selected:\n",
        "            idx_list = shards[cid]\n",
        "            if len(idx_list) == 0:\n",
        "                continue\n",
        "            s_state, s_size = client_update(global_state, idx_list, train_ds,\n",
        "                                            steps=J, batch_size=client_batch, lr=lr_local, device=device)\n",
        "            if s_state is None:\n",
        "                continue\n",
        "            local_states.append(s_state)\n",
        "            local_sizes.append(s_size)\n",
        "\n",
        "        # --- aggregation ---\n",
        "        if len(local_states) > 0:\n",
        "            new_state = aggregate_states(local_states, local_sizes)\n",
        "            global_state = {k: new_state[k].to(device) for k in new_state}\n",
        "            global_model.load_state_dict(global_state)\n",
        "        else:\n",
        "            print(\"Warning: no client contributions this round; skipping aggregation.\")\n",
        "\n",
        "        # --- evaluation ---\n",
        "        val_loss, val_acc = evaluate(global_model, val_ds, device=device)\n",
        "        test_loss, test_acc = evaluate(global_model, test_ds, device=device)\n",
        "\n",
        "        # train acc/loss only every 10 rounds\n",
        "        if r % 10 == 0:\n",
        "            train_loss, train_acc = evaluate(global_model, train_ds, device=device)\n",
        "        else:\n",
        "            train_loss, train_acc = -1, -1\n",
        "\n",
        "        csv_path=f'./shakespeare_with_name.csv'\n",
        "\n",
        "        #csv_path_res = f'./'+name+'_results.csv'\n",
        "        if not os.path.exists(csv_path):\n",
        "            with open(csv_path, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(['name','round',\"sel_counts\", 'val_loss', 'val_acc', 'test_loss', 'test_acc','train_loss', 'train_acc'])\n",
        "\n",
        "\n",
        "        with open(csv_path, 'a', newline='') as f:\n",
        "            csv.writer(f).writerow([\n",
        "                name,\n",
        "                r,\n",
        "                json.dumps(sel_counts.tolist()),\n",
        "\n",
        "                f\"{val_loss:.4f}\", f\"{val_acc:.4f}\",\n",
        "                f\"{test_loss:.4f}\", f\"{test_acc:.4f}\",\n",
        "                 f\"{train_loss:.4f}\",  f\"{train_acc:.4f}\"\n",
        "            ])\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        save_every=25\n",
        "\n",
        "\n",
        "        if (r % save_every == 0) or (r == rounds_global):\n",
        "            ckpt = {\n",
        "                \"round\": r,\n",
        "                \"model_state\": global_model.state_dict(),\n",
        "                \"val_losses\": val_losses,\n",
        "                \"val_accs\": val_accs,\n",
        "                \"sel_counts\": sel_counts.tolist(),\n",
        "                \"pvec\": pvec,\n",
        "                \"rng_state\": torch.get_rng_state(),\n",
        "                \"cuda_rng_state\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "                \"gamma\": gamma,\n",
        "                \"J\": J\n",
        "            }\n",
        "            ckpt_path = os.path.join(ckpt_dir, f\"{name}_round_{r}.pth\")\n",
        "            torch.save(ckpt, ckpt_path)\n",
        "            print(f\"Saved checkpoint -> {ckpt_path}\")\n",
        "            for f in os.listdir(ckpt_dir):\n",
        "                if f.startswith(f\"{name}_round_\") and f.endswith(\".pth\"):\n",
        "                    f_path = os.path.join(ckpt_dir, f)\n",
        "                    if f_path != ckpt_path:\n",
        "                        os.remove(f_path)\n",
        "                        print(f\"Deleted old checkpoint {f_path}\")\n",
        "\n",
        "        if r % max(1, rounds_global // 5) == 0 or r in [1, rounds_global]:\n",
        "            print(f\"[{scheme_name}] Round {r}/{rounds_global}  val_acc={val_acc:.4f}  val_loss={val_loss:.4f}\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"Finished {scheme_name} in {elapsed:.1f}s  final_val_acc={val_accs[-1]:.4f}\")\n",
        "\n",
        "    # --- save results ---\n",
        "    results = {\n",
        "        'rounds': rounds_global,\n",
        "        'J': J,\n",
        "        'gamma': gamma,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "        'sel_counts': sel_counts,\n",
        "        'pvec_last_round': pvec,\n",
        "        'time_s': elapsed\n",
        "    }\n",
        "\n",
        "    # --- plotting ---\n",
        "    if plot:\n",
        "        plt.figure(figsize=(8,3))\n",
        "        plt.subplot(1,2,1); plt.plot(val_accs); plt.title(f\"{scheme_name} val_acc\"); plt.xlabel('round')\n",
        "        plt.subplot(1,2,2); plt.plot(val_losses); plt.title(f\"{scheme_name} val_loss\"); plt.xlabel('round')\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        if gamma != \"uniform\":\n",
        "            plt.figure(figsize=(8,3))\n",
        "            plt.bar(np.arange(K), sel_counts)\n",
        "            plt.title(f\"Selection counts per client ({scheme_name})\")\n",
        "            plt.xlabel(\"client id\"); plt.ylabel(\"times selected\")\n",
        "            plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "results = {}\n",
        "device = DEVICE\n",
        "round_base=500\n",
        "schemes = ['uniform', 'skewed']\n",
        "\n",
        "GAMMAS= [ \"uniform\"] \n",
        "LOCAL_STEPS_LIST = [4,8,16]\n",
        "\n",
        "\n",
        "\n",
        "for nc in [1]:\n",
        "                    # IID shards\n",
        "    if nc==\"iid\":\n",
        "\n",
        "        shards = iid_shards_by_chunk_indices(len(train_ds), num_clients=K, seed=SEED)\n",
        "    \n",
        "    else:\n",
        "                    \n",
        "\n",
        "        shards,realNc=create_label_shards_by_role(train_ds, idx2char, K, nc, seed=SEED)\n",
        "\n",
        "    for gamma in GAMMAS:\n",
        "\n",
        "            for J in LOCAL_STEPS_LIST:\n",
        "\n",
        "                \n",
        "                global_rounds = max(1, math.ceil((round_base * 4) / float(J)))\n",
        "                res_uniform = run_global(name=f\"gamma_{str(gamma)}_nc_{str(nc)}_J_{J}\",J=J, gamma=gamma,rounds_global=global_rounds,shards=shards)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advanced-machine-learning-labs-ROHCwjr8-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
