{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Shakespeare dataset (this may take a few seconds)...\n",
            "Train size: 3803542, Val size: 211308, Vocab size: 80\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'dataset_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     80\u001b[39m     val_ds = ShakespeareDataset(val_hf, char2idx)\n\u001b[32m     81\u001b[39m         \u001b[38;5;66;03m# wrap test dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     test_ds = ShakespeareDataset(\u001b[43mdataset_test\u001b[49m, char2idx)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mShakespeare dataset already loaded; skipping reload.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'dataset_test' is not defined"
          ]
        }
      ],
      "source": [
        "# ---------------- Full runnable Colab cell (copy-paste) ----------------\n",
        "import random, math, time\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------------- settings (tune these) ----------------\n",
        "K = 100                    # number of clients\n",
        "C = 0.1                    # fraction of clients per round\n",
        "CLIENTS_PER_ROUND = max(1, int(math.ceil(C * K)))\n",
        "ROUNDS = 200                # number of federated rounds per experiment\n",
        "LOCAL_STEPS_LIST = [1, 4, 8, 16]\n",
        "BASE_J = 4\n",
        "CLIENT_BATCH = 16\n",
        "EMB_DIM = 32\n",
        "HIDDEN_DIM = 128\n",
        "LR_LOCAL = 1e-3\n",
        "SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "GAMMAS = [0.01, 0.1, 1.0, 10.0]  # dirichlet concentration values for skewed scheme\n",
        "log_every = 1      # print participation summary every `log_every` rounds\n",
        "topk = 5\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ---------------- load dataset & build vocab ----------------\n",
        "def iid_shards_by_chunk_indices(dataset_len, num_clients=100, seed=42):\n",
        "    idxs = list(range(dataset_len))\n",
        "    random.Random(seed).shuffle(idxs)\n",
        "    shards = [[] for _ in range(num_clients)]\n",
        "    for i, idx in enumerate(idxs):\n",
        "        shards[i % num_clients].append(idx)\n",
        "    return shards\n",
        "if \"train_ds\" not in globals() or \"val_ds\" not in globals() or \"test_ds\" not in globals():\n",
        "    print(\"Loading Shakespeare dataset (this may take a few seconds)...\")\n",
        "    hf = load_dataset(\"flwrlabs/shakespeare\", split=\"train\").shuffle(seed=SEED)\n",
        "   \n",
        "\n",
        "\n",
        "    # train/val split\n",
        "    parts = hf.train_test_split(test_size=0.1, seed=SEED)  # 10% for val+test\n",
        "    train_hf = parts['train']\n",
        "\n",
        "    # split the 10% leftover into val and test (50-50)\n",
        "    val_test_hf = parts['test'].train_test_split(test_size=0.5, seed=SEED)\n",
        "    val_hf = val_test_hf['train']\n",
        "    test_hf = val_test_hf['test']\n",
        "\n",
        "    # build vocab from training split\n",
        "    chars = sorted({c for txt in train_hf['x'] for c in txt} | set(train_hf['y']))\n",
        "    PAD_TOKEN = \"<pad>\"\n",
        "    vocab = [PAD_TOKEN] + chars\n",
        "    char2idx = {c: i for i, c in enumerate(vocab)}\n",
        "    idx2char = {i: c for c, i in char2idx.items()}\n",
        "    VOCAB_SIZE = len(vocab)\n",
        "    print(f\"Train size: {len(train_hf)}, Val size: {len(val_hf)}, Vocab size: {VOCAB_SIZE}\")\n",
        "\n",
        "    # dataset wrapper\n",
        "    class ShakespeareDataset(Dataset):\n",
        "        def __init__(self, hf_dataset, char2idx):\n",
        "            self.hf = hf_dataset\n",
        "            self.char2idx = char2idx\n",
        "        def __len__(self):\n",
        "            return len(self.hf)\n",
        "        def __getitem__(self, idx):\n",
        "            item = self.hf[int(idx)]\n",
        "            x_str, y_str = item['x'], item['y']\n",
        "            x_idx = torch.tensor([self.char2idx[ch] for ch in x_str], dtype=torch.long)\n",
        "            y_idx = torch.tensor(self.char2idx[y_str], dtype=torch.long)\n",
        "            return x_idx, y_idx\n",
        "\n",
        "    train_ds = ShakespeareDataset(train_hf, char2idx)\n",
        "    val_ds = ShakespeareDataset(val_hf, char2idx)\n",
        "        # wrap test dataset\n",
        "    test_ds = ShakespeareDataset(test_hf, char2idx)\n",
        "\n",
        "    \n",
        "else:\n",
        "    print(\"Shakespeare dataset already loaded; skipping reload.\")\n",
        "\n",
        "\n",
        "# ---------------- model definition ----------------\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=EMB_DIM, hid=HIDDEN_DIM, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n",
        "        self.fc = nn.Linear(hid, vocab_size)\n",
        "    def forward(self, x):\n",
        "        e = self.embed(x)\n",
        "        out, _ = self.lstm(e)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "# ---------------- evaluate helper ----------------\n",
        "def evaluate(model, dataset, device=DEVICE, batch_size=256):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss = 0.0; total = 0; correct = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += xb.size(0)\n",
        "    return total_loss / total, correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XAOd3qTCGNE7",
        "outputId": "beedb14b-72d4-42be-d048-2185d4f8a161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Experiment: skewed_gamma0.01_J4 | rounds=500, local_steps=4 ===\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'test_ds' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 369\u001b[39m\n\u001b[32m    365\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m J \u001b[38;5;129;01min\u001b[39;00m LOCAL_STEPS_LIST:\n\u001b[32m    368\u001b[39m                 global_rounds = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, math.ceil((round_base * \u001b[32m4\u001b[39m) / \u001b[38;5;28mfloat\u001b[39m(J)))\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m                 res_uniform = \u001b[43mrun_global\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_nc_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_J_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mJ\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mJ\u001b[49m\u001b[43m=\u001b[49m\u001b[43mJ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrounds_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[33;03m\"\"\"             t1 = time.time()\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m            elapsed = t1 - t0\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m            print(f\"Finished {scheme_name} in {elapsed:.1f}s  final_val_acc={val_accs[-1]:.4f}\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m \u001b[33;03m                plt.show()\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m \"\"\"\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 251\u001b[39m, in \u001b[36mrun_global\u001b[39m\u001b[34m(J, shards, gamma, K, C, rounds_global, client_batch, lr_local, device, log_every, name, topk, plot)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# --- evaluation ---\u001b[39;00m\n\u001b[32m    250\u001b[39m val_loss, val_acc = evaluate(global_model, val_ds, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m test_loss, test_acc = evaluate(global_model, \u001b[43mtest_ds\u001b[49m, device=device)\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# train acc/loss only every 10 rounds\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
            "\u001b[31mNameError\u001b[39m: name 'test_ds' is not defined"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import json\n",
        "# ---------------- client local update (expects dataset returning (x_tensor,y_tensor)) ----------------\n",
        "def client_update(global_state, client_idx_list, train_dataset, steps, batch_size=CLIENT_BATCH, lr=LR_LOCAL, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Local client training starting from global_state.\n",
        "    Returns (state_cpu_dict, num_examples).\n",
        "    Assumes train_dataset[i] returns (x_tensor, y_tensor).\n",
        "    \"\"\"\n",
        "    if len(client_idx_list) == 0:\n",
        "        return None, 0\n",
        "\n",
        "    # DataLoader for this client's subset\n",
        "    subset = Subset(train_dataset, client_idx_list)\n",
        "    loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # local model init\n",
        "    local_model = CharLSTM(VOCAB_SIZE).to(device)\n",
        "    local_model.load_state_dict(global_state)\n",
        "    opt = optim.Adam(local_model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    it = iter(loader)\n",
        "    steps_done = 0\n",
        "    while steps_done < steps:\n",
        "        try:\n",
        "            xb, yb = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            xb, yb = next(it)\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = local_model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=1.0)\n",
        "        opt.step()\n",
        "        steps_done += 1\n",
        "\n",
        "    state_cpu = {k: v.cpu().clone() for k, v in local_model.state_dict().items()}\n",
        "    return state_cpu, len(client_idx_list)\n",
        "\n",
        "# ---------------- aggregation helper ----------------\n",
        "def aggregate_states(local_states, local_sizes):\n",
        "    \"\"\"Weighted average of CPU state dicts.\"\"\"\n",
        "    new_state = {}\n",
        "    total = float(sum(local_sizes))\n",
        "    keys = local_states[0].keys()\n",
        "    for k in keys:\n",
        "        new_state[k] = torch.zeros_like(local_states[0][k], dtype=local_states[0][k].dtype)\n",
        "        for s, sz in zip(local_states, local_sizes):\n",
        "            new_state[k] += s[k] * (sz / total)\n",
        "    return new_state\n",
        "\n",
        "# ---------------- sampling function (Dirichlet skew) ----------------\n",
        "def sample_skewed_clients(K, C, gamma):\n",
        "    \"\"\"\n",
        "    Draw Dirichlet probabilities and sample clients without replacement.\n",
        "    Defensively handles numerical issues by falling back to uniform p if needed.\n",
        "    Returns (selected_list, pvec)\n",
        "    \"\"\"\n",
        "    if gamma is None or not (isinstance(gamma, (int, float)) and gamma > 0):\n",
        "        raise ValueError(\"gamma must be a positive float for skewed sampling\")\n",
        "\n",
        "    p = np.random.dirichlet([gamma] * K)\n",
        "    # defensive normalization / fallback\n",
        "    if np.isnan(p).any() or p.sum() == 0:\n",
        "        p = np.ones(K) / K\n",
        "    else:\n",
        "        p = p / p.sum()\n",
        "\n",
        "    num_sel = max(1, int(math.ceil(C * K)))\n",
        "    selected = list(np.random.choice(np.arange(K), size=num_sel, replace=False, p=p))\n",
        "    return selected, p\n",
        "\n",
        "# ---------------- optional: non-iid shard creator (by target label count Nc) ----------------\n",
        "def create_label_shards(train_dataset, idx2char, K, Nc, seed=SEED):\n",
        "    \"\"\"\n",
        "    Create client shards where each client has at most Nc distinct target labels.\n",
        "    Returns client_indices list-of-lists and clients_allowed (labels per client).\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    all_labels = list(idx2char.values())  # char list\n",
        "    clients_allowed = []\n",
        "    for k in range(K):\n",
        "        allowed = random.sample(all_labels, min(Nc, len(all_labels)))\n",
        "        clients_allowed.append(set(allowed))\n",
        "\n",
        "    # Ensure each label appears at least once\n",
        "    label_to_clients = defaultdict(list)\n",
        "    for k, labels in enumerate(clients_allowed):\n",
        "        for lab in labels:\n",
        "            label_to_clients[lab].append(k)\n",
        "    for lab in all_labels:\n",
        "        if len(label_to_clients[lab]) == 0:\n",
        "            k = random.randrange(K)\n",
        "            clients_allowed[k].add(lab)\n",
        "            label_to_clients[lab].append(k)\n",
        "\n",
        "    # assign examples to clients that have that label in their allowed set\n",
        "    client_indices = [[] for _ in range(K)]\n",
        "    for i in range(len(train_dataset)):\n",
        "        _, y = train_dataset[i]\n",
        "        label_char = idx2char[int(y.item())]\n",
        "        eligible = label_to_clients[label_char]\n",
        "        chosen = random.choice(eligible)\n",
        "        client_indices[chosen].append(i)\n",
        "    return client_indices, clients_allowed\n",
        "\n",
        "# ---------------- main experiment loop: uniform vs skewed ----------------\n",
        "\n",
        "\n",
        "def run_global(\n",
        "    J,\n",
        "      shards,\n",
        "    gamma=\"uniform\",        # \"uniform\" or float\n",
        "    K=K,\n",
        "    C=C,\n",
        "    rounds_global=ROUNDS,\n",
        "    client_batch=CLIENT_BATCH,\n",
        "    lr_local=LR_LOCAL,\n",
        "    device=DEVICE,\n",
        "    log_every=log_every,\n",
        "    name=\"shakespeare\",\n",
        "    topk=topk,\n",
        "    plot=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Run FedAvg with a single gamma and local steps J.\n",
        "\n",
        "    Args:\n",
        "        J (int): Number of local steps per client.\n",
        "        gamma (str|float): \"uniform\" for uniform sampling, or positive float for skewed sampling.\n",
        "        plot (bool): Whether to plot validation metrics and selection counts.\n",
        "\n",
        "    Returns:\n",
        "        dict: Results dictionary with losses, accuracies, selection counts, etc.\n",
        "    \"\"\"\n",
        "    # check gamma validity\n",
        "    if gamma != \"uniform\" and not (isinstance(gamma, (int, float)) and gamma > 0):\n",
        "        raise ValueError(\"gamma must be 'uniform' or a positive float\")\n",
        "\n",
        "    # rounds adjusted to keep total local updates ~ constant\n",
        "   \n",
        "    scheme_name = f\"uniform_J{J}\" if gamma == \"uniform\" else f\"skewed_gamma{gamma}_J{J}\"\n",
        "\n",
        "    print(f\"\\n=== Experiment: {scheme_name} | rounds={rounds_global}, local_steps={J} ===\")\n",
        "\n",
        "    # fresh global model\n",
        "    global_model = CharLSTM(VOCAB_SIZE).to(device)\n",
        "    global_state = global_model.state_dict()\n",
        "    val_losses, val_accs = [], []\n",
        "    sel_counts = np.zeros(K, dtype=int)\n",
        "    pvec = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ckpt_dir=\"checkpoint_shakespeare\"\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    start_round=1\n",
        "\n",
        "\n",
        "    ckpt_files = [f for f in os.listdir(ckpt_dir) if f.startswith(f\"{name}_round_\")]\n",
        "    if ckpt_files:\n",
        "            latest = max(ckpt_files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "            resume_ckpt = os.path.join(ckpt_dir, latest)\n",
        "            print(f\"Resuming from {resume_ckpt}\")\n",
        "            ckpt = torch.load(resume_ckpt, map_location=device, weights_only=False)\n",
        "\n",
        "            # restore model\n",
        "            global_model.load_state_dict(ckpt[\"model_state\"])\n",
        "            global_state = global_model.state_dict()\n",
        "\n",
        "            # restore histories\n",
        "            val_losses = ckpt[\"val_losses\"]\n",
        "            val_accs = ckpt[\"val_accs\"]\n",
        "            sel_counts = np.array(ckpt[\"sel_counts\"])\n",
        "            pvec = ckpt[\"pvec\"]\n",
        "\n",
        "            # restore RNG states\n",
        "            rng_state = ckpt[\"rng_state\"]\n",
        "            if isinstance(rng_state, torch.ByteTensor):\n",
        "                rng_state = rng_state.clone().detach().cpu()\n",
        "            else:\n",
        "                rng_state = torch.as_tensor(rng_state, dtype=torch.uint8, device=\"cpu\")\n",
        "            torch.set_rng_state(rng_state)\n",
        "\n",
        "            if torch.cuda.is_available() and ckpt.get(\"cuda_rng_state\") is not None:\n",
        "                cuda_states = []\n",
        "                for s in ckpt[\"cuda_rng_state\"]:\n",
        "                    if isinstance(s, torch.ByteTensor):\n",
        "                        s = s.clone().detach().cpu()\n",
        "                    else:\n",
        "                        s = torch.as_tensor(s, dtype=torch.uint8, device=\"cpu\")\n",
        "                    cuda_states.append(s)\n",
        "                torch.cuda.set_rng_state_all(cuda_states)\n",
        "\n",
        "            # figure out resume round\n",
        "            start_round = int(latest.split(\"_\")[-1].split(\".\")[0]) + 1\n",
        "            print(f\"Resuming at round {start_round}\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    for r in range(start_round, rounds_global + 1):\n",
        "        # --- client sampling ---\n",
        "        if gamma == \"uniform\":\n",
        "            selected = random.sample(range(K), CLIENTS_PER_ROUND)\n",
        "            pvec = np.ones(K) / K\n",
        "        else:\n",
        "            selected, pvec = sample_skewed_clients(K, C, gamma)\n",
        "            if pvec is None or np.isnan(pvec).any() or pvec.sum() == 0:\n",
        "                pvec = np.ones(K) / K\n",
        "                selected = random.sample(range(K), CLIENTS_PER_ROUND)\n",
        "\n",
        "        for s in selected:\n",
        "            sel_counts[s] += 1\n",
        "\n",
        "        # --- logging ---\n",
        "        \"\"\"if (r % log_every) == 0:\n",
        "            #p_min, p_max, p_sum = float(np.min(pvec)), float(np.max(pvec)), float(np.sum(pvec))\n",
        "             selected\n",
        "            print(f\"[{scheme_name}] Round {r}/{rounds_global}  p_min={p_min:.4e}, p_max={p_max:.4e}, p_sum={p_sum:.4f}\")\n",
        "            print(\"  selected this round (first 20):\", selected[:20], f\"(total={len(selected)})\")\n",
        "            print(\"-\" * 60) \"\"\"\n",
        "\n",
        "        # --- local updates ---\n",
        "        local_states, local_sizes = [], []\n",
        "        for cid in selected:\n",
        "            idx_list = shards[cid]\n",
        "            if len(idx_list) == 0:\n",
        "                continue\n",
        "            s_state, s_size = client_update(global_state, idx_list, train_ds,\n",
        "                                            steps=J, batch_size=client_batch, lr=lr_local, device=device)\n",
        "            if s_state is None:\n",
        "                continue\n",
        "            local_states.append(s_state)\n",
        "            local_sizes.append(s_size)\n",
        "\n",
        "        # --- aggregation ---\n",
        "        if len(local_states) > 0:\n",
        "            new_state = aggregate_states(local_states, local_sizes)\n",
        "            global_state = {k: new_state[k].to(device) for k in new_state}\n",
        "            global_model.load_state_dict(global_state)\n",
        "        else:\n",
        "            print(\"Warning: no client contributions this round; skipping aggregation.\")\n",
        "\n",
        "        # --- evaluation ---\n",
        "        val_loss, val_acc = evaluate(global_model, val_ds, device=device)\n",
        "        test_loss, test_acc = evaluate(global_model, test_ds, device=device)\n",
        "\n",
        "        # train acc/loss only every 10 rounds\n",
        "        if r % 10 == 0:\n",
        "            train_loss, train_acc = evaluate(global_model, train_ds, device=device)\n",
        "        else:\n",
        "            train_loss, train_acc = -1, -1\n",
        "\n",
        "        csv_path=f'./shakespeare_with_name.csv'\n",
        "\n",
        "        #csv_path_res = f'./'+name+'_results.csv'\n",
        "        if not os.path.exists(csv_path):\n",
        "            with open(csv_path, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(['name','round',\"sel_counts\", 'val_loss', 'val_acc', 'test_loss', 'test_acc','train_loss', 'train_acc'])\n",
        "\n",
        "\n",
        "        with open(csv_path, 'a', newline='') as f:\n",
        "            csv.writer(f).writerow([\n",
        "                name,\n",
        "                r,\n",
        "                json.dumps(sel_counts.tolist()),\n",
        "\n",
        "                f\"{val_loss:.4f}\", f\"{val_acc:.4f}\",\n",
        "                f\"{test_loss:.4f}\", f\"{test_acc:.4f}\",\n",
        "                 f\"{train_loss:.4f}\",  f\"{train_acc:.4f}\"\n",
        "            ])\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        save_every=2\n",
        "\n",
        "\n",
        "        if (r % save_every == 0) or (r == rounds_global):\n",
        "            ckpt = {\n",
        "                \"round\": r,\n",
        "                \"model_state\": global_model.state_dict(),\n",
        "                \"val_losses\": val_losses,\n",
        "                \"val_accs\": val_accs,\n",
        "                \"sel_counts\": sel_counts.tolist(),\n",
        "                \"pvec\": pvec,\n",
        "                \"rng_state\": torch.get_rng_state(),\n",
        "                \"cuda_rng_state\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "                \"gamma\": gamma,\n",
        "                \"J\": J\n",
        "            }\n",
        "            ckpt_path = os.path.join(ckpt_dir, f\"{name}_round_{r}.pth\")\n",
        "            torch.save(ckpt, ckpt_path)\n",
        "            print(f\"Saved checkpoint -> {ckpt_path}\")\n",
        "            for f in os.listdir(ckpt_dir):\n",
        "                if f.startswith(f\"{name}_round_\") and f.endswith(\".pth\"):\n",
        "                    f_path = os.path.join(ckpt_dir, f)\n",
        "                    if f_path != ckpt_path:\n",
        "                        os.remove(f_path)\n",
        "                        print(f\"Deleted old checkpoint {f_path}\")\n",
        "\n",
        "        if r % max(1, rounds_global // 5) == 0 or r in [1, rounds_global]:\n",
        "            print(f\"[{scheme_name}] Round {r}/{rounds_global}  val_acc={val_acc:.4f}  val_loss={val_loss:.4f}\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"Finished {scheme_name} in {elapsed:.1f}s  final_val_acc={val_accs[-1]:.4f}\")\n",
        "\n",
        "    # --- save results ---\n",
        "    results = {\n",
        "        'rounds': rounds_global,\n",
        "        'J': J,\n",
        "        'gamma': gamma,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "        'sel_counts': sel_counts,\n",
        "        'pvec_last_round': pvec,\n",
        "        'time_s': elapsed\n",
        "    }\n",
        "\n",
        "    # --- plotting ---\n",
        "    if plot:\n",
        "        plt.figure(figsize=(8,3))\n",
        "        plt.subplot(1,2,1); plt.plot(val_accs); plt.title(f\"{scheme_name} val_acc\"); plt.xlabel('round')\n",
        "        plt.subplot(1,2,2); plt.plot(val_losses); plt.title(f\"{scheme_name} val_loss\"); plt.xlabel('round')\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        if gamma != \"uniform\":\n",
        "            plt.figure(figsize=(8,3))\n",
        "            plt.bar(np.arange(K), sel_counts)\n",
        "            plt.title(f\"Selection counts per client ({scheme_name})\")\n",
        "            plt.xlabel(\"client id\"); plt.ylabel(\"times selected\")\n",
        "            plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "results = {}\n",
        "device = DEVICE\n",
        "round_base=500\n",
        "schemes = ['uniform', 'skewed']\n",
        "\n",
        "GAMMAS= [0.01,1,10, \"uniform\"] \n",
        "LOCAL_STEPS_LIST = [4,8,16]\n",
        "for nc in [\"iid\"]:\n",
        "                    # IID shards\n",
        "    if nc==\"iid\":\n",
        "\n",
        "        shards = iid_shards_by_chunk_indices(len(train_ds), num_clients=K, seed=SEED)\n",
        "    \n",
        "    else:\n",
        "                    \n",
        "\n",
        "        shards,realNc=create_label_shards(train_ds, idx2char, K, nc, seed=SEED)\n",
        "\n",
        "    for gamma in GAMMAS:\n",
        "\n",
        "            for J in LOCAL_STEPS_LIST:\n",
        "\n",
        "                \n",
        "                global_rounds = max(1, math.ceil((round_base * 4) / float(J)))\n",
        "                res_uniform = run_global(name=f\"gamma_{str(gamma)}_nc_{str(nc)}_J_{J}\",J=J, gamma=gamma,rounds_global=global_rounds,shards=shards)\n",
        "\n",
        "\"\"\"             t1 = time.time()\n",
        "            elapsed = t1 - t0\n",
        "            print(f\"Finished {scheme_name} in {elapsed:.1f}s  final_val_acc={val_accs[-1]:.4f}\")\n",
        "\n",
        "            # save results for this run\n",
        "            results[scheme_name] = {\n",
        "                'rounds': rounds,\n",
        "                'J': J,\n",
        "                'scheme': scheme,\n",
        "                'gamma': gamma,\n",
        "                'val_losses': val_losses,\n",
        "                'val_accs': val_accs,\n",
        "                'sel_counts': sel_counts,\n",
        "                'pvec_last_round': pvec,\n",
        "                'time_s': elapsed\n",
        "            }\n",
        "\n",
        "            # quick plots per run (optional)\n",
        "            plt.figure(figsize=(8,3))\n",
        "            plt.subplot(1,2,1)\n",
        "            plt.plot(val_accs, label='val_acc'); plt.title(f\"{scheme_name} val_acc\"); plt.xlabel('round')\n",
        "            plt.subplot(1,2,2)\n",
        "            plt.plot(val_losses, label='val_loss'); plt.title(f\"{scheme_name} val_loss\"); plt.xlabel('round')\n",
        "            plt.tight_layout(); plt.show()\n",
        "\n",
        "            if scheme == 'skewed':\n",
        "                plt.figure(figsize=(8,3))\n",
        "                plt.bar(np.arange(K), sel_counts)\n",
        "                plt.title(f\"Selection counts per client ({scheme_name})\")\n",
        "                plt.xlabel(\"client id\"); plt.ylabel(\"times selected\")\n",
        "                plt.show()\n",
        " \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JDfMK-mGNGL"
      },
      "outputs": [],
      "source": [
        "# ---------------- Nc experiments cell ----------------\n",
        "import math, time\n",
        "import pandas as pd\n",
        "\n",
        "# Experiment parameters (can be tuned)\n",
        "Nc_values = [1, 5, 10, 50]      # non-iid label counts per client\n",
        "J_values = [1, 4, 8, 16]           # local steps per selected client\n",
        "C = 0.1                         # fraction of clients per round\n",
        "m = max(1, int(math.ceil(C * K)))   # clients per round (should be 10)\n",
        "base_rounds_ref = 200            # reference rounds used to compute total local updates in baseline (50 rounds at J=4)\n",
        "\n",
        "# storage\n",
        "nc_results = {}   # (Nc_or_iid, J) -> dict with summary and full history\n",
        "\n",
        "print(f\"Running Nc experiments: K={K}, clients/round={m}; base reference rounds={base_rounds_ref}\")\n",
        "\n",
        "for Nc in (Nc_values + ['iid']):\n",
        "    # Build shards\n",
        "    if Nc == 'iid':\n",
        "        shards = client_indices_iid\n",
        "        label_str = 'iid'\n",
        "    else:\n",
        "        # create_label_shards(train_dataset, idx2char, K, Nc)\n",
        "        shards, clients_allowed = create_label_shards(train_ds, idx2char, K, Nc, seed=SEED)\n",
        "        label_str = f'Nc{Nc}'\n",
        "    print(\"\\n--- Shard type:\", label_str, \" ---\")\n",
        "    for J in J_values:\n",
        "        # compute rounds so total local updates ~ constant across J\n",
        "        # base_total_updates = base_rounds_ref * m * 4  -> dividing by (m*J) gives rounds = ceil((base_rounds_ref * 4) / J)\n",
        "        R = max(1, math.ceil((base_rounds_ref * 4) / J))\n",
        "        print(f\"Running J={J} local steps => rounds R={R}\")\n",
        "        # fresh global model for this run\n",
        "        global_model = CharLSTM(VOCAB_SIZE).to(DEVICE)\n",
        "        global_state = global_model.state_dict()\n",
        "        val_losses = []\n",
        "        val_accs = []\n",
        "        sel_counts = np.zeros(K, dtype=int)\n",
        "\n",
        "        t0 = time.time()\n",
        "        for r in range(1, R + 1):\n",
        "            # Uniform sampling of clients each round (per your spec for this comparison)\n",
        "            selected = random.sample(range(K), m)\n",
        "            for s in selected:\n",
        "                sel_counts[s] += 1\n",
        "\n",
        "            # local updates\n",
        "            local_states = []\n",
        "            local_sizes = []\n",
        "            for cid in selected:\n",
        "                idx_list = shards[cid]\n",
        "                if len(idx_list) == 0:\n",
        "                    # skip empty client shards\n",
        "                    continue\n",
        "                s_state, s_size = client_update(global_state, idx_list, train_ds,\n",
        "                                                steps=J, batch_size=CLIENT_BATCH, lr=LR_LOCAL, device=DEVICE)\n",
        "                if s_state is None:\n",
        "                    continue\n",
        "                local_states.append(s_state)\n",
        "                local_sizes.append(s_size)\n",
        "\n",
        "            # If no client contributed (unlikely), skip aggregation\n",
        "            if len(local_states) == 0:\n",
        "                print(\"Warning: no client contributions this round, skipping aggregation.\")\n",
        "            else:\n",
        "                # aggregate and update global\n",
        "                new_state = aggregate_states(local_states, local_sizes)\n",
        "                # move to device\n",
        "                global_state = {k: new_state[k].to(DEVICE) for k in new_state}\n",
        "                global_model.load_state_dict(global_state)\n",
        "\n",
        "            # evaluate on validation\n",
        "            val_loss, val_acc = evaluate(global_model, val_ds, device=DEVICE)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            if (r % max(1, R//5) == 0) or r == 1 or r == R:\n",
        "                print(f\"[{label_str} J={J}] Round {r}/{R}: val_acc={val_acc:.4f}, val_loss={val_loss:.4f}\")\n",
        "\n",
        "        t1 = time.time()\n",
        "        elapsed = t1 - t0\n",
        "        final_acc = val_accs[-1] if len(val_accs) > 0 else None\n",
        "\n",
        "        # store results\n",
        "        nc_results[(label_str, J)] = {\n",
        "            'rounds': R,\n",
        "            'val_losses': val_losses,\n",
        "            'val_accs': val_accs,\n",
        "            'final_acc': final_acc,\n",
        "            'sel_counts': sel_counts,\n",
        "            'elapsed_s': elapsed\n",
        "        }\n",
        "        print(f\"Finished {label_str}, J={J}. Final val_acc={final_acc:.4f}. Time: {elapsed:.1f}s\")\n",
        "\n",
        "# ---------------- Print Summary table ----------------\n",
        "rows = []\n",
        "for (label_str, J), info in nc_results.items():\n",
        "    rows.append({'shard': label_str, 'J': J, 'rounds': info['rounds'], 'final_val_acc': info['final_acc'], 'time_s': info['elapsed_s']})\n",
        "df = pd.DataFrame(rows).sort_values(['shard','J'])\n",
        "print(\"\\nSummary (final validation accuracies):\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# ---------------- Optional: quick plots ----------------\n",
        "# plot final accuracies grouped by J\n",
        "plt.figure(figsize=(8,4))\n",
        "for J in J_values:\n",
        "    xs = []\n",
        "    ys = []\n",
        "    labels = []\n",
        "    for Nc in Nc_values + ['iid']:\n",
        "        key = (('iid' if Nc=='iid' else f'Nc{Nc}'), J)\n",
        "        if key in nc_results:\n",
        "            xs.append(str(key[0]))\n",
        "            ys.append(nc_results[key]['final_acc'])\n",
        "            labels.append(key[0])\n",
        "    plt.plot(labels, ys, marker='o', label=f'J={J}')\n",
        "plt.xlabel(\"Sharding (iid or Nc)\")\n",
        "plt.ylabel(\"Final val accuracy\")\n",
        "plt.title(\"Nc experiments: final validation accuracy vs shard type (by J)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# plot per-shard example: pick Nc=1 and plot val curve for each J\n",
        "example_shard = 'Nc1'\n",
        "plt.figure(figsize=(10,4))\n",
        "for J in J_values:\n",
        "    k = (example_shard, J)\n",
        "    if k in nc_results:\n",
        "        plt.plot(nc_results[k]['val_accs'], label=f'J={J}')\n",
        "plt.title(f\"Validation accuracy curves for {example_shard}\")\n",
        "plt.xlabel(\"round\")\n",
        "plt.ylabel(\"val acc\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# ---------------- end cell ----------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hexoUlVQGNHh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advanced-machine-learning-labs-ROHCwjr8-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
