{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lGxnD3UbdWmb"
   },
   "outputs": [],
   "source": [
    "# ── A. Imports & Reproducibility ────────────────────────────────────────────────\n",
    "import os\n",
    "import csv                                                  # For result logging :contentReference[oaicite:0]{index=0}\n",
    "import random                                               # For seeding :contentReference[oaicite:1]{index=1}\n",
    "import numpy as np                                          # For numeric ops :contentReference[oaicite:2]{index=2}\n",
    "import torch                                               # Core PyTorch :contentReference[oaicite:3]{index=3}\n",
    "import torch.nn as nn                                       # Neural-net modules :contentReference[oaicite:4]{index=4}\n",
    "import torch.nn.functional as F                             # Functional API :contentReference[oaicite:5]{index=5}\n",
    "import torch.optim as optim                                 # Optimizers :contentReference[oaicite:6]{index=6}\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR      # Scheduler :contentReference[oaicite:7]{index=7}\n",
    "from torch.utils.data import DataLoader, random_split       # Data loaders & splits :contentReference[oaicite:8]{index=8}\n",
    "import torchvision                                          # Datasets & transforms :contentReference[oaicite:9]{index=9}\n",
    "import torchvision.transforms as T                          # Transforms :contentReference[oaicite:10]{index=10}\n",
    "from torch.utils.tensorboard import SummaryWriter           # TensorBoard logging :contentReference[oaicite:11]{index=11}\n",
    "import matplotlib.pyplot as plt                             # Plotting :contentReference[oaicite:12]{index=12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "39Q3SwyBdWnP"
   },
   "outputs": [],
   "source": [
    "# Seed everything for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746450506102,
     "user": {
      "displayName": "Mattia Cappellino",
      "userId": "05271634366428679167"
     },
     "user_tz": -120
    },
    "id": "r0PBwz2GdWoD",
    "outputId": "e7c49bfd-f989-4ada-fc33-32a24f86bf1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ── B. Device ───────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")                             # Confirm GPU vs CPU :contentReference[oaicite:13]{index=13}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hocKqvhTdWpL"
   },
   "outputs": [],
   "source": [
    "# ── C. Data Preparation ─────────────────────────────────────────────────────────\n",
    "# Transforms\n",
    "transform_train = T.Compose([\n",
    "    T.RandomCrop(32, padding=4), T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "])\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6010,
     "status": "ok",
     "timestamp": 1746450512115,
     "user": {
      "displayName": "Mattia Cappellino",
      "userId": "05271634366428679167"
     },
     "user_tz": -120
    },
    "id": "1KsmNAdjdWqH",
    "outputId": "6c3bd418-5e4e-4961-ce02-87eafebe7687"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:47<00:00, 3.52MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Download & train/val/test split\n",
    "dataset_full = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "val_size = 5000\n",
    "train_size = len(dataset_full) - val_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(seed))\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9zukJAc0dWq0"
   },
   "outputs": [],
   "source": [
    "# ── D. Model Definition ─────────────────────────────────────────────────────────\n",
    "class LELeNetCIFAR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1   = nn.Linear(64*8*8, 384)\n",
    "        self.fc2   = nn.Linear(384, 192)\n",
    "        self.fc3   = nn.Linear(192, 100)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x)); x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0ambCIcmdWrf"
   },
   "outputs": [],
   "source": [
    "# ── E. Utilities: Train/Eval & Checkpointing ────────────────────────────────────\n",
    "def train_one_epoch(model, optimizer, criterion, loader):\n",
    "    model.train()\n",
    "    running_loss = correct = total = 0\n",
    "    for imgs, lbls in loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, lbls)\n",
    "        loss.backward(); optimizer.step()\n",
    "        running_loss += loss.item()*imgs.size(0)\n",
    "        correct += out.argmax(1).eq(lbls).sum().item()\n",
    "        total += lbls.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "def eval_model(model, criterion, loader):\n",
    "    model.eval()\n",
    "    running_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            out = model(imgs); loss = criterion(out, lbls)\n",
    "            running_loss += loss.item()*imgs.size(0)\n",
    "            correct += out.argmax(1).eq(lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "    return running_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ELDe-m-idWsO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Checkpoint saves model + optimizer + scheduler + RNG\\nckpt_dir = \\'./checkpoints\\'\\nos.makedirs(ckpt_dir, exist_ok=True)\\ndef save_checkpoint(model, optimizer, scheduler, epoch, is_best=False):\\n    fname = f\"{\\'best\\' if is_best else \\'last\\'}_ckpt_epoch_{epoch}.pth\"\\n    torch.save({\\n        \\'epoch\\': epoch,\\n        \\'model_state\\': model.state_dict(),\\n        \\'optim_state\\': optimizer.state_dict(),\\n        \\'sched_state\\': scheduler.state_dict(),\\n        \\'rng_state\\': torch.get_rng_state(),\\n    }, os.path.join(ckpt_dir, fname)) '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Checkpoint saves model + optimizer + scheduler + RNG\n",
    "ckpt_dir = './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, is_best=False):\n",
    "    fname = f\"{'best' if is_best else 'last'}_ckpt_epoch_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optim_state': optimizer.state_dict(),\n",
    "        'sched_state': scheduler.state_dict(),\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "    }, os.path.join(ckpt_dir, fname)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, torch, os\n",
    "\n",
    "\n",
    "def latest_ckpt(dirpath, pattern=\"last_ckpt__round_*.pth\"):\n",
    "    paths = glob.glob(os.path.join(dirpath, pattern))\n",
    "    if not paths:\n",
    "        return None\n",
    "    paths.sort(key=lambda p: int(p.rsplit(\"_\", 1)[1].split(\".\")[0]))\n",
    "    return paths[-1]\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, ckpt_dir,scheduler, resume=True,name=\"\"):\n",
    "    if not resume:\n",
    "        print(\"[Checkpoint] Starting training from scratch.\")\n",
    "        return 1\n",
    "    if name:\n",
    "        pattern='last_ckpt_'+name+'_round_*.pth'\n",
    "        ckpt_path = latest_ckpt(ckpt_dir,pattern)\n",
    "    else:\n",
    "        ckpt_path = latest_ckpt(ckpt_dir)\n",
    "    if ckpt_path is None:\n",
    "        print(\"[Checkpoint] No checkpoint found; training from scratch.\")\n",
    "        return 1\n",
    "    # Load checkpoint tensors onto CPU to preserve RNG state tensor\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "    scheduler.load_state_dict(ckpt['sched_state'])\n",
    "    # Restore CPU RNG state\n",
    "    rng_state = ckpt['rng_state']\n",
    "    if rng_state.device.type != 'cpu':\n",
    "        rng_state = rng_state.cpu()\n",
    "    torch.set_rng_state(rng_state)\n",
    "    print(f\"[Checkpoint] Resumed from round {ckpt['round']} (loaded {os.path.basename(ckpt_path)})\")\n",
    "    return ckpt['round'] + 1\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer,scheduler, round_num, ckpt_dir,personalized_par_string=\"\", is_best=False):\n",
    "    print(f\"[Checkpoint] Saving round {round_num}...\")\n",
    "    state = {\n",
    "        'round': round_num,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "          'sched_state': scheduler.state_dict(),\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "    }\n",
    "    fname = f\"{'best' if is_best else 'last'}_ckpt_{personalized_par_string}_round_{round_num}.pth\"\n",
    "    half_name=f\"last_ckpt_{personalized_par_string}_round_\"\n",
    "    if is_best:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir,fname))\n",
    "    else:\n",
    "            torch.save(state, os.path.join(ckpt_dir, fname))\n",
    "            for existing in os.listdir(ckpt_dir):\n",
    "                existing_path = os.path.join(ckpt_dir, existing)\n",
    "                if (\n",
    "                    existing.endswith('.pth')\n",
    "                    and existing != fname\n",
    "                    and 'best' not in existing\n",
    "                    and half_name in existing\n",
    "                ):\n",
    "                    os.remove(existing_path)\n",
    "                    print(f\"  Deleted: {existing}\")\n",
    "    print(f\"[Checkpoint] Done saving to {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###always copy this for logging ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "import os, csv,json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "def log_results(name, rnd,maxround, val_loss, val_acc, test_loss, test_acc, train_loss, train_acc,local_train_mean,local_train_std, csv_path='results_log.csv',csv_path_final='global_results.csv',params={}):\n",
    "        file_exists = os.path.exists(csv_path)\n",
    "        if not file_exists:\n",
    "            with open(csv_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','round', 'val_loss', 'val_acc', 'test_loss', 'test_acc','train_loss', 'train_acc','local_train_mean','local_train_std'])\n",
    "        with open(csv_path, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow([\n",
    "                name,\n",
    "                rnd,\n",
    "                f\"{val_loss:.4f}\", f\"{val_acc:.4f}\",\n",
    "                f\"{test_loss:.4f}\", f\"{test_acc:.4f}\",\n",
    "                 f\"{train_loss:.4f}\",  f\"{train_acc:.4f}\",\n",
    "                 f\"{local_train_mean:.4f}\",  f\"{local_train_std:.4f}\",\n",
    "\n",
    "            ])\n",
    "        \n",
    "\n",
    "        if rnd==maxround:\n",
    "                csv_path_res='results_only_'+name+'.csv'\n",
    "                clean_results_history(csv_path,csv_path_res,name)\n",
    "                write_final_results(name,params,csv_path_res,csv_path_final)\n",
    "\n",
    "def clean_results_history(results_file_name,new_file_name,name):   #da fare eliminare righe che vengono prima della successiva \n",
    "    input_file = results_file_name\n",
    "    output_file=new_file_name\n",
    "\n",
    "\n",
    "    # Read and clean lines\n",
    "    with open(input_file, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    filtered = []\n",
    "    last_seen_index = float('inf')  # Start with a very large number\n",
    "    header = lines[0]\n",
    "    data_lines = lines[1:]\n",
    "\n",
    "    # Iterate in reverse\n",
    "    for line in reversed(data_lines):\n",
    "        current_index = int(line.split(',')[1])\n",
    "        if current_index < last_seen_index and (line.split(',')[0]==name):\n",
    "            filtered.append(line)\n",
    "            last_seen_index = current_index\n",
    "        else:\n",
    "            # Skip this line, as its index is higher than the next one\n",
    "            pass\n",
    "\n",
    "    # Reverse again to restore original order (except removed lines)\n",
    "    filtered.reverse()\n",
    "\n",
    "    file_exists = os.path.exists(output_file)\n",
    "    # Write to output\n",
    "\n",
    "\n",
    "    if os.path.exists(new_file_name):\n",
    "        with open(new_file_name, \"r\") as f:\n",
    "            history_lines = [line.strip() for line in f if line.strip()]\n",
    "        history_header = history_lines[0]\n",
    "        history_data = history_lines[1:]\n",
    "    else:\n",
    "        history_header = header\n",
    "        history_data = []\n",
    "\n",
    "    # --- Remove from history any round that will be updated ---\n",
    "    new_rounds_set = {int(line.split(',')[1]) for line in filtered}\n",
    "    updated_history_data = [\n",
    "        line for line in history_data\n",
    "        if int(line.split(',')[1]) not in new_rounds_set\n",
    "    ]\n",
    "\n",
    "    # --- Merge history and new data ---\n",
    "    merged_data = updated_history_data + filtered\n",
    "\n",
    "    # --- Write back ---\n",
    "    with open(new_file_name, \"w\") as f:\n",
    "        f.write(history_header + \"\\n\")\n",
    "        f.write(\"\\n\".join(merged_data))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Filtered and merged results written to {new_file_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_results(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        acc_col = f\"{split}_acc\"\n",
    "        loss_col = f\"{split}_loss\"\n",
    "\n",
    "        max_acc = df[acc_col].max()\n",
    "        max_idx = df[acc_col].idxmax()\n",
    "\n",
    "        max_round = df.loc[max_idx, 'round']\n",
    "        loss_at_max = df.loc[max_idx, loss_col]\n",
    "\n",
    "        results[split] = {\n",
    "            'max_acc': max_acc,\n",
    "            'round': int(max_round),\n",
    "            'loss_at_max': loss_at_max\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_final_results(name, params, csv_path='results_log.csv', results_csv_path='global_results.csv'):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    results = get_results(csv_path)\n",
    "\n",
    "    row = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_name': name,\n",
    "        'parameters': json.dumps(params),\n",
    "        'train_max_acc': results['train']['max_acc'],\n",
    "        'train_round': results['train']['round'],\n",
    "        'train_loss': results['train']['loss_at_max'],\n",
    "        'val_max_acc': results['val']['max_acc'],\n",
    "        'val_round': results['val']['round'],\n",
    "        'val_loss': results['val']['loss_at_max'],\n",
    "        'test_max_acc': results['test']['max_acc'],\n",
    "        'test_round': results['test']['round'],\n",
    "        'test_loss': results['test']['loss_at_max'],\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.exists(results_csv_path)\n",
    "\n",
    "    with open(results_csv_path, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    # If import succeeds, we are likely in Colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    # If import fails, we are likely not in Colab\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    CKPT_DIR = '/content/drive/MyDrive/fl_checkpoints'\n",
    "else:\n",
    "    CKPT_DIR = './fl_checkpoints'\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAGLQObvdWs7",
    "outputId": "ff8068e4-9922-48ff-eafa-4c7be80dab60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Running cifar_more_step_scheduler_OneCycleLR with config: lr=0.1, wd=0.0001, bs=128, epochs=500\n",
      "[Checkpoint] Resumed from round 500 (loaded last_ckpt_cifar_more_step_scheduler_OneCycleLR_round_500.pth)\n",
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 500} with cifar_more_step_scheduler_OneCycleLR → best_val_acc=0.0000, test_acc=0.5631\n",
      "\n",
      "▶ Running cifar_more_step_scheduler_CosineAnnealingWarmRestarts with config: lr=0.1, wd=0.0001, bs=128, epochs=500\n",
      "[Checkpoint] Resumed from round 500 (loaded last_ckpt_cifar_more_step_scheduler_CosineAnnealingWarmRestarts_round_500.pth)\n",
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 500} with cifar_more_step_scheduler_CosineAnnealingWarmRestarts → best_val_acc=0.0000, test_acc=0.3955\n",
      "\n",
      "▶ Running cifar_more_step_scheduler_CosineAnnealingLR with config: lr=0.1, wd=0.0001, bs=128, epochs=500\n",
      "[Checkpoint] Resumed from round 500 (loaded last_ckpt_cifar_more_step_scheduler_CosineAnnealingLR_round_500.pth)\n",
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 500} with cifar_more_step_scheduler_CosineAnnealingLR → best_val_acc=0.0000, test_acc=0.5484\n",
      "\n",
      "▶ Running cifar_more_step_scheduler_StepLR with config: lr=0.1, wd=0.0001, bs=128, epochs=500\n",
      "[Checkpoint] Resumed from round 500 (loaded last_ckpt_cifar_more_step_scheduler_StepLR_round_500.pth)\n",
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 500} with cifar_more_step_scheduler_StepLR → best_val_acc=0.0000, test_acc=0.5267\n",
      "\n",
      "▶ Running cifar_more_step_scheduler_ExponentialLR with config: lr=0.1, wd=0.0001, bs=128, epochs=500\n",
      "[Checkpoint] Resumed from round 500 (loaded last_ckpt_cifar_more_step_scheduler_ExponentialLR_round_500.pth)\n",
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 500} with cifar_more_step_scheduler_ExponentialLR → best_val_acc=0.0000, test_acc=0.5399\n",
      "\n",
      "▶ Running cifar_more_step_scheduler_ReduceLROnPlateau with config: lr=0.1, wd=0.0001, bs=128, epochs=500\n",
      "[Checkpoint] Resumed from round 500 (loaded last_ckpt_cifar_more_step_scheduler_ReduceLROnPlateau_round_500.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adri/.cache/pypoetry/virtualenvs/advanced-machine-learning-labs-ROHCwjr8-py3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 500} with cifar_more_step_scheduler_ReduceLROnPlateau → best_val_acc=0.0000, test_acc=0.5282\n"
     ]
    }
   ],
   "source": [
    "# ── A. Hyperparameter Grid ─────────────────────────────────────────────────────\n",
    "param_grid = [   #incresed_epochs\n",
    "    {'lr': 0.1, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 500},\n",
    "    # … add more combinations as needed …\n",
    "]\n",
    "\n",
    "# Only testing new schedulers here\n",
    "scheduler_classes = [\n",
    "    torch.optim.lr_scheduler.OneCycleLR,\n",
    "    torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "    torch.optim.lr_scheduler.StepLR,\n",
    "    torch.optim.lr_scheduler.ExponentialLR,\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "]\n",
    "\n",
    "for sched_class in scheduler_classes:\n",
    "    sched_name = f\"cifar_more_step_scheduler_{sched_class.__name__}\"\n",
    "\n",
    "    for cfg in param_grid:\n",
    "        lr, wd, bs, epochs = cfg['lr'], cfg['weight_decay'], cfg['batch_size'], cfg['epochs']\n",
    "        print(f\"\\n▶ Running {sched_name} with config: lr={lr}, wd={wd}, bs={bs}, epochs={epochs}\")\n",
    "\n",
    "        # Fresh model/optimizer/etc. for each config\n",
    "        model     = LELeNetCIFAR().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Re-create DataLoaders per batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "        val_loader   = DataLoader(val_dataset,   batch_size=bs, shuffle=False, num_workers=2)\n",
    "        test_loader  = DataLoader(test_dataset,  batch_size=bs, shuffle=False, num_workers=2)\n",
    "\n",
    "        if sched_class == torch.optim.lr_scheduler.OneCycleLR:\n",
    "            scheduler = sched_class(\n",
    "                optimizer,\n",
    "                max_lr=lr,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                epochs=epochs\n",
    "            )\n",
    "        elif sched_class == torch.optim.lr_scheduler.CosineAnnealingWarmRestarts:\n",
    "            # T_0 should be reasonable relative to total epochs\n",
    "            T_0 = max(10, epochs // 10)  # At least 10, or 10% of total epochs\n",
    "            scheduler = sched_class(optimizer, T_0=T_0, T_mult=2)\n",
    "        elif sched_class == torch.optim.lr_scheduler.CosineAnnealingLR:\n",
    "            scheduler = sched_class(optimizer, T_max=epochs)\n",
    "        elif sched_class == torch.optim.lr_scheduler.StepLR:\n",
    "            # Step every 30% of total epochs\n",
    "            step_size = max(30, epochs // 3)\n",
    "            scheduler = sched_class(optimizer, step_size=step_size, gamma=0.1)\n",
    "        elif sched_class == torch.optim.lr_scheduler.ExponentialLR:\n",
    "            # Calculate gamma to reach ~1% of initial LR by end of training\n",
    "            gamma = (0.01) ** (1.0 / epochs)\n",
    "            scheduler = sched_class(optimizer, gamma=gamma)\n",
    "        elif sched_class == torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
    "            scheduler = sched_class(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "        else:\n",
    "            scheduler = sched_class(optimizer)\n",
    "\n",
    "        start_round = load_checkpoint(model, optimizer, CKPT_DIR, scheduler, resume=True, name=sched_name)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=f'./logs/{sched_name}_lr{lr}_wd{wd}_bs{bs}_ep{epochs}')\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        for epoch in range(start_round, epochs + 1):\n",
    "            model.train()\n",
    "            train_loss, correct, total = 0.0, 0, 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # LR scheduling per-batch if OneCycleLR\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            train_acc = correct / total\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            # Validation + test\n",
    "            val_loss,   val_acc   = eval_model(model, criterion, val_loader)\n",
    "            test_loss, test_acc   = eval_model(model, criterion, test_loader)\n",
    "\n",
    "            log_results(sched_name, epoch, epochs, val_loss, val_acc, test_loss, test_acc,\n",
    "                        train_loss, train_acc, -1, -1, params=cfg)\n",
    "\n",
    "            # Epoch-based scheduler stepping\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "                scheduler.step()\n",
    "            elif isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingLR):\n",
    "                scheduler.step()\n",
    "            elif isinstance(scheduler, torch.optim.lr_scheduler.StepLR):\n",
    "                scheduler.step()\n",
    "            elif isinstance(scheduler, torch.optim.lr_scheduler.ExponentialLR):\n",
    "                scheduler.step()\n",
    "            elif isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)  # ReduceLROnPlateau needs a metric\n",
    "\n",
    "\n",
    "            # TensorBoard logging\n",
    "            writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "            writer.add_scalars('Acc',  {'train': train_acc,  'val': val_acc}, epoch)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if epoch % 20 == 0 or epoch == epochs:\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, is_best=False,\n",
    "                                ckpt_dir=CKPT_DIR, personalized_par_string=sched_name)\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            print(f\"  Epoch {epoch}/{epochs}  train_acc={train_acc:.4f}  val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # Final Test Evaluation\n",
    "        test_loss, test_acc = eval_model(model, criterion, test_loader)\n",
    "        print(f\"Config {cfg} with {sched_name} → best_val_acc={best_val_acc:.4f}, test_acc={test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZm-lbwPdWvT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Owzk--gFdWwB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5IgnGhEdWw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1O3ub6UdWxr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMO88ACkR5/uTJX6n18OJVi",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "advanced-machine-learning-labs-ROHCwjr8-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
