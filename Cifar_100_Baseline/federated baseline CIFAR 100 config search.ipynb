{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lGxnD3UbdWmb"
   },
   "outputs": [],
   "source": [
    "# ── A. Imports & Reproducibility ────────────────────────────────────────────────\n",
    "import os\n",
    "import csv                                                  # For result logging :contentReference[oaicite:0]{index=0}\n",
    "import random                                               # For seeding :contentReference[oaicite:1]{index=1}\n",
    "import numpy as np                                          # For numeric ops :contentReference[oaicite:2]{index=2}\n",
    "import torch                                               # Core PyTorch :contentReference[oaicite:3]{index=3}\n",
    "import torch.nn as nn                                       # Neural-net modules :contentReference[oaicite:4]{index=4}\n",
    "import torch.nn.functional as F                             # Functional API :contentReference[oaicite:5]{index=5}\n",
    "import torch.optim as optim                                 # Optimizers :contentReference[oaicite:6]{index=6}\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR      # Scheduler :contentReference[oaicite:7]{index=7}\n",
    "from torch.utils.data import DataLoader, random_split       # Data loaders & splits :contentReference[oaicite:8]{index=8}\n",
    "import torchvision                                          # Datasets & transforms :contentReference[oaicite:9]{index=9}\n",
    "import torchvision.transforms as T                          # Transforms :contentReference[oaicite:10]{index=10}\n",
    "from torch.utils.tensorboard import SummaryWriter           # TensorBoard logging :contentReference[oaicite:11]{index=11}\n",
    "import matplotlib.pyplot as plt                             # Plotting :contentReference[oaicite:12]{index=12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "39Q3SwyBdWnP"
   },
   "outputs": [],
   "source": [
    "# Seed everything for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746450506102,
     "user": {
      "displayName": "Mattia Cappellino",
      "userId": "05271634366428679167"
     },
     "user_tz": -120
    },
    "id": "r0PBwz2GdWoD",
    "outputId": "e7c49bfd-f989-4ada-fc33-32a24f86bf1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ── B. Device ───────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")                             # Confirm GPU vs CPU :contentReference[oaicite:13]{index=13}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hocKqvhTdWpL"
   },
   "outputs": [],
   "source": [
    "# ── C. Data Preparation ─────────────────────────────────────────────────────────\n",
    "# Transforms\n",
    "transform_train = T.Compose([\n",
    "    T.RandomCrop(32, padding=4), T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "])\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6010,
     "status": "ok",
     "timestamp": 1746450512115,
     "user": {
      "displayName": "Mattia Cappellino",
      "userId": "05271634366428679167"
     },
     "user_tz": -120
    },
    "id": "1KsmNAdjdWqH",
    "outputId": "6c3bd418-5e4e-4961-ce02-87eafebe7687"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:45<00:00, 3.75MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Download & train/val/test split\n",
    "dataset_full = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "val_size = 5000\n",
    "train_size = len(dataset_full) - val_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(seed))\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9zukJAc0dWq0"
   },
   "outputs": [],
   "source": [
    "# ── D. Model Definition ─────────────────────────────────────────────────────────\n",
    "class LELeNetCIFAR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1   = nn.Linear(64*8*8, 384)\n",
    "        self.fc2   = nn.Linear(384, 192)\n",
    "        self.fc3   = nn.Linear(192, 100)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x)); x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0ambCIcmdWrf"
   },
   "outputs": [],
   "source": [
    "# ── E. Utilities: Train/Eval & Checkpointing ────────────────────────────────────\n",
    "def train_one_epoch(model, optimizer, criterion, loader):\n",
    "    model.train()\n",
    "    running_loss = correct = total = 0\n",
    "    for imgs, lbls in loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, lbls)\n",
    "        loss.backward(); optimizer.step()\n",
    "        running_loss += loss.item()*imgs.size(0)\n",
    "        correct += out.argmax(1).eq(lbls).sum().item()\n",
    "        total += lbls.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "def eval_model(model, criterion, loader):\n",
    "    model.eval()\n",
    "    running_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            out = model(imgs); loss = criterion(out, lbls)\n",
    "            running_loss += loss.item()*imgs.size(0)\n",
    "            correct += out.argmax(1).eq(lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "    return running_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ELDe-m-idWsO"
   },
   "outputs": [],
   "source": [
    "# Checkpoint saves model + optimizer + scheduler + RNG\n",
    "ckpt_dir = './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, is_best=False):\n",
    "    fname = f\"{'best' if is_best else 'last'}_ckpt_epoch_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optim_state': optimizer.state_dict(),\n",
    "        'sched_state': scheduler.state_dict(),\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "    }, os.path.join(ckpt_dir, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAGLQObvdWs7",
    "outputId": "ff8068e4-9922-48ff-eafa-4c7be80dab60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running config: lr=0.1, wd=0.0001, bs=128, epochs=100\n",
      "  Epoch 1/100  train_acc=0.0593  val_acc=0.0848\n",
      "  Epoch 2/100  train_acc=0.0990  val_acc=0.1136\n",
      "  Epoch 3/100  train_acc=0.1149  val_acc=0.1286\n",
      "  Epoch 4/100  train_acc=0.1277  val_acc=0.1492\n",
      "  Epoch 5/100  train_acc=0.1383  val_acc=0.1302\n",
      "  Epoch 6/100  train_acc=0.1439  val_acc=0.1456\n",
      "  Epoch 7/100  train_acc=0.1508  val_acc=0.1548\n",
      "  Epoch 8/100  train_acc=0.1577  val_acc=0.1552\n",
      "  Epoch 9/100  train_acc=0.1622  val_acc=0.1542\n",
      "  Epoch 10/100  train_acc=0.1717  val_acc=0.1714\n",
      "  Epoch 11/100  train_acc=0.1801  val_acc=0.1760\n",
      "  Epoch 12/100  train_acc=0.1809  val_acc=0.1790\n",
      "  Epoch 13/100  train_acc=0.1857  val_acc=0.1810\n",
      "  Epoch 14/100  train_acc=0.1894  val_acc=0.1886\n",
      "  Epoch 15/100  train_acc=0.1911  val_acc=0.1880\n",
      "  Epoch 16/100  train_acc=0.1998  val_acc=0.1974\n",
      "  Epoch 17/100  train_acc=0.1990  val_acc=0.2014\n",
      "  Epoch 18/100  train_acc=0.2045  val_acc=0.2000\n",
      "  Epoch 19/100  train_acc=0.2069  val_acc=0.1840\n",
      "  Epoch 20/100  train_acc=0.2147  val_acc=0.2024\n",
      "  Epoch 21/100  train_acc=0.2151  val_acc=0.2000\n",
      "  Epoch 22/100  train_acc=0.2207  val_acc=0.2140\n",
      "  Epoch 23/100  train_acc=0.2230  val_acc=0.2304\n",
      "  Epoch 24/100  train_acc=0.2285  val_acc=0.2192\n",
      "  Epoch 25/100  train_acc=0.2328  val_acc=0.2208\n",
      "  Epoch 26/100  train_acc=0.2391  val_acc=0.2348\n",
      "  Epoch 27/100  train_acc=0.2403  val_acc=0.2296\n",
      "  Epoch 28/100  train_acc=0.2464  val_acc=0.2406\n",
      "  Epoch 29/100  train_acc=0.2533  val_acc=0.2416\n",
      "  Epoch 30/100  train_acc=0.2569  val_acc=0.2316\n",
      "  Epoch 31/100  train_acc=0.2604  val_acc=0.2588\n",
      "  Epoch 32/100  train_acc=0.2669  val_acc=0.2572\n",
      "  Epoch 33/100  train_acc=0.2677  val_acc=0.2542\n",
      "  Epoch 34/100  train_acc=0.2726  val_acc=0.2708\n",
      "  Epoch 35/100  train_acc=0.2822  val_acc=0.2610\n",
      "  Epoch 36/100  train_acc=0.2878  val_acc=0.2696\n",
      "  Epoch 37/100  train_acc=0.2940  val_acc=0.2652\n",
      "  Epoch 38/100  train_acc=0.2973  val_acc=0.2750\n",
      "  Epoch 39/100  train_acc=0.3116  val_acc=0.2726\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs+\u001b[32m1\u001b[39m):\n\u001b[32m     53\u001b[39m     train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     val_loss,   val_acc   = \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     scheduler.step()\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Log to TensorBoard\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36meval_model\u001b[39m\u001b[34m(model, criterion, loader)\u001b[39m\n\u001b[32m     18\u001b[39m running_loss = correct = total = \u001b[32m0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m;\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/advanced-machine-learning-labs-ROHCwjr8-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/advanced-machine-learning-labs-ROHCwjr8-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/advanced-machine-learning-labs-ROHCwjr8-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1420\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1422\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/advanced-machine-learning-labs-ROHCwjr8-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ── A. Hyperparameter Grid ─────────────────────────────────────────────────────\n",
    "param_grid = [\n",
    "    {'lr': 0.1, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.001, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.01, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.1, 'weight_decay': 1e-5, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.001, 'weight_decay': 1e-5, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.01, 'weight_decay': 1e-3, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.1, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.001, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.01, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.1, 'weight_decay': 1e-5, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.001, 'weight_decay': 1e-5, 'batch_size': 128, 'epochs': 100},\n",
    "    {'lr': 0.01, 'weight_decay': 1e-3, 'batch_size': 128, 'epochs': 100},\n",
    "    # … add more combinations as needed …\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#all config to test\n",
    "#testing all with 128 and 100 for last 2 values\n",
    "\n",
    "#tested lr = 0.001 and weight decay = 1e-5\n",
    "\"\"\"# ── Define the values you want to test ────────────────────────────────────────────\n",
    "learning_rates   = [0.001, 0.01, 0.1]        # typical LR grid for CIFAR-100 :contentReference[oaicite:0]{index=0}\n",
    "weight_decays    = [1e-5, 1e-4, 1e-3]        # common L2 regs :contentReference[oaicite:1]{index=1}\n",
    "batch_sizes      = [32, 64, 128]             # fits 6 GB VRAM on Colab GPUs :contentReference[oaicite:2]{index=2}\n",
    "epoch_counts     = [50, 100]                 # short vs full training\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare CSV logging\n",
    "csv_path = './results_grid.csv'\n",
    "if not os.path.exists(csv_path):\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            'lr', 'weight_decay', 'batch_size', 'epochs',\n",
    "            'best_val_acc', 'test_acc'\n",
    "        ])\n",
    "\n",
    "# ── B. Loop Over Configurations ────────────────────────────────────────────────\n",
    "for cfg in param_grid:\n",
    "    lr, wd, bs, epochs = cfg['lr'], cfg['weight_decay'], cfg['batch_size'], cfg['epochs']\n",
    "    print(f\"Running config: lr={lr}, wd={wd}, bs={bs}, epochs={epochs}\")\n",
    "\n",
    "    # Re-create DataLoaders per bs\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=bs, shuffle=False, num_workers=2)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=bs, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Instantiate fresh model, optimizer, scheduler, writer\n",
    "    model     = LELeNetCIFAR().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    writer    = SummaryWriter(log_dir=f'./logs/lr{lr}_wd{wd}_bs{bs}_ep{epochs}')\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader)\n",
    "        val_loss,   val_acc   = eval_model(model, criterion, val_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "        writer.add_scalars('Acc',  {'train': train_acc,  'val': val_acc},   epoch)\n",
    "\n",
    "        # Save checkpoints\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, is_best=(val_acc>best_val_acc))\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"  Epoch {epoch}/{epochs}  train_acc={train_acc:.4f}  val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # Final Test Evaluation\n",
    "    test_loss, test_acc = eval_model(model, criterion, test_loader)\n",
    "    print(f\"Config {cfg} → best_val_acc={best_val_acc:.4f}, test_acc={test_acc:.4f}\")\n",
    "\n",
    "    # Append results to CSV\n",
    "    with open(csv_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([lr, wd, bs, epochs, best_val_acc, test_acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qx5SLtcQggbL"
   },
   "outputs": [],
   "source": [
    "# ── Configuration Summary Cell ──────────────────────────────────────────────────\n",
    "import torch, torchvision, sys, platform, time, os\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def summarize_run(cfg, train_loader, val_loader, test_loader, writer=None):\n",
    "    \"\"\"\n",
    "    Print and log a full summary of the current run configuration and environment.\n",
    "\n",
    "    Args:\n",
    "        cfg (dict): Hyperparameter dict with 'lr', 'weight_decay', 'batch_size', 'epochs', etc.\n",
    "        train_loader, val_loader, test_loader: DataLoaders for computing dataset sizes.\n",
    "        writer (SummaryWriter, optional): if provided, logs summary to TensorBoard under 'RunInfo'.\n",
    "    \"\"\"\n",
    "    # 1. Timestamp\n",
    "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "    # 2. Environment\n",
    "    env_info = {\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'torchvision_version': torchvision.__version__,\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'cuda_device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only',\n",
    "        'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        'platform': platform.platform(),\n",
    "        'cwd': os.getcwd(),\n",
    "    }\n",
    "\n",
    "    # 3. Data sizes\n",
    "    data_info = {\n",
    "        'train_samples': len(train_loader.dataset),\n",
    "        'val_samples': len(val_loader.dataset),\n",
    "        'test_samples': len(test_loader.dataset),\n",
    "        'batch_size': cfg.get('batch_size'),\n",
    "        'num_batches_train': len(train_loader),\n",
    "        'num_batches_val': len(val_loader),\n",
    "        'num_batches_test': len(test_loader),\n",
    "    }\n",
    "\n",
    "    # 4. Seed & Hyperparams\n",
    "    seed_info = {\n",
    "        'seed': cfg.get('seed', 'not set'),\n",
    "    }\n",
    "    hyperparams = {k: v for k, v in cfg.items() if k not in seed_info}\n",
    "\n",
    "    # 5. Print Summary\n",
    "    print(f\"{'='*20} RUN SUMMARY ({ts}) {'='*20}\\n\")\n",
    "    print(\"➜ Environment:\")\n",
    "    for k, v in env_info.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(\"\\n➜ Data:\")\n",
    "    for k, v in data_info.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(\"\\n➜ Seed:\")\n",
    "    for k, v in seed_info.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(\"\\n➜ Hyperparameters:\")\n",
    "    for k, v in hyperparams.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "    # 6. Optional TensorBoard Logging\n",
    "    if writer is not None:\n",
    "        for k, v in {**env_info, **data_info, **seed_info, **hyperparams}.items():\n",
    "            # Non-numeric values will be logged as text under a scalar tag\n",
    "            try:\n",
    "                writer.add_text('RunInfo/' + k, str(v), 0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ── Example Usage ────────────────────────────────────────────────────────────────\n",
    "# After defining `cfg`, DataLoaders, and `writer` in your Run Cell, just call:\n",
    "summarize_run(cfg, train_loader, val_loader, test_loader, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMYuWSfTdWtt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./results_grid.csv')\n",
    "display(df.sort_values('test_acc', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rNf-hPZdWuf"
   },
   "outputs": [],
   "source": [
    "# ── Final Analysis & Plotting ────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load results\n",
    "csv_path = './results_grid.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. Display top 5 configs by test accuracy\n",
    "top5 = df.sort_values('test_acc', ascending=False).head(5)\n",
    "print(\"Top 5 hyperparameter configurations:\")\n",
    "display(top5)\n",
    "\n",
    "# 3. Bar plot of test accuracy for each config\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    x=range(len(df)),\n",
    "    height=df['test_acc'],\n",
    "    tick_label=[f\"lr={lr}\\nwd={wd}\\nbs={bs}\\nep={ep}\"\n",
    "                for lr, wd, bs, ep in zip(df['lr'], df['weight_decay'], df['batch_size'], df['epochs'])]\n",
    ")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Grid Search Results: Test Accuracy by Hyperparameter Configuration')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZm-lbwPdWvT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Owzk--gFdWwB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5IgnGhEdWw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1O3ub6UdWxr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMO88ACkR5/uTJX6n18OJVi",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "advanced-machine-learning-labs-ROHCwjr8-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
