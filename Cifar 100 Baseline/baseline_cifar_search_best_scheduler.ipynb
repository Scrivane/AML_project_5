{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lGxnD3UbdWmb"
   },
   "outputs": [],
   "source": [
    "# ── A. Imports & Reproducibility ────────────────────────────────────────────────\n",
    "import os\n",
    "import csv                                                  # For result logging :contentReference[oaicite:0]{index=0}\n",
    "import random                                               # For seeding :contentReference[oaicite:1]{index=1}\n",
    "import numpy as np                                          # For numeric ops :contentReference[oaicite:2]{index=2}\n",
    "import torch                                               # Core PyTorch :contentReference[oaicite:3]{index=3}\n",
    "import torch.nn as nn                                       # Neural-net modules :contentReference[oaicite:4]{index=4}\n",
    "import torch.nn.functional as F                             # Functional API :contentReference[oaicite:5]{index=5}\n",
    "import torch.optim as optim                                 # Optimizers :contentReference[oaicite:6]{index=6}\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR      # Scheduler :contentReference[oaicite:7]{index=7}\n",
    "from torch.utils.data import DataLoader, random_split       # Data loaders & splits :contentReference[oaicite:8]{index=8}\n",
    "import torchvision                                          # Datasets & transforms :contentReference[oaicite:9]{index=9}\n",
    "import torchvision.transforms as T                          # Transforms :contentReference[oaicite:10]{index=10}\n",
    "from torch.utils.tensorboard import SummaryWriter           # TensorBoard logging :contentReference[oaicite:11]{index=11}\n",
    "import matplotlib.pyplot as plt                             # Plotting :contentReference[oaicite:12]{index=12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "39Q3SwyBdWnP"
   },
   "outputs": [],
   "source": [
    "# Seed everything for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746450506102,
     "user": {
      "displayName": "Mattia Cappellino",
      "userId": "05271634366428679167"
     },
     "user_tz": -120
    },
    "id": "r0PBwz2GdWoD",
    "outputId": "e7c49bfd-f989-4ada-fc33-32a24f86bf1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ── B. Device ───────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")                             # Confirm GPU vs CPU :contentReference[oaicite:13]{index=13}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hocKqvhTdWpL"
   },
   "outputs": [],
   "source": [
    "# ── C. Data Preparation ─────────────────────────────────────────────────────────\n",
    "# Transforms\n",
    "transform_train = T.Compose([\n",
    "    T.RandomCrop(32, padding=4), T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "])\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6010,
     "status": "ok",
     "timestamp": 1746450512115,
     "user": {
      "displayName": "Mattia Cappellino",
      "userId": "05271634366428679167"
     },
     "user_tz": -120
    },
    "id": "1KsmNAdjdWqH",
    "outputId": "6c3bd418-5e4e-4961-ce02-87eafebe7687"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:49<00:00, 3.42MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Download & train/val/test split\n",
    "dataset_full = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "val_size = 5000\n",
    "train_size = len(dataset_full) - val_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(seed))\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9zukJAc0dWq0"
   },
   "outputs": [],
   "source": [
    "# ── D. Model Definition ─────────────────────────────────────────────────────────\n",
    "class LELeNetCIFAR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1   = nn.Linear(64*8*8, 384)\n",
    "        self.fc2   = nn.Linear(384, 192)\n",
    "        self.fc3   = nn.Linear(192, 100)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x)); x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0ambCIcmdWrf"
   },
   "outputs": [],
   "source": [
    "# ── E. Utilities: Train/Eval & Checkpointing ────────────────────────────────────\n",
    "def train_one_epoch(model, optimizer, criterion, loader):\n",
    "    model.train()\n",
    "    running_loss = correct = total = 0\n",
    "    for imgs, lbls in loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, lbls)\n",
    "        loss.backward(); optimizer.step()\n",
    "        running_loss += loss.item()*imgs.size(0)\n",
    "        correct += out.argmax(1).eq(lbls).sum().item()\n",
    "        total += lbls.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "def eval_model(model, criterion, loader):\n",
    "    model.eval()\n",
    "    running_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            out = model(imgs); loss = criterion(out, lbls)\n",
    "            running_loss += loss.item()*imgs.size(0)\n",
    "            correct += out.argmax(1).eq(lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "    return running_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ELDe-m-idWsO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Checkpoint saves model + optimizer + scheduler + RNG\\nckpt_dir = \\'./checkpoints\\'\\nos.makedirs(ckpt_dir, exist_ok=True)\\ndef save_checkpoint(model, optimizer, scheduler, epoch, is_best=False):\\n    fname = f\"{\\'best\\' if is_best else \\'last\\'}_ckpt_epoch_{epoch}.pth\"\\n    torch.save({\\n        \\'epoch\\': epoch,\\n        \\'model_state\\': model.state_dict(),\\n        \\'optim_state\\': optimizer.state_dict(),\\n        \\'sched_state\\': scheduler.state_dict(),\\n        \\'rng_state\\': torch.get_rng_state(),\\n    }, os.path.join(ckpt_dir, fname)) '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Checkpoint saves model + optimizer + scheduler + RNG\n",
    "ckpt_dir = './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, is_best=False):\n",
    "    fname = f\"{'best' if is_best else 'last'}_ckpt_epoch_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optim_state': optimizer.state_dict(),\n",
    "        'sched_state': scheduler.state_dict(),\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "    }, os.path.join(ckpt_dir, fname)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, torch, os\n",
    "\n",
    "\n",
    "def latest_ckpt(dirpath, pattern=\"last_ckpt__round_*.pth\"):\n",
    "    paths = glob.glob(os.path.join(dirpath, pattern))\n",
    "    if not paths:\n",
    "        return None\n",
    "    paths.sort(key=lambda p: int(p.rsplit(\"_\", 1)[1].split(\".\")[0]))\n",
    "    return paths[-1]\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, ckpt_dir,scheduler, resume=True,name=\"\"):\n",
    "    if not resume:\n",
    "        print(\"[Checkpoint] Starting training from scratch.\")\n",
    "        return 1\n",
    "    if name:\n",
    "        pattern='last_ckpt_'+name+'_round_*.pth'\n",
    "        ckpt_path = latest_ckpt(ckpt_dir,pattern)\n",
    "    else:\n",
    "        ckpt_path = latest_ckpt(ckpt_dir)\n",
    "    if ckpt_path is None:\n",
    "        print(\"[Checkpoint] No checkpoint found; training from scratch.\")\n",
    "        return 1\n",
    "    # Load checkpoint tensors onto CPU to preserve RNG state tensor\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "    scheduler.load_state_dict(ckpt['sched_state'])\n",
    "    # Restore CPU RNG state\n",
    "    rng_state = ckpt['rng_state']\n",
    "    if rng_state.device.type != 'cpu':\n",
    "        rng_state = rng_state.cpu()\n",
    "    torch.set_rng_state(rng_state)\n",
    "    print(f\"[Checkpoint] Resumed from round {ckpt['round']} (loaded {os.path.basename(ckpt_path)})\")\n",
    "    return ckpt['round'] + 1\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer,scheduler, round_num, ckpt_dir,personalized_par_string=\"\", is_best=False):\n",
    "    print(f\"[Checkpoint] Saving round {round_num}...\")\n",
    "    state = {\n",
    "        'round': round_num,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "          'sched_state': scheduler.state_dict(),\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "    }\n",
    "    fname = f\"{'best' if is_best else 'last'}_ckpt_{personalized_par_string}_round_{round_num}.pth\"\n",
    "    half_name=f\"last_ckpt_{personalized_par_string}_round_\"\n",
    "    if is_best:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir,fname))\n",
    "    else:\n",
    "            torch.save(state, os.path.join(ckpt_dir, fname))\n",
    "            for existing in os.listdir(ckpt_dir):\n",
    "                existing_path = os.path.join(ckpt_dir, existing)\n",
    "                if (\n",
    "                    existing.endswith('.pth')\n",
    "                    and existing != fname\n",
    "                    and 'best' not in existing\n",
    "                    and half_name in existing\n",
    "                ):\n",
    "                    os.remove(existing_path)\n",
    "                    print(f\"  Deleted: {existing}\")\n",
    "    print(f\"[Checkpoint] Done saving to {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###always copy this for logging ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "import os, csv,json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "def log_results(name, rnd,maxround, val_loss, val_acc, test_loss, test_acc, train_loss, train_acc,local_train_mean,local_train_std, csv_path='results_log.csv',csv_path_final='global_results.csv',params={}):\n",
    "        file_exists = os.path.exists(csv_path)\n",
    "        if not file_exists:\n",
    "            with open(csv_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','round', 'val_loss', 'val_acc', 'test_loss', 'test_acc','train_loss', 'train_acc','local_train_mean','local_train_std'])\n",
    "        with open(csv_path, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow([\n",
    "                name,\n",
    "                rnd,\n",
    "                f\"{val_loss:.4f}\", f\"{val_acc:.4f}\",\n",
    "                f\"{test_loss:.4f}\", f\"{test_acc:.4f}\",\n",
    "                 f\"{train_loss:.4f}\",  f\"{train_acc:.4f}\",\n",
    "                 f\"{local_train_mean:.4f}\",  f\"{local_train_std:.4f}\",\n",
    "\n",
    "            ])\n",
    "        \n",
    "\n",
    "        if rnd==maxround:\n",
    "                csv_path_res='results_only_'+name+'.csv'\n",
    "                clean_results_history(csv_path,csv_path_res,name)\n",
    "                write_final_results(name,params,csv_path_res,csv_path_final)\n",
    "\n",
    "def clean_results_history(results_file_name,new_file_name,name):   #da fare eliminare righe che vengono prima della successiva \n",
    "    input_file = results_file_name\n",
    "    output_file=new_file_name\n",
    "\n",
    "\n",
    "    # Read and clean lines\n",
    "    with open(input_file, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    filtered = []\n",
    "    last_seen_index = float('inf')  # Start with a very large number\n",
    "    header = lines[0]\n",
    "    data_lines = lines[1:]\n",
    "\n",
    "    # Iterate in reverse\n",
    "    for line in reversed(data_lines):\n",
    "        current_index = int(line.split(',')[1])\n",
    "        if current_index < last_seen_index and (line.split(',')[0]==name):\n",
    "            filtered.append(line)\n",
    "            last_seen_index = current_index\n",
    "        else:\n",
    "            # Skip this line, as its index is higher than the next one\n",
    "            pass\n",
    "\n",
    "    # Reverse again to restore original order (except removed lines)\n",
    "    filtered.reverse()\n",
    "\n",
    "    file_exists = os.path.exists(output_file)\n",
    "    # Write to output\n",
    "\n",
    "\n",
    "    if os.path.exists(new_file_name):\n",
    "        with open(new_file_name, \"r\") as f:\n",
    "            history_lines = [line.strip() for line in f if line.strip()]\n",
    "        history_header = history_lines[0]\n",
    "        history_data = history_lines[1:]\n",
    "    else:\n",
    "        history_header = header\n",
    "        history_data = []\n",
    "\n",
    "    # --- Remove from history any round that will be updated ---\n",
    "    new_rounds_set = {int(line.split(',')[1]) for line in filtered}\n",
    "    updated_history_data = [\n",
    "        line for line in history_data\n",
    "        if int(line.split(',')[1]) not in new_rounds_set\n",
    "    ]\n",
    "\n",
    "    # --- Merge history and new data ---\n",
    "    merged_data = updated_history_data + filtered\n",
    "\n",
    "    # --- Write back ---\n",
    "    with open(new_file_name, \"w\") as f:\n",
    "        f.write(history_header + \"\\n\")\n",
    "        f.write(\"\\n\".join(merged_data))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Filtered and merged results written to {new_file_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_results(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        acc_col = f\"{split}_acc\"\n",
    "        loss_col = f\"{split}_loss\"\n",
    "\n",
    "        max_acc = df[acc_col].max()\n",
    "        max_idx = df[acc_col].idxmax()\n",
    "\n",
    "        max_round = df.loc[max_idx, 'round']\n",
    "        loss_at_max = df.loc[max_idx, loss_col]\n",
    "\n",
    "        results[split] = {\n",
    "            'max_acc': max_acc,\n",
    "            'round': int(max_round),\n",
    "            'loss_at_max': loss_at_max\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_final_results(name, params, csv_path='results_log.csv', results_csv_path='global_results.csv'):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    results = get_results(csv_path)\n",
    "\n",
    "    row = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_name': name,\n",
    "        'parameters': json.dumps(params),\n",
    "        'train_max_acc': results['train']['max_acc'],\n",
    "        'train_round': results['train']['round'],\n",
    "        'train_loss': results['train']['loss_at_max'],\n",
    "        'val_max_acc': results['val']['max_acc'],\n",
    "        'val_round': results['val']['round'],\n",
    "        'val_loss': results['val']['loss_at_max'],\n",
    "        'test_max_acc': results['test']['max_acc'],\n",
    "        'test_round': results['test']['round'],\n",
    "        'test_loss': results['test']['loss_at_max'],\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.exists(results_csv_path)\n",
    "\n",
    "    with open(results_csv_path, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    # If import succeeds, we are likely in Colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    # If import fails, we are likely not in Colab\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    CKPT_DIR = '/content/drive/MyDrive/fl_checkpoints'\n",
    "else:\n",
    "    CKPT_DIR = './fl_checkpoints'\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAGLQObvdWs7",
    "outputId": "ff8068e4-9922-48ff-eafa-4c7be80dab60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Running cifar2_scheduler_OneCycleLR with config: lr=0.1, wd=0.0001, bs=128, epochs=100\n",
      "[Checkpoint] No checkpoint found; training from scratch.\n",
      "  Epoch 1/100  train_acc=0.0478  val_acc=0.0840\n",
      "  Epoch 2/100  train_acc=0.1112  val_acc=0.1392\n",
      "  Epoch 3/100  train_acc=0.1646  val_acc=0.1866\n",
      "  Epoch 4/100  train_acc=0.2087  val_acc=0.2274\n",
      "  Epoch 5/100  train_acc=0.2447  val_acc=0.2548\n",
      "  Epoch 6/100  train_acc=0.2729  val_acc=0.2766\n",
      "  Epoch 7/100  train_acc=0.2953  val_acc=0.3024\n",
      "  Epoch 8/100  train_acc=0.3117  val_acc=0.3112\n",
      "  Epoch 9/100  train_acc=0.3218  val_acc=0.3220\n",
      "[Checkpoint] Saving round 10...\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_10.pth\n",
      "  Epoch 10/100  train_acc=0.3246  val_acc=0.3164\n",
      "  Epoch 11/100  train_acc=0.3366  val_acc=0.3310\n",
      "  Epoch 12/100  train_acc=0.3317  val_acc=0.2970\n",
      "  Epoch 13/100  train_acc=0.3389  val_acc=0.3166\n",
      "  Epoch 14/100  train_acc=0.3417  val_acc=0.3102\n",
      "  Epoch 15/100  train_acc=0.3368  val_acc=0.3142\n",
      "  Epoch 16/100  train_acc=0.3385  val_acc=0.3202\n",
      "  Epoch 17/100  train_acc=0.3349  val_acc=0.3180\n",
      "  Epoch 18/100  train_acc=0.3265  val_acc=0.3202\n",
      "  Epoch 19/100  train_acc=0.3335  val_acc=0.3112\n",
      "[Checkpoint] Saving round 20...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_10.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_20.pth\n",
      "  Epoch 20/100  train_acc=0.3290  val_acc=0.3130\n",
      "  Epoch 21/100  train_acc=0.3271  val_acc=0.3024\n",
      "  Epoch 22/100  train_acc=0.3275  val_acc=0.2980\n",
      "  Epoch 23/100  train_acc=0.3229  val_acc=0.2908\n",
      "  Epoch 24/100  train_acc=0.3212  val_acc=0.3064\n",
      "  Epoch 25/100  train_acc=0.3200  val_acc=0.2874\n",
      "  Epoch 26/100  train_acc=0.3241  val_acc=0.2832\n",
      "  Epoch 27/100  train_acc=0.3240  val_acc=0.2838\n",
      "  Epoch 28/100  train_acc=0.3227  val_acc=0.2854\n",
      "  Epoch 29/100  train_acc=0.3222  val_acc=0.3156\n",
      "[Checkpoint] Saving round 30...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_20.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_30.pth\n",
      "  Epoch 30/100  train_acc=0.3284  val_acc=0.2970\n",
      "  Epoch 31/100  train_acc=0.3279  val_acc=0.2980\n",
      "  Epoch 32/100  train_acc=0.3288  val_acc=0.3164\n",
      "  Epoch 33/100  train_acc=0.3331  val_acc=0.2822\n",
      "  Epoch 34/100  train_acc=0.3292  val_acc=0.2964\n",
      "  Epoch 35/100  train_acc=0.3283  val_acc=0.3116\n",
      "  Epoch 36/100  train_acc=0.3326  val_acc=0.3086\n",
      "  Epoch 37/100  train_acc=0.3376  val_acc=0.3054\n",
      "  Epoch 38/100  train_acc=0.3339  val_acc=0.2896\n",
      "  Epoch 39/100  train_acc=0.3398  val_acc=0.3032\n",
      "[Checkpoint] Saving round 40...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_30.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_40.pth\n",
      "  Epoch 40/100  train_acc=0.3475  val_acc=0.2990\n",
      "  Epoch 41/100  train_acc=0.3472  val_acc=0.3092\n",
      "  Epoch 42/100  train_acc=0.3454  val_acc=0.3186\n",
      "  Epoch 43/100  train_acc=0.3523  val_acc=0.3092\n",
      "  Epoch 44/100  train_acc=0.3565  val_acc=0.3196\n",
      "  Epoch 45/100  train_acc=0.3555  val_acc=0.3178\n",
      "  Epoch 46/100  train_acc=0.3628  val_acc=0.3124\n",
      "  Epoch 47/100  train_acc=0.3641  val_acc=0.3264\n",
      "  Epoch 48/100  train_acc=0.3682  val_acc=0.3122\n",
      "  Epoch 49/100  train_acc=0.3650  val_acc=0.3254\n",
      "[Checkpoint] Saving round 50...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_40.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_50.pth\n",
      "  Epoch 50/100  train_acc=0.3756  val_acc=0.3274\n",
      "  Epoch 51/100  train_acc=0.3710  val_acc=0.3382\n",
      "  Epoch 52/100  train_acc=0.3772  val_acc=0.3440\n",
      "  Epoch 53/100  train_acc=0.3806  val_acc=0.3382\n",
      "  Epoch 54/100  train_acc=0.3864  val_acc=0.3318\n",
      "  Epoch 55/100  train_acc=0.3907  val_acc=0.3540\n",
      "  Epoch 56/100  train_acc=0.3901  val_acc=0.3608\n",
      "  Epoch 57/100  train_acc=0.3954  val_acc=0.3530\n",
      "  Epoch 58/100  train_acc=0.4013  val_acc=0.3478\n",
      "  Epoch 59/100  train_acc=0.4032  val_acc=0.3640\n",
      "[Checkpoint] Saving round 60...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_50.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_60.pth\n",
      "  Epoch 60/100  train_acc=0.4098  val_acc=0.3526\n",
      "  Epoch 61/100  train_acc=0.4148  val_acc=0.3554\n",
      "  Epoch 62/100  train_acc=0.4193  val_acc=0.3432\n",
      "  Epoch 63/100  train_acc=0.4235  val_acc=0.3598\n",
      "  Epoch 64/100  train_acc=0.4294  val_acc=0.3742\n",
      "  Epoch 65/100  train_acc=0.4398  val_acc=0.3690\n",
      "  Epoch 66/100  train_acc=0.4383  val_acc=0.3728\n",
      "  Epoch 67/100  train_acc=0.4494  val_acc=0.3840\n",
      "  Epoch 68/100  train_acc=0.4559  val_acc=0.3836\n",
      "  Epoch 69/100  train_acc=0.4619  val_acc=0.3980\n",
      "[Checkpoint] Saving round 70...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_60.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_70.pth\n",
      "  Epoch 70/100  train_acc=0.4681  val_acc=0.3818\n",
      "  Epoch 71/100  train_acc=0.4750  val_acc=0.3984\n",
      "  Epoch 72/100  train_acc=0.4802  val_acc=0.3904\n",
      "  Epoch 73/100  train_acc=0.4910  val_acc=0.4060\n",
      "  Epoch 74/100  train_acc=0.5032  val_acc=0.4074\n",
      "  Epoch 75/100  train_acc=0.5114  val_acc=0.4188\n",
      "  Epoch 76/100  train_acc=0.5142  val_acc=0.4214\n",
      "  Epoch 77/100  train_acc=0.5293  val_acc=0.4212\n",
      "  Epoch 78/100  train_acc=0.5368  val_acc=0.4214\n",
      "  Epoch 79/100  train_acc=0.5475  val_acc=0.4464\n",
      "[Checkpoint] Saving round 80...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_70.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_80.pth\n",
      "  Epoch 80/100  train_acc=0.5595  val_acc=0.4384\n",
      "  Epoch 81/100  train_acc=0.5699  val_acc=0.4566\n",
      "  Epoch 82/100  train_acc=0.5781  val_acc=0.4516\n",
      "  Epoch 83/100  train_acc=0.5883  val_acc=0.4500\n",
      "  Epoch 84/100  train_acc=0.5998  val_acc=0.4602\n",
      "  Epoch 85/100  train_acc=0.6095  val_acc=0.4596\n",
      "  Epoch 86/100  train_acc=0.6226  val_acc=0.4636\n",
      "  Epoch 87/100  train_acc=0.6308  val_acc=0.4698\n",
      "  Epoch 88/100  train_acc=0.6416  val_acc=0.4768\n",
      "  Epoch 89/100  train_acc=0.6528  val_acc=0.4828\n",
      "[Checkpoint] Saving round 90...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_80.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_90.pth\n",
      "  Epoch 90/100  train_acc=0.6662  val_acc=0.4834\n",
      "  Epoch 91/100  train_acc=0.6763  val_acc=0.4876\n",
      "  Epoch 92/100  train_acc=0.6827  val_acc=0.4926\n",
      "  Epoch 93/100  train_acc=0.6892  val_acc=0.5004\n",
      "  Epoch 94/100  train_acc=0.6949  val_acc=0.4966\n",
      "  Epoch 95/100  train_acc=0.7012  val_acc=0.5030\n",
      "  Epoch 96/100  train_acc=0.7048  val_acc=0.4952\n",
      "  Epoch 97/100  train_acc=0.7096  val_acc=0.4986\n",
      "  Epoch 98/100  train_acc=0.7126  val_acc=0.5022\n",
      "  Epoch 99/100  train_acc=0.7169  val_acc=0.4972\n",
      "Filtered and merged results written to results_only_cifar2_scheduler_OneCycleLR.csv\n",
      "[Checkpoint] Saving round 100...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_OneCycleLR_round_90.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_OneCycleLR_round_100.pth\n",
      "  Epoch 100/100  train_acc=0.7172  val_acc=0.4974\n",
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 100} with cifar2_scheduler_OneCycleLR → best_val_acc=0.5030, test_acc=0.5324\n",
      "\n",
      "▶ Running cifar2_scheduler_CosineAnnealingWarmRestarts with config: lr=0.1, wd=0.0001, bs=128, epochs=100\n",
      "[Checkpoint] No checkpoint found; training from scratch.\n",
      "  Epoch 1/100  train_acc=0.0566  val_acc=0.0762\n",
      "  Epoch 2/100  train_acc=0.0959  val_acc=0.1140\n",
      "  Epoch 3/100  train_acc=0.1252  val_acc=0.1288\n",
      "  Epoch 4/100  train_acc=0.1551  val_acc=0.1630\n",
      "  Epoch 5/100  train_acc=0.1847  val_acc=0.2016\n",
      "  Epoch 6/100  train_acc=0.2235  val_acc=0.2402\n",
      "  Epoch 7/100  train_acc=0.2608  val_acc=0.2604\n",
      "  Epoch 8/100  train_acc=0.3068  val_acc=0.3092\n",
      "  Epoch 9/100  train_acc=0.3456  val_acc=0.3264\n",
      "[Checkpoint] Saving round 10...\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_10.pth\n",
      "  Epoch 10/100  train_acc=0.3767  val_acc=0.3436\n",
      "  Epoch 11/100  train_acc=0.1805  val_acc=0.1966\n",
      "  Epoch 12/100  train_acc=0.1729  val_acc=0.1646\n",
      "  Epoch 13/100  train_acc=0.1765  val_acc=0.1850\n",
      "  Epoch 14/100  train_acc=0.1907  val_acc=0.1764\n",
      "  Epoch 15/100  train_acc=0.1960  val_acc=0.1940\n",
      "  Epoch 16/100  train_acc=0.2118  val_acc=0.2156\n",
      "  Epoch 17/100  train_acc=0.2217  val_acc=0.2392\n",
      "  Epoch 18/100  train_acc=0.2416  val_acc=0.2332\n",
      "  Epoch 19/100  train_acc=0.2572  val_acc=0.2672\n",
      "[Checkpoint] Saving round 20...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_10.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_20.pth\n",
      "  Epoch 20/100  train_acc=0.2794  val_acc=0.2742\n",
      "  Epoch 21/100  train_acc=0.3038  val_acc=0.2998\n",
      "  Epoch 22/100  train_acc=0.3268  val_acc=0.3120\n",
      "  Epoch 23/100  train_acc=0.3532  val_acc=0.3188\n",
      "  Epoch 24/100  train_acc=0.3775  val_acc=0.3466\n",
      "  Epoch 25/100  train_acc=0.4045  val_acc=0.3636\n",
      "  Epoch 26/100  train_acc=0.4291  val_acc=0.3728\n",
      "  Epoch 27/100  train_acc=0.4534  val_acc=0.3860\n",
      "  Epoch 28/100  train_acc=0.4693  val_acc=0.4012\n",
      "  Epoch 29/100  train_acc=0.4787  val_acc=0.4066\n",
      "[Checkpoint] Saving round 30...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_20.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_30.pth\n",
      "  Epoch 30/100  train_acc=0.4862  val_acc=0.4098\n",
      "  Epoch 31/100  train_acc=0.2312  val_acc=0.2114\n",
      "  Epoch 32/100  train_acc=0.2189  val_acc=0.1984\n",
      "  Epoch 33/100  train_acc=0.2107  val_acc=0.2042\n",
      "  Epoch 34/100  train_acc=0.2084  val_acc=0.2134\n",
      "  Epoch 35/100  train_acc=0.2091  val_acc=0.2170\n",
      "  Epoch 36/100  train_acc=0.2152  val_acc=0.2144\n",
      "  Epoch 37/100  train_acc=0.2131  val_acc=0.2002\n",
      "  Epoch 38/100  train_acc=0.2236  val_acc=0.2208\n",
      "  Epoch 39/100  train_acc=0.2276  val_acc=0.2186\n",
      "[Checkpoint] Saving round 40...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_30.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_40.pth\n",
      "  Epoch 40/100  train_acc=0.2361  val_acc=0.2264\n",
      "  Epoch 41/100  train_acc=0.2440  val_acc=0.2378\n",
      "  Epoch 42/100  train_acc=0.2505  val_acc=0.2330\n",
      "  Epoch 43/100  train_acc=0.2614  val_acc=0.2482\n",
      "  Epoch 44/100  train_acc=0.2723  val_acc=0.2582\n",
      "  Epoch 45/100  train_acc=0.2811  val_acc=0.2658\n",
      "  Epoch 46/100  train_acc=0.2924  val_acc=0.2798\n",
      "  Epoch 47/100  train_acc=0.3030  val_acc=0.2964\n",
      "  Epoch 48/100  train_acc=0.3164  val_acc=0.2956\n",
      "  Epoch 49/100  train_acc=0.3277  val_acc=0.3030\n",
      "[Checkpoint] Saving round 50...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_40.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_50.pth\n",
      "  Epoch 50/100  train_acc=0.3457  val_acc=0.3156\n",
      "  Epoch 51/100  train_acc=0.3569  val_acc=0.3300\n",
      "  Epoch 52/100  train_acc=0.3723  val_acc=0.3336\n",
      "  Epoch 53/100  train_acc=0.3870  val_acc=0.3374\n",
      "  Epoch 54/100  train_acc=0.4004  val_acc=0.3572\n",
      "  Epoch 55/100  train_acc=0.4168  val_acc=0.3668\n",
      "  Epoch 56/100  train_acc=0.4322  val_acc=0.3730\n",
      "  Epoch 57/100  train_acc=0.4435  val_acc=0.4002\n",
      "  Epoch 58/100  train_acc=0.4651  val_acc=0.3888\n",
      "  Epoch 59/100  train_acc=0.4790  val_acc=0.4078\n",
      "[Checkpoint] Saving round 60...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_50.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_60.pth\n",
      "  Epoch 60/100  train_acc=0.4924  val_acc=0.4060\n",
      "  Epoch 61/100  train_acc=0.5030  val_acc=0.4116\n",
      "  Epoch 62/100  train_acc=0.5180  val_acc=0.4212\n",
      "  Epoch 63/100  train_acc=0.5317  val_acc=0.4288\n",
      "  Epoch 64/100  train_acc=0.5409  val_acc=0.4262\n",
      "  Epoch 65/100  train_acc=0.5481  val_acc=0.4348\n",
      "  Epoch 66/100  train_acc=0.5575  val_acc=0.4336\n",
      "  Epoch 67/100  train_acc=0.5600  val_acc=0.4448\n",
      "  Epoch 68/100  train_acc=0.5668  val_acc=0.4454\n",
      "  Epoch 69/100  train_acc=0.5719  val_acc=0.4350\n",
      "[Checkpoint] Saving round 70...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_60.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_70.pth\n",
      "  Epoch 70/100  train_acc=0.5714  val_acc=0.4500\n",
      "  Epoch 71/100  train_acc=0.2621  val_acc=0.2136\n",
      "  Epoch 72/100  train_acc=0.2440  val_acc=0.2244\n",
      "  Epoch 73/100  train_acc=0.2335  val_acc=0.2248\n",
      "  Epoch 74/100  train_acc=0.2366  val_acc=0.2328\n",
      "  Epoch 75/100  train_acc=0.2304  val_acc=0.2220\n",
      "  Epoch 76/100  train_acc=0.2296  val_acc=0.1976\n",
      "  Epoch 77/100  train_acc=0.2259  val_acc=0.2140\n",
      "  Epoch 78/100  train_acc=0.2292  val_acc=0.2038\n",
      "  Epoch 79/100  train_acc=0.2317  val_acc=0.2146\n",
      "[Checkpoint] Saving round 80...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_70.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_80.pth\n",
      "  Epoch 80/100  train_acc=0.2329  val_acc=0.2274\n",
      "  Epoch 81/100  train_acc=0.2333  val_acc=0.1930\n",
      "  Epoch 82/100  train_acc=0.2407  val_acc=0.2398\n",
      "  Epoch 83/100  train_acc=0.2432  val_acc=0.2452\n",
      "  Epoch 84/100  train_acc=0.2498  val_acc=0.2364\n",
      "  Epoch 85/100  train_acc=0.2476  val_acc=0.2358\n",
      "  Epoch 86/100  train_acc=0.2527  val_acc=0.2398\n",
      "  Epoch 87/100  train_acc=0.2610  val_acc=0.2410\n",
      "  Epoch 88/100  train_acc=0.2623  val_acc=0.2408\n",
      "  Epoch 89/100  train_acc=0.2659  val_acc=0.2452\n",
      "[Checkpoint] Saving round 90...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_80.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_90.pth\n",
      "  Epoch 90/100  train_acc=0.2658  val_acc=0.2324\n",
      "  Epoch 91/100  train_acc=0.2719  val_acc=0.2292\n",
      "  Epoch 92/100  train_acc=0.2836  val_acc=0.2604\n",
      "  Epoch 93/100  train_acc=0.2839  val_acc=0.2816\n",
      "  Epoch 94/100  train_acc=0.2896  val_acc=0.2626\n",
      "  Epoch 95/100  train_acc=0.2978  val_acc=0.2802\n",
      "  Epoch 96/100  train_acc=0.2973  val_acc=0.2702\n",
      "  Epoch 97/100  train_acc=0.3095  val_acc=0.2868\n",
      "  Epoch 98/100  train_acc=0.3209  val_acc=0.2812\n",
      "  Epoch 99/100  train_acc=0.3241  val_acc=0.2864\n",
      "Filtered and merged results written to results_only_cifar2_scheduler_CosineAnnealingWarmRestarts.csv\n",
      "[Checkpoint] Saving round 100...\n",
      "  Deleted: last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_90.pth\n",
      "[Checkpoint] Done saving to last_ckpt_cifar2_scheduler_CosineAnnealingWarmRestarts_round_100.pth\n",
      "  Epoch 100/100  train_acc=0.3303  val_acc=0.2964\n",
      "Config {'lr': 0.1, 'weight_decay': 0.0001, 'batch_size': 128, 'epochs': 100} with cifar2_scheduler_CosineAnnealingWarmRestarts → best_val_acc=0.4500, test_acc=0.3361\n"
     ]
    }
   ],
   "source": [
    "# ── A. Hyperparameter Grid ─────────────────────────────────────────────────────\n",
    "param_grid = [\n",
    "    {'lr': 0.1, 'weight_decay': 1e-4, 'batch_size': 128, 'epochs': 100},\n",
    "    # … add more combinations as needed …\n",
    "]\n",
    "\n",
    "# Only testing new schedulers here\n",
    "scheduler_classes = [\n",
    "    torch.optim.lr_scheduler.OneCycleLR,\n",
    "    torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "]\n",
    "\n",
    "for sched_class in scheduler_classes:\n",
    "    sched_name = f\"cifar2_scheduler_{sched_class.__name__}\"\n",
    "\n",
    "    for cfg in param_grid:\n",
    "        lr, wd, bs, epochs = cfg['lr'], cfg['weight_decay'], cfg['batch_size'], cfg['epochs']\n",
    "        print(f\"\\n▶ Running {sched_name} with config: lr={lr}, wd={wd}, bs={bs}, epochs={epochs}\")\n",
    "\n",
    "        # Fresh model/optimizer/etc. for each config\n",
    "        model     = LELeNetCIFAR().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Re-create DataLoaders per batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "        val_loader   = DataLoader(val_dataset,   batch_size=bs, shuffle=False, num_workers=2)\n",
    "        test_loader  = DataLoader(test_dataset,  batch_size=bs, shuffle=False, num_workers=2)\n",
    "\n",
    "        # Instantiate scheduler AFTER train_loader is created\n",
    "        if sched_class == torch.optim.lr_scheduler.OneCycleLR:\n",
    "            scheduler = sched_class(\n",
    "                optimizer,\n",
    "                max_lr=lr,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                epochs=epochs\n",
    "            )\n",
    "        elif sched_class == torch.optim.lr_scheduler.CosineAnnealingWarmRestarts:\n",
    "            scheduler = sched_class(optimizer, T_0=10, T_mult=2)\n",
    "        else:\n",
    "            scheduler = sched_class(optimizer)  # fallback\n",
    "\n",
    "        start_round = load_checkpoint(model, optimizer, CKPT_DIR, scheduler, resume=True, name=sched_name)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=f'./logs/{sched_name}_lr{lr}_wd{wd}_bs{bs}_ep{epochs}')\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        for epoch in range(start_round, epochs + 1):\n",
    "            # Training loop (modified for OneCycleLR per-batch stepping)\n",
    "            model.train()\n",
    "            train_loss, correct, total = 0.0, 0, 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # LR scheduling per-batch if OneCycleLR\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            train_acc = correct / total\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            # Validation + test\n",
    "            val_loss,   val_acc   = eval_model(model, criterion, val_loader)\n",
    "            test_loss, test_acc   = eval_model(model, criterion, test_loader)\n",
    "\n",
    "            log_results(sched_name, epoch, epochs, val_loss, val_acc, test_loss, test_acc,\n",
    "                        train_loss, train_acc, -1, -1, params=cfg)\n",
    "\n",
    "            # Epoch-based scheduler stepping\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "                scheduler.step(epoch)\n",
    "\n",
    "            # TensorBoard logging\n",
    "            writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "            writer.add_scalars('Acc',  {'train': train_acc,  'val': val_acc}, epoch)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if epoch % 10 == 0 or epoch == epochs:\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, is_best=False,\n",
    "                                ckpt_dir=CKPT_DIR, personalized_par_string=sched_name)\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            print(f\"  Epoch {epoch}/{epochs}  train_acc={train_acc:.4f}  val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # Final Test Evaluation\n",
    "        test_loss, test_acc = eval_model(model, criterion, test_loader)\n",
    "        print(f\"Config {cfg} with {sched_name} → best_val_acc={best_val_acc:.4f}, test_acc={test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Qx5SLtcQggbL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     71\u001b[39m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# ── Example Usage ────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# After defining `cfg`, DataLoaders, and `writer` in your Run Cell, just call:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m summarize_run(\u001b[43mcfg\u001b[49m, train_loader, val_loader, test_loader, writer)\n",
      "\u001b[31mNameError\u001b[39m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "# ── Configuration Summary Cell ──────────────────────────────────────────────────\n",
    "import torch, torchvision, sys, platform, time, os\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def summarize_run(cfg, train_loader, val_loader, test_loader, writer=None):\n",
    "    \"\"\"\n",
    "    Print and log a full summary of the current run configuration and environment.\n",
    "\n",
    "    Args:\n",
    "        cfg (dict): Hyperparameter dict with 'lr', 'weight_decay', 'batch_size', 'epochs', etc.\n",
    "        train_loader, val_loader, test_loader: DataLoaders for computing dataset sizes.\n",
    "        writer (SummaryWriter, optional): if provided, logs summary to TensorBoard under 'RunInfo'.\n",
    "    \"\"\"\n",
    "    # 1. Timestamp\n",
    "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "    # 2. Environment\n",
    "    env_info = {\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'torchvision_version': torchvision.__version__,\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'cuda_device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only',\n",
    "        'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        'platform': platform.platform(),\n",
    "        'cwd': os.getcwd(),\n",
    "    }\n",
    "\n",
    "    # 3. Data sizes\n",
    "    data_info = {\n",
    "        'train_samples': len(train_loader.dataset),\n",
    "        'val_samples': len(val_loader.dataset),\n",
    "        'test_samples': len(test_loader.dataset),\n",
    "        'batch_size': cfg.get('batch_size'),\n",
    "        'num_batches_train': len(train_loader),\n",
    "        'num_batches_val': len(val_loader),\n",
    "        'num_batches_test': len(test_loader),\n",
    "    }\n",
    "\n",
    "    # 4. Seed & Hyperparams\n",
    "    seed_info = {\n",
    "        'seed': cfg.get('seed', 'not set'),\n",
    "    }\n",
    "    hyperparams = {k: v for k, v in cfg.items() if k not in seed_info}\n",
    "\n",
    "    # 5. Print Summary\n",
    "    print(f\"{'='*20} RUN SUMMARY ({ts}) {'='*20}\\n\")\n",
    "    print(\"➜ Environment:\")\n",
    "    for k, v in env_info.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(\"\\n➜ Data:\")\n",
    "    for k, v in data_info.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(\"\\n➜ Seed:\")\n",
    "    for k, v in seed_info.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(\"\\n➜ Hyperparameters:\")\n",
    "    for k, v in hyperparams.items():\n",
    "        print(f\"    • {k}: {v}\")\n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "    # 6. Optional TensorBoard Logging\n",
    "    if writer is not None:\n",
    "        for k, v in {**env_info, **data_info, **seed_info, **hyperparams}.items():\n",
    "            # Non-numeric values will be logged as text under a scalar tag\n",
    "            try:\n",
    "                writer.add_text('RunInfo/' + k, str(v), 0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ── Example Usage ────────────────────────────────────────────────────────────────\n",
    "# After defining `cfg`, DataLoaders, and `writer` in your Run Cell, just call:\n",
    "summarize_run(cfg, train_loader, val_loader, test_loader, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMYuWSfTdWtt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./results_grid.csv')\n",
    "display(df.sort_values('test_acc', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rNf-hPZdWuf"
   },
   "outputs": [],
   "source": [
    "# ── Final Analysis & Plotting ────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load results\n",
    "csv_path = './results_grid.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. Display top 5 configs by test accuracy\n",
    "top5 = df.sort_values('test_acc', ascending=False).head(5)\n",
    "print(\"Top 5 hyperparameter configurations:\")\n",
    "display(top5)\n",
    "\n",
    "# 3. Bar plot of test accuracy for each config\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    x=range(len(df)),\n",
    "    height=df['test_acc'],\n",
    "    tick_label=[f\"lr={lr}\\nwd={wd}\\nbs={bs}\\nep={ep}\"\n",
    "                for lr, wd, bs, ep in zip(df['lr'], df['weight_decay'], df['batch_size'], df['epochs'])]\n",
    ")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Grid Search Results: Test Accuracy by Hyperparameter Configuration')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZm-lbwPdWvT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Owzk--gFdWwB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5IgnGhEdWw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1O3ub6UdWxr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMO88ACkR5/uTJX6n18OJVi",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "advanced-machine-learning-labs-ROHCwjr8-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
