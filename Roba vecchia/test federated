{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNvmr6KvXz60pIjBeV3KArw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Nuova sezione"],"metadata":{"id":"OWAylmyMqIXD"}},{"cell_type":"code","source":["\"\"\"\n","\n","include Cifar 100 as a dataset and perform a validation splitting\n","\n","\n","Federated Client Splitting:\n","\n","IID Sharding: Randomly split the training data into K disjoint subsets, ensuring that each client receives roughly an equal number of samples uniformly drawn from all classes.\n","\n","Non-IID Sharding: Decide on the hyperparameter Nc (number of classes per client).\n","For each client, select Nc classes and assign samples only from those classes.\n","\n","Ensure that the number of samples per client is balanced as much as possible. This may require a preliminary grouping by class before partitioning.\n","\n","\"\"\""],"metadata":{"id":"E8NAS3OfqQdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"sBnBGUwt1bU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745917562658,"user_tz":-120,"elapsed":48190,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}},"outputId":"cd1e45ed-b62e-427b-876e-c8848d18689f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:14<00:00, 11.6MB/s]\n"]}],"source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import random_split, Subset\n","import numpy as np\n","\n","# Define transformation\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","])\n","\n","# Download CIFAR-100 dataset\n","full_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n","\n","# Create a validation split (e.g., 10% for validation)\n","val_size = int(0.1 * len(full_dataset))\n","train_size = len(full_dataset) - val_size\n","train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","# IID Sharding: split train_dataset into K clients\n","def split_dataset_iid(dataset, num_clients):\n","    indices = np.random.permutation(len(dataset))\n","    split_size = len(dataset) // num_clients\n","    client_indices = [indices[i * split_size:(i + 1) * split_size] for i in range(num_clients)]\n","    return [Subset(dataset, inds) for inds in client_indices]\n","\n","# Non-IID Sharding: assign Nc classes per client\n","def split_dataset_non_iid(dataset, num_clients, Nc):\n","    # Group indices by class\n","    class_indices = {i: [] for i in range(100)}\n","    for idx, (_, target) in enumerate(dataset):\n","        class_indices[target].append(idx)\n","\n","    client_subsets = []\n","    classes = list(range(100))\n","    for _ in range(num_clients):\n","        selected_classes = np.random.choice(classes, Nc, replace=False)\n","        client_idx = []\n","        for cls in selected_classes:\n","            # Ensure balanced assignment from each selected class\n","            cls_indices = class_indices[cls]\n","            num_samples = len(cls_indices) // num_clients\n","            client_idx.extend(cls_indices[:num_samples])\n","            class_indices[cls] = cls_indices[num_samples:]\n","        client_subsets.append(Subset(dataset, client_idx))\n","\n","    return client_subsets\n","\n","# Usage:\n","num_clients = 10  # example value\n","iid_clients = split_dataset_iid(train_dataset, num_clients)\n","non_iid_clients = split_dataset_non_iid(train_dataset, num_clients, Nc=1)  # for extreme non-iid case\n"]},{"cell_type":"code","source":["\"\"\"\n","model implementation according to section 6.5 of paper 11\n","\n","\n","\"For CIFAR-10 and CIFAR-100 experiments, we use a CNN similar to LeNet-5 which has:\n","\n","Two 5×5 convolution layers with 64 filters each,\n","Each followed by 2×2 max pooling,\n","Two fully-connected layers with 384 and 192 units respectively,\n","A final softmax classifier.\"\n","\n","This model is not state-of-the-art, but is sufficient to show relative performance.\n","\n","\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"DlLcc0zegSds","executionInfo":{"status":"ok","timestamp":1744218195985,"user_tz":-120,"elapsed":12,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}},"outputId":"4b8c517d-0b28-48e0-ba17-17fe4e6453b0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nmodel implementation according to section 6.5 of paper 11\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LeNetLikeCNN(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(LeNetLikeCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # CIFAR-100 input is 32x32x3 → compute the size after conv + pool layers\n","        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n","        self.fc2 = nn.Linear(384, 192)\n","        self.fc3 = nn.Linear(192, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x)))  # [B, 64, 14, 14]\n","        x = self.pool2(F.relu(self.conv2(x)))  # [B, 64, 5, 5]\n","        x = x.view(-1, 64 * 5 * 5)             # flatten\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"aWvbrvIEgR4A","executionInfo":{"status":"ok","timestamp":1745918282141,"user_tz":-120,"elapsed":35,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","Model Training:\n","Use Stochastic Gradient Descent with Momentum (SGDM) as your optimizer.\n","Experiment with the cosine annealing learning rate scheduler as it’s known to help with convergence.\n","\n","\n","Key Considerations:\n","Hyperparameter Tuning: Search for the best learning rate, momentum, weight decay, and batch size. You can use the validation set for this purpose.\n","Checkpointing: Save model checkpoints periodically. In PyTorch, you can use torch.save() to save the state dictionaries for the model and optimizer.\n","Experiment Logging: Tools like TensorBoard or Weights & Biases can be very useful. Log training and validation loss and accuracy. Also, save the learning rate values and any scheduler information so that you can analyze the effect of the scheduler.\n","\n","\n","\n","\"\"\""],"metadata":{"id":"1C3DCYVvqogk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"e9OYualCqGxn"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Assume model is defined in your model module\n","model = MyModel()  # Replace with your model\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n","scheduler = CosineAnnealingLR(optimizer, T_max=100)  # Adjust T_max based on epochs\n","\n","# Setup logging\n","writer = SummaryWriter(log_dir='./logs')\n","\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for inputs, labels in train_loader:  # train_loader from DataLoader\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    scheduler.step()\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:  # val_loader from DataLoader\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            val_total += labels.size(0)\n","            val_correct += predicted.eq(labels).sum().item()\n","\n","    # Log metrics\n","    writer.add_scalar('Loss/train', train_loss/len(train_loader), epoch)\n","    writer.add_scalar('Loss/val', val_loss/len(val_loader), epoch)\n","    writer.add_scalar('Accuracy/train', 100.*correct/total, epoch)\n","    writer.add_scalar('Accuracy/val', 100.*val_correct/val_total, epoch)\n","\n","    # Save checkpoints periodically\n","    if epoch % 10 == 0:\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","        }, f'./checkpoints/model_epoch_{epoch}.pth')\n","\n","writer.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"Aj0ADIlvg3RL","executionInfo":{"status":"error","timestamp":1745918288591,"user_tz":-120,"elapsed":4123,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}},"outputId":"e103260a-2c8a-497c-821c-ee2f37144f2f"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'MyModel' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-43a9ea6b50f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assume model is defined in your model module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MyModel' is not defined"]}]},{"cell_type":"code","source":["\"\"\"\n","4: Questions to Explore:\n","\n","Number of Epochs:\n","How many epochs are required to reach convergence? Experiment with different training durations.\n","\n","Learning Rate Schedulers:\n","While you are using cosine annealing, you might compare its performance against other schedulers (like step decay or exponential decay) to see which performs best in your setting.\n","\n","Effect on Test Loss and Accuracy:\n","Plot and compare these metrics over training epochs. Look for overfitting, convergence speed, and stability.\n","\n","\n","\n","\"\"\""],"metadata":{"id":"BPn3hCZ6g3TT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pXhWka0Ag3VP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","import numpy as np\n","import os\n","\n","# -----------------------------\n","# 1. Dataset Preparation\n","# -----------------------------\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","])\n","\n","# Download CIFAR-100 dataset\n","full_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n","\n","# Create a validation split (10% validation)\n","val_size = int(0.1 * len(full_dataset))\n","train_size = len(full_dataset) - val_size\n","train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","# IID Sharding: split train_dataset among clients\n","def split_dataset_iid(dataset, num_clients):\n","    indices = np.random.permutation(len(dataset))\n","    split_size = len(dataset) // num_clients\n","    client_indices = [indices[i * split_size:(i + 1) * split_size] for i in range(num_clients)]\n","    return [Subset(dataset, inds) for inds in client_indices]\n","\n","# Non-IID Sharding: assign Nc classes per client (for extreme non-IID use Nc=1)\n","def split_dataset_non_iid(dataset, num_clients, Nc):\n","    # Group indices by class label\n","    class_indices = {i: [] for i in range(100)}\n","    for idx, (_, target) in enumerate(dataset):\n","        class_indices[target].append(idx)\n","    client_subsets = []\n","    classes = list(range(100))\n","    for _ in range(num_clients):\n","        selected_classes = np.random.choice(classes, Nc, replace=False)\n","        client_idx = []\n","        for cls in selected_classes:\n","            # Assign an equal number of samples from the selected class per client\n","            cls_indices = class_indices[cls]\n","            num_samples = len(cls_indices) // num_clients\n","            client_idx.extend(cls_indices[:num_samples])\n","            # Remove the used indices\n","            class_indices[cls] = cls_indices[num_samples:]\n","        client_subsets.append(Subset(dataset, client_idx))\n","    return client_subsets\n","\n","# Set number of clients and sharding type here:\n","num_clients = 10  # or 100, based on your experimental setting\n","# For an IID split:\n","client_datasets = split_dataset_iid(train_dataset, num_clients)\n","# For a non-IID split (e.g. Nc=1 for each client):\n","# client_datasets = split_dataset_non_iid(train_dataset, num_clients, Nc=1)\n","\n","# -----------------------------\n","# 2. Model Definition\n","# -----------------------------\n","class LeNetLikeCNN(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(LeNetLikeCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        # After two conv+pool layers, the spatial size becomes 5x5 (for 32x32 input)\n","        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n","        self.fc2 = nn.Linear(384, 192)\n","        self.fc3 = nn.Linear(192, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x)))  # [Batch, 64, 14, 14]\n","        x = self.pool2(F.relu(self.conv2(x)))  # [Batch, 64, 5, 5]\n","        x = x.view(-1, 64 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# -----------------------------\n","# 3. Client Update Function\n","# -----------------------------\n","def client_update(client_model, optimizer, train_loader, criterion, local_epochs=1, device='cpu'):\n","    client_model.train()\n","    for epoch in range(local_epochs):\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = client_model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","    # Return the updated state_dict and the number of data samples\n","    return client_model.state_dict(), len(train_loader.dataset)\n","\n","# -----------------------------\n","# 4. Client Selection Function\n","# -----------------------------\n","def select_clients(client_datasets, fraction=0.1):\n","    num_clients_to_select = max(1, int(len(client_datasets) * fraction))\n","    selected = np.random.choice(len(client_datasets), num_clients_to_select, replace=False)\n","    return [client_datasets[i] for i in selected]\n","\n","# -----------------------------\n","# 5. Aggregation Function\n","# -----------------------------\n","def aggregate_updates(global_model, client_states, client_sizes):\n","    global_state = global_model.state_dict()\n","    # Weighted average of client updates\n","    total_samples = sum(client_sizes)\n","    for key in global_state.keys():\n","        # Initialize to zero tensor\n","        global_state[key] = torch.zeros_like(global_state[key])\n","        # Sum weighted client states\n","        for state, size in zip(client_states, client_sizes):\n","            global_state[key] += state[key] * (size / total_samples)\n","    global_model.load_state_dict(global_state)\n","    return global_model\n","\n","# -----------------------------\n","# 6. Federated Learning Loop\n","# -----------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","global_model = LeNetLikeCNN(num_classes=100).to(device)\n","# Global validation loader for evaluating the global model on the validation set\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Logging and checkpointing setup\n","writer = SummaryWriter(log_dir='./logs')\n","checkpoint_dir = './checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","# Federated learning parameters\n","num_rounds = 200  # Number of communication rounds (adjust as needed)\n","local_epochs = 1  # Number of local epochs per round (or local iterations)\n","client_fraction = 0.5  # Proportion of clients participating per round (e.g., 50%)\n","\n","# Simulate federated rounds\n","for round in range(1, num_rounds+1):\n","    # Select a subset of clients\n","    selected_clients = select_clients(client_datasets, fraction=client_fraction)\n","    client_states = []\n","    client_sizes = []\n","\n","    # For each selected client, create a DataLoader and perform local update\n","    for client_data in selected_clients:\n","        client_loader = DataLoader(client_data, batch_size=64, shuffle=True)\n","        # Clone the global model for local update\n","        client_model = LeNetLikeCNN(num_classes=100).to(device)\n","        client_model.load_state_dict(global_model.state_dict())\n","        # Local optimizer (each client uses the same hyperparameters as centralized)\n","        optimizer = optim.SGD(client_model.parameters(), lr=0.1, momentum=0.9, weight_decay=4e-4)\n","        state_dict, num_samples = client_update(client_model, optimizer, client_loader, criterion, local_epochs, device)\n","        client_states.append(state_dict)\n","        client_sizes.append(num_samples)\n","\n","    # Aggregate client updates into the global model\n","    global_model = aggregate_updates(global_model, client_states, client_sizes)\n","\n","    # Optional: Evaluate the global model on the validation set\n","    global_model.eval()\n","    total, correct, val_loss = 0, 0, 0.0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = global_model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    accuracy = 100. * correct / total\n","    print(f\"Round {round}: Val Loss = {avg_val_loss:.4f}, Val Accuracy = {accuracy:.2f}%\")\n","\n","    writer.add_scalar('FL/Val_Loss', avg_val_loss, round)\n","    writer.add_scalar('FL/Val_Accuracy', accuracy, round)\n","\n","    # Save a checkpoint every 10 rounds\n","    if round % 10 == 0:\n","        torch.save({\n","            'round': round,\n","            'global_model_state': global_model.state_dict(),\n","        }, os.path.join(checkpoint_dir, f'global_model_round_{round}.pth'))\n","\n","writer.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEjOhZE7k7fw","executionInfo":{"status":"ok","timestamp":1744221646859,"user_tz":-120,"elapsed":1507195,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}},"outputId":"7f1af911-dbce-45a7-a9c3-6736e862529f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:14<00:00, 12.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Round 1: Val Loss = 4.5859, Val Accuracy = 2.30%\n","Round 2: Val Loss = 4.4918, Val Accuracy = 3.24%\n","Round 3: Val Loss = 4.5062, Val Accuracy = 2.68%\n","Round 4: Val Loss = 4.5191, Val Accuracy = 1.98%\n","Round 5: Val Loss = 4.4347, Val Accuracy = 3.62%\n","Round 6: Val Loss = 4.3169, Val Accuracy = 5.28%\n","Round 7: Val Loss = 4.2074, Val Accuracy = 7.44%\n","Round 8: Val Loss = 4.1780, Val Accuracy = 7.42%\n","Round 9: Val Loss = 4.1265, Val Accuracy = 9.36%\n","Round 10: Val Loss = 4.0399, Val Accuracy = 10.42%\n","Round 11: Val Loss = 4.0690, Val Accuracy = 9.68%\n","Round 12: Val Loss = 3.9360, Val Accuracy = 11.90%\n","Round 13: Val Loss = 3.9210, Val Accuracy = 11.42%\n","Round 14: Val Loss = 3.8663, Val Accuracy = 12.64%\n","Round 15: Val Loss = 3.8399, Val Accuracy = 13.20%\n","Round 16: Val Loss = 3.7732, Val Accuracy = 13.70%\n","Round 17: Val Loss = 3.7474, Val Accuracy = 15.14%\n","Round 18: Val Loss = 3.6790, Val Accuracy = 15.02%\n","Round 19: Val Loss = 3.6880, Val Accuracy = 15.50%\n","Round 20: Val Loss = 3.6879, Val Accuracy = 15.14%\n","Round 21: Val Loss = 3.6050, Val Accuracy = 16.18%\n","Round 22: Val Loss = 3.6238, Val Accuracy = 16.58%\n","Round 23: Val Loss = 3.5897, Val Accuracy = 17.60%\n","Round 24: Val Loss = 3.5406, Val Accuracy = 17.72%\n","Round 25: Val Loss = 3.5220, Val Accuracy = 18.24%\n","Round 26: Val Loss = 3.4703, Val Accuracy = 19.14%\n","Round 27: Val Loss = 3.5241, Val Accuracy = 18.16%\n","Round 28: Val Loss = 3.4305, Val Accuracy = 18.72%\n","Round 29: Val Loss = 3.3657, Val Accuracy = 20.94%\n","Round 30: Val Loss = 3.4280, Val Accuracy = 20.22%\n","Round 31: Val Loss = 3.4237, Val Accuracy = 20.18%\n","Round 32: Val Loss = 3.3035, Val Accuracy = 21.68%\n","Round 33: Val Loss = 3.4151, Val Accuracy = 21.16%\n","Round 34: Val Loss = 3.3929, Val Accuracy = 21.08%\n","Round 35: Val Loss = 3.3400, Val Accuracy = 21.66%\n","Round 36: Val Loss = 3.3371, Val Accuracy = 22.38%\n","Round 37: Val Loss = 3.2964, Val Accuracy = 22.40%\n","Round 38: Val Loss = 3.3010, Val Accuracy = 22.52%\n","Round 39: Val Loss = 3.2991, Val Accuracy = 22.94%\n","Round 40: Val Loss = 3.2984, Val Accuracy = 22.72%\n","Round 41: Val Loss = 3.3093, Val Accuracy = 22.26%\n","Round 42: Val Loss = 3.3001, Val Accuracy = 22.82%\n","Round 43: Val Loss = 3.2720, Val Accuracy = 22.24%\n","Round 44: Val Loss = 3.2893, Val Accuracy = 22.78%\n","Round 45: Val Loss = 3.2624, Val Accuracy = 22.80%\n","Round 46: Val Loss = 3.2355, Val Accuracy = 23.60%\n","Round 47: Val Loss = 3.2798, Val Accuracy = 23.26%\n","Round 48: Val Loss = 3.2861, Val Accuracy = 23.34%\n","Round 49: Val Loss = 3.2499, Val Accuracy = 23.38%\n","Round 50: Val Loss = 3.1993, Val Accuracy = 23.90%\n","Round 51: Val Loss = 3.2196, Val Accuracy = 24.10%\n","Round 52: Val Loss = 3.1972, Val Accuracy = 24.50%\n","Round 53: Val Loss = 3.2465, Val Accuracy = 23.48%\n","Round 54: Val Loss = 3.1968, Val Accuracy = 24.88%\n","Round 55: Val Loss = 3.1905, Val Accuracy = 24.60%\n","Round 56: Val Loss = 3.1651, Val Accuracy = 24.84%\n","Round 57: Val Loss = 3.1754, Val Accuracy = 25.22%\n","Round 58: Val Loss = 3.2393, Val Accuracy = 23.72%\n","Round 59: Val Loss = 3.2205, Val Accuracy = 24.76%\n","Round 60: Val Loss = 3.2179, Val Accuracy = 23.88%\n","Round 61: Val Loss = 3.2082, Val Accuracy = 24.38%\n","Round 62: Val Loss = 3.2435, Val Accuracy = 24.62%\n","Round 63: Val Loss = 3.1721, Val Accuracy = 25.78%\n","Round 64: Val Loss = 3.2130, Val Accuracy = 25.04%\n","Round 65: Val Loss = 3.2243, Val Accuracy = 24.42%\n","Round 66: Val Loss = 3.1984, Val Accuracy = 24.98%\n","Round 67: Val Loss = 3.2086, Val Accuracy = 23.70%\n","Round 68: Val Loss = 3.1511, Val Accuracy = 25.82%\n","Round 69: Val Loss = 3.1893, Val Accuracy = 25.14%\n","Round 70: Val Loss = 3.1617, Val Accuracy = 26.28%\n","Round 71: Val Loss = 3.1657, Val Accuracy = 25.66%\n","Round 72: Val Loss = 3.1415, Val Accuracy = 25.96%\n","Round 73: Val Loss = 3.1909, Val Accuracy = 25.92%\n","Round 74: Val Loss = 3.1519, Val Accuracy = 25.36%\n","Round 75: Val Loss = 3.2248, Val Accuracy = 24.44%\n","Round 76: Val Loss = 3.2088, Val Accuracy = 25.20%\n","Round 77: Val Loss = 3.2097, Val Accuracy = 25.06%\n","Round 78: Val Loss = 3.1791, Val Accuracy = 25.32%\n","Round 79: Val Loss = 3.2312, Val Accuracy = 24.38%\n","Round 80: Val Loss = 3.2252, Val Accuracy = 24.38%\n","Round 81: Val Loss = 3.1880, Val Accuracy = 25.68%\n","Round 82: Val Loss = 3.2407, Val Accuracy = 24.56%\n","Round 83: Val Loss = 3.2030, Val Accuracy = 25.26%\n","Round 84: Val Loss = 3.2614, Val Accuracy = 24.44%\n","Round 85: Val Loss = 3.1938, Val Accuracy = 24.94%\n","Round 86: Val Loss = 3.1835, Val Accuracy = 26.02%\n","Round 87: Val Loss = 3.1290, Val Accuracy = 27.02%\n","Round 88: Val Loss = 3.1315, Val Accuracy = 26.04%\n","Round 89: Val Loss = 3.1951, Val Accuracy = 25.72%\n","Round 90: Val Loss = 3.2291, Val Accuracy = 25.42%\n","Round 91: Val Loss = 3.2110, Val Accuracy = 24.56%\n","Round 92: Val Loss = 3.1701, Val Accuracy = 25.72%\n","Round 93: Val Loss = 3.1844, Val Accuracy = 25.66%\n","Round 94: Val Loss = 3.1885, Val Accuracy = 25.58%\n","Round 95: Val Loss = 3.1399, Val Accuracy = 25.70%\n","Round 96: Val Loss = 3.2150, Val Accuracy = 25.30%\n","Round 97: Val Loss = 3.2308, Val Accuracy = 24.68%\n","Round 98: Val Loss = 3.1802, Val Accuracy = 26.12%\n","Round 99: Val Loss = 3.1780, Val Accuracy = 25.58%\n","Round 100: Val Loss = 3.2380, Val Accuracy = 25.16%\n","Round 101: Val Loss = 3.2115, Val Accuracy = 25.34%\n","Round 102: Val Loss = 3.1989, Val Accuracy = 25.58%\n","Round 103: Val Loss = 3.2369, Val Accuracy = 24.82%\n","Round 104: Val Loss = 3.2196, Val Accuracy = 25.12%\n","Round 105: Val Loss = 3.2386, Val Accuracy = 24.44%\n","Round 106: Val Loss = 3.2294, Val Accuracy = 25.30%\n","Round 107: Val Loss = 3.1790, Val Accuracy = 25.92%\n","Round 108: Val Loss = 3.2521, Val Accuracy = 24.88%\n","Round 109: Val Loss = 3.2568, Val Accuracy = 24.34%\n","Round 110: Val Loss = 3.2106, Val Accuracy = 25.24%\n","Round 111: Val Loss = 3.1753, Val Accuracy = 26.34%\n","Round 112: Val Loss = 3.1971, Val Accuracy = 25.04%\n","Round 113: Val Loss = 3.1593, Val Accuracy = 26.44%\n","Round 114: Val Loss = 3.2249, Val Accuracy = 25.00%\n","Round 115: Val Loss = 3.2024, Val Accuracy = 25.36%\n","Round 116: Val Loss = 3.2645, Val Accuracy = 24.52%\n","Round 117: Val Loss = 3.2468, Val Accuracy = 24.16%\n","Round 118: Val Loss = 3.2391, Val Accuracy = 24.98%\n","Round 119: Val Loss = 3.2340, Val Accuracy = 25.04%\n","Round 120: Val Loss = 3.1807, Val Accuracy = 25.78%\n","Round 121: Val Loss = 3.2724, Val Accuracy = 24.40%\n","Round 122: Val Loss = 3.2293, Val Accuracy = 25.56%\n","Round 123: Val Loss = 3.2411, Val Accuracy = 24.72%\n","Round 124: Val Loss = 3.2977, Val Accuracy = 24.08%\n","Round 125: Val Loss = 3.2514, Val Accuracy = 24.68%\n","Round 126: Val Loss = 3.2412, Val Accuracy = 24.72%\n","Round 127: Val Loss = 3.2428, Val Accuracy = 25.08%\n","Round 128: Val Loss = 3.2917, Val Accuracy = 24.14%\n","Round 129: Val Loss = 3.2770, Val Accuracy = 23.88%\n","Round 130: Val Loss = 3.2587, Val Accuracy = 25.18%\n","Round 131: Val Loss = 3.2718, Val Accuracy = 24.40%\n","Round 132: Val Loss = 3.2198, Val Accuracy = 25.50%\n","Round 133: Val Loss = 3.2448, Val Accuracy = 25.18%\n","Round 134: Val Loss = 3.2390, Val Accuracy = 25.40%\n","Round 135: Val Loss = 3.2609, Val Accuracy = 25.24%\n","Round 136: Val Loss = 3.2276, Val Accuracy = 25.10%\n","Round 137: Val Loss = 3.2679, Val Accuracy = 24.48%\n","Round 138: Val Loss = 3.2456, Val Accuracy = 25.32%\n","Round 139: Val Loss = 3.2900, Val Accuracy = 24.56%\n","Round 140: Val Loss = 3.3243, Val Accuracy = 24.18%\n","Round 141: Val Loss = 3.2796, Val Accuracy = 24.78%\n","Round 142: Val Loss = 3.2792, Val Accuracy = 24.98%\n","Round 143: Val Loss = 3.2671, Val Accuracy = 25.54%\n","Round 144: Val Loss = 3.2541, Val Accuracy = 25.86%\n","Round 145: Val Loss = 3.2429, Val Accuracy = 25.52%\n","Round 146: Val Loss = 3.2627, Val Accuracy = 24.86%\n","Round 147: Val Loss = 3.2213, Val Accuracy = 25.92%\n","Round 148: Val Loss = 3.2448, Val Accuracy = 25.40%\n","Round 149: Val Loss = 3.2547, Val Accuracy = 25.58%\n","Round 150: Val Loss = 3.2387, Val Accuracy = 26.28%\n","Round 151: Val Loss = 3.2445, Val Accuracy = 25.98%\n","Round 152: Val Loss = 3.2421, Val Accuracy = 25.66%\n","Round 153: Val Loss = 3.2251, Val Accuracy = 26.14%\n","Round 154: Val Loss = 3.2296, Val Accuracy = 25.90%\n","Round 155: Val Loss = 3.2472, Val Accuracy = 25.54%\n","Round 156: Val Loss = 3.2905, Val Accuracy = 24.62%\n","Round 157: Val Loss = 3.3444, Val Accuracy = 24.48%\n","Round 158: Val Loss = 3.2725, Val Accuracy = 24.64%\n","Round 159: Val Loss = 3.3130, Val Accuracy = 24.88%\n","Round 160: Val Loss = 3.2650, Val Accuracy = 25.16%\n","Round 161: Val Loss = 3.2484, Val Accuracy = 25.70%\n","Round 162: Val Loss = 3.3088, Val Accuracy = 24.38%\n","Round 163: Val Loss = 3.3397, Val Accuracy = 25.10%\n","Round 164: Val Loss = 3.2784, Val Accuracy = 25.36%\n","Round 165: Val Loss = 3.3248, Val Accuracy = 25.18%\n","Round 166: Val Loss = 3.3013, Val Accuracy = 24.68%\n","Round 167: Val Loss = 3.3367, Val Accuracy = 24.44%\n","Round 168: Val Loss = 3.3119, Val Accuracy = 24.62%\n","Round 169: Val Loss = 3.2750, Val Accuracy = 24.62%\n","Round 170: Val Loss = 3.3039, Val Accuracy = 25.50%\n","Round 171: Val Loss = 3.3821, Val Accuracy = 23.66%\n","Round 172: Val Loss = 3.3072, Val Accuracy = 24.10%\n","Round 173: Val Loss = 3.2774, Val Accuracy = 25.02%\n","Round 174: Val Loss = 3.3376, Val Accuracy = 24.92%\n","Round 175: Val Loss = 3.2972, Val Accuracy = 24.86%\n","Round 176: Val Loss = 3.2774, Val Accuracy = 25.36%\n","Round 177: Val Loss = 3.2981, Val Accuracy = 24.64%\n","Round 178: Val Loss = 3.3239, Val Accuracy = 24.46%\n","Round 179: Val Loss = 3.2590, Val Accuracy = 25.00%\n","Round 180: Val Loss = 3.3191, Val Accuracy = 24.74%\n","Round 181: Val Loss = 3.2778, Val Accuracy = 25.90%\n","Round 182: Val Loss = 3.3394, Val Accuracy = 25.20%\n","Round 183: Val Loss = 3.3593, Val Accuracy = 23.96%\n","Round 184: Val Loss = 3.3204, Val Accuracy = 24.78%\n","Round 185: Val Loss = 3.3519, Val Accuracy = 23.54%\n","Round 186: Val Loss = 3.3116, Val Accuracy = 24.30%\n","Round 187: Val Loss = 3.3108, Val Accuracy = 23.98%\n","Round 188: Val Loss = 3.2557, Val Accuracy = 25.88%\n","Round 189: Val Loss = 3.3048, Val Accuracy = 24.94%\n","Round 190: Val Loss = 3.3522, Val Accuracy = 23.94%\n","Round 191: Val Loss = 3.2954, Val Accuracy = 25.14%\n","Round 192: Val Loss = 3.3345, Val Accuracy = 25.48%\n","Round 193: Val Loss = 3.3158, Val Accuracy = 25.18%\n","Round 194: Val Loss = 3.2897, Val Accuracy = 25.34%\n","Round 195: Val Loss = 3.3509, Val Accuracy = 24.08%\n","Round 196: Val Loss = 3.3281, Val Accuracy = 25.18%\n","Round 197: Val Loss = 3.3061, Val Accuracy = 25.52%\n","Round 198: Val Loss = 3.3142, Val Accuracy = 25.50%\n","Round 199: Val Loss = 3.2628, Val Accuracy = 25.28%\n","Round 200: Val Loss = 3.3358, Val Accuracy = 24.78%\n"]}]},{"cell_type":"code","source":["#modified code according to chat gpt point one\n","\n","#1. Centralized Baseline vs. FL Simulation\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","import numpy as np\n","import os\n","\n","# -----------------------------\n","# 1. Dataset Preparation\n","# -----------------------------\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","])\n","\n","# For federated simulation, using train split\n","full_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n","val_size = int(0.1 * len(full_dataset))\n","train_size = len(full_dataset) - val_size\n","train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","# Also load the provided test set for centralized baseline evaluation\n","test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n","\n","# IID Sharding for FL simulation (unchanged)\n","def split_dataset_iid(dataset, num_clients):\n","    indices = np.random.permutation(len(dataset))\n","    split_size = len(dataset) // num_clients\n","    client_indices = [indices[i * split_size:(i + 1) * split_size] for i in range(num_clients)]\n","    return [Subset(dataset, inds) for inds in client_indices]\n","\n","# Non-IID Sharding (unchanged)\n","def split_dataset_non_iid(dataset, num_clients, Nc):\n","    class_indices = {i: [] for i in range(100)}\n","    for idx, (_, target) in enumerate(dataset):\n","        class_indices[target].append(idx)\n","    client_subsets = []\n","    classes = list(range(100))\n","    for _ in range(num_clients):\n","        selected_classes = np.random.choice(classes, Nc, replace=False)\n","        client_idx = []\n","        for cls in selected_classes:\n","            cls_indices = class_indices[cls]\n","            num_samples = len(cls_indices) // num_clients\n","            client_idx.extend(cls_indices[:num_samples])\n","            class_indices[cls] = cls_indices[num_samples:]\n","        client_subsets.append(Subset(dataset, client_idx))\n","    return client_subsets\n","\n","# Set number of clients for FL simulation\n","num_clients = 10\n","client_datasets = split_dataset_iid(train_dataset, num_clients)\n","# Uncomment below for non-IID split (e.g. Nc=1)\n","# client_datasets = split_dataset_non_iid(train_dataset, num_clients, Nc=1)\n","\n","# -----------------------------\n","# 2. Model Definition\n","# -----------------------------\n","class LeNetLikeCNN(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(LeNetLikeCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        # For 32x32 inputs, after two conv+pool layers we get 5x5 feature maps\n","        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n","        self.fc2 = nn.Linear(384, 192)\n","        self.fc3 = nn.Linear(192, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x)))  # [B, 64, 14, 14]\n","        x = self.pool2(F.relu(self.conv2(x)))  # [B, 64, 5, 5]\n","        x = x.view(-1, 64 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# -----------------------------\n","# 3. Federated Learning (FL) Simulation (Existing Code)\n","# -----------------------------\n","def client_update(client_model, optimizer, train_loader, criterion, local_epochs=1, device='cpu'):\n","    client_model.train()\n","    for epoch in range(local_epochs):\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = client_model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","    return client_model.state_dict(), len(train_loader.dataset)\n","\n","def select_clients(client_datasets, fraction=0.1):\n","    num_clients_to_select = max(1, int(len(client_datasets) * fraction))\n","    selected = np.random.choice(len(client_datasets), num_clients_to_select, replace=False)\n","    return [client_datasets[i] for i in selected]\n","\n","def aggregate_updates(global_model, client_states, client_sizes):\n","    global_state = global_model.state_dict()\n","    total_samples = sum(client_sizes)\n","    for key in global_state.keys():\n","        global_state[key] = torch.zeros_like(global_state[key])\n","        for state, size in zip(client_states, client_sizes):\n","            global_state[key] += state[key] * (size / total_samples)\n","    global_model.load_state_dict(global_state)\n","    return global_model\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","global_model_FL = LeNetLikeCNN(num_classes=100).to(device)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","criterion = nn.CrossEntropyLoss()\n","writer_FL = SummaryWriter(log_dir='./logs/fl')\n","checkpoint_dir = './checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","num_rounds = 200     # Number of federated rounds\n","local_epochs = 1     # Local epochs per round\n","client_fraction = 0.5  # Fraction of clients participating\n","\n","for round in range(1, num_rounds+1):\n","    selected_clients = select_clients(client_datasets, fraction=client_fraction)\n","    client_states = []\n","    client_sizes = []\n","    for client_data in selected_clients:\n","        client_loader = DataLoader(client_data, batch_size=64, shuffle=True)\n","        client_model = LeNetLikeCNN(num_classes=100).to(device)\n","        client_model.load_state_dict(global_model_FL.state_dict())\n","        optimizer = optim.SGD(client_model.parameters(), lr=0.1, momentum=0.9, weight_decay=4e-4)\n","        state_dict, num_samples = client_update(client_model, optimizer, client_loader, criterion, local_epochs, device)\n","        client_states.append(state_dict)\n","        client_sizes.append(num_samples)\n","    global_model_FL = aggregate_updates(global_model_FL, client_states, client_sizes)\n","\n","    # Evaluate on validation set\n","    global_model_FL.eval()\n","    total, correct, val_loss = 0, 0, 0.0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = global_model_FL(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","    avg_val_loss = val_loss / len(val_loader)\n","    accuracy = 100. * correct / total\n","    print(f\"FL Round {round}: Val Loss = {avg_val_loss:.4f}, Val Accuracy = {accuracy:.2f}%\")\n","    writer_FL.add_scalar('FL/Val_Loss', avg_val_loss, round)\n","    writer_FL.add_scalar('FL/Val_Accuracy', accuracy, round)\n","\n","    if round % 10 == 0:\n","        torch.save({\n","            'round': round,\n","            'global_model_state': global_model_FL.state_dict(),\n","        }, os.path.join(checkpoint_dir, f'global_model_round_{round}.pth'))\n","writer_FL.close()\n","\n","# -----------------------------\n","# 4. Centralized Baseline Training\n","# -----------------------------\n","# We now create a baseline using the entire training set (combining the train_dataset split)\n","# and the provided test set for evaluation.\n","# We will run two separate training loops: one for SGDM (SGD with momentum) and one for AdamW.\n","# Both will use a cosine annealing learning rate scheduler and run for 150 epochs.\n","\n","# Create DataLoaders for centralized training:\n","batch_size = 64\n","central_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","central_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Function to compute accuracy for a given dataloader and model\n","def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    total, correct, loss_sum = 0, 0, 0.0\n","    with torch.no_grad():\n","        for inputs, labels in loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss_sum += loss.item() * labels.size(0)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","    avg_loss = loss_sum / total\n","    accuracy = 100. * correct / total\n","    return avg_loss, accuracy\n","\n","# Centralized training loop\n","def centralized_training(optimizer_type='SGDM'):\n","    model = LeNetLikeCNN(num_classes=100).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    # Choose optimizer:\n","    if optimizer_type == 'SGDM':\n","        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=4e-4)\n","    elif optimizer_type == 'AdamW':\n","        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)  # example lr and weight decay\n","    else:\n","        raise ValueError(\"Unsupported optimizer type. Choose 'SGDM' or 'AdamW'.\")\n","\n","    # Cosine annealing scheduler (you may need to tune T_max based on your dataset)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=150)\n","\n","    # Setup SummaryWriter for centralized training\n","    writer = SummaryWriter(log_dir=f'./logs/central_{optimizer_type}')\n","\n","    num_epochs = 150\n","    for epoch in range(1, num_epochs+1):\n","        model.train()\n","        running_loss = 0.0\n","        running_correct = 0\n","        total_samples = 0\n","\n","        # Training loop\n","        for inputs, labels in central_train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * labels.size(0)\n","            _, predicted = outputs.max(1)\n","            total_samples += labels.size(0)\n","            running_correct += predicted.eq(labels).sum().item()\n","\n","        train_loss = running_loss / total_samples\n","        train_accuracy = 100. * running_correct / total_samples\n","\n","        # Evaluate on validation and test sets\n","        val_loss, val_accuracy = evaluate(model, central_val_loader, criterion, device)\n","        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n","\n","        # Logging\n","        writer.add_scalar('Central/Train_Loss', train_loss, epoch)\n","        writer.add_scalar('Central/Train_Accuracy', train_accuracy, epoch)\n","        writer.add_scalar('Central/Val_Loss', val_loss, epoch)\n","        writer.add_scalar('Central/Val_Accuracy', val_accuracy, epoch)\n","        writer.add_scalar('Central/Test_Loss', test_loss, epoch)\n","        writer.add_scalar('Central/Test_Accuracy', test_accuracy, epoch)\n","\n","        print(f\"[Central-{optimizer_type}] Epoch {epoch}: \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n","              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}% | \"\n","              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n","\n","        scheduler.step()\n","\n","    writer.close()\n","    # Optionally, save the final model\n","    os.makedirs(f'./checkpoints/central_{optimizer_type}', exist_ok=True)\n","    torch.save(model.state_dict(), f'./checkpoints/central_{optimizer_type}/final_model.pth')\n","\n","# Run the centralized baseline with both optimizers:\n","centralized_training(optimizer_type='SGDM')\n","centralized_training(optimizer_type='AdamW')\n","\n"],"metadata":{"id":"_SyNdtFfk7iH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fff3bd5e-7b22-4642-a489-1dc634a8b5a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FL Round 1: Val Loss = 4.5355, Val Accuracy = 3.52%\n","FL Round 2: Val Loss = 4.5297, Val Accuracy = 2.74%\n","FL Round 3: Val Loss = 4.4996, Val Accuracy = 2.62%\n","FL Round 4: Val Loss = 4.5089, Val Accuracy = 2.42%\n","FL Round 5: Val Loss = 4.3139, Val Accuracy = 5.46%\n","FL Round 6: Val Loss = 4.2823, Val Accuracy = 6.20%\n","FL Round 7: Val Loss = 4.2119, Val Accuracy = 7.36%\n","FL Round 8: Val Loss = 4.1118, Val Accuracy = 9.02%\n","FL Round 9: Val Loss = 4.1757, Val Accuracy = 8.12%\n","FL Round 10: Val Loss = 4.0572, Val Accuracy = 9.12%\n","FL Round 11: Val Loss = 4.0188, Val Accuracy = 11.14%\n","FL Round 12: Val Loss = 4.0077, Val Accuracy = 10.38%\n","FL Round 13: Val Loss = 3.9031, Val Accuracy = 13.18%\n","FL Round 14: Val Loss = 3.8808, Val Accuracy = 11.96%\n","FL Round 15: Val Loss = 3.8231, Val Accuracy = 13.38%\n","FL Round 16: Val Loss = 3.9072, Val Accuracy = 12.68%\n","FL Round 17: Val Loss = 3.8069, Val Accuracy = 13.82%\n","FL Round 18: Val Loss = 3.7327, Val Accuracy = 15.42%\n","FL Round 19: Val Loss = 3.7080, Val Accuracy = 15.92%\n","FL Round 20: Val Loss = 3.7191, Val Accuracy = 16.24%\n","FL Round 21: Val Loss = 3.6696, Val Accuracy = 16.66%\n","FL Round 22: Val Loss = 3.6039, Val Accuracy = 16.84%\n","FL Round 23: Val Loss = 3.6087, Val Accuracy = 17.54%\n","FL Round 24: Val Loss = 3.5403, Val Accuracy = 18.30%\n","FL Round 25: Val Loss = 3.6245, Val Accuracy = 17.92%\n","FL Round 26: Val Loss = 3.5022, Val Accuracy = 19.12%\n","FL Round 27: Val Loss = 3.5138, Val Accuracy = 19.76%\n","FL Round 28: Val Loss = 3.4255, Val Accuracy = 20.54%\n","FL Round 29: Val Loss = 3.4253, Val Accuracy = 20.44%\n","FL Round 30: Val Loss = 3.4660, Val Accuracy = 19.68%\n","FL Round 31: Val Loss = 3.3681, Val Accuracy = 21.54%\n","FL Round 32: Val Loss = 3.4144, Val Accuracy = 20.40%\n","FL Round 33: Val Loss = 3.4639, Val Accuracy = 19.90%\n","FL Round 34: Val Loss = 3.2967, Val Accuracy = 22.76%\n","FL Round 35: Val Loss = 3.2706, Val Accuracy = 22.62%\n","FL Round 36: Val Loss = 3.3566, Val Accuracy = 22.36%\n","FL Round 37: Val Loss = 3.3723, Val Accuracy = 21.70%\n","FL Round 38: Val Loss = 3.3085, Val Accuracy = 23.32%\n","FL Round 39: Val Loss = 3.3238, Val Accuracy = 22.66%\n","FL Round 40: Val Loss = 3.2601, Val Accuracy = 24.48%\n","FL Round 41: Val Loss = 3.3146, Val Accuracy = 22.92%\n","FL Round 42: Val Loss = 3.2648, Val Accuracy = 23.92%\n","FL Round 43: Val Loss = 3.2756, Val Accuracy = 23.60%\n","FL Round 44: Val Loss = 3.2638, Val Accuracy = 23.54%\n","FL Round 45: Val Loss = 3.2866, Val Accuracy = 22.94%\n","FL Round 46: Val Loss = 3.2606, Val Accuracy = 23.48%\n","FL Round 47: Val Loss = 3.2307, Val Accuracy = 24.20%\n","FL Round 48: Val Loss = 3.2493, Val Accuracy = 24.52%\n","FL Round 49: Val Loss = 3.2566, Val Accuracy = 24.40%\n","FL Round 50: Val Loss = 3.2526, Val Accuracy = 22.82%\n","FL Round 51: Val Loss = 3.1937, Val Accuracy = 25.26%\n","FL Round 52: Val Loss = 3.2139, Val Accuracy = 25.48%\n","FL Round 53: Val Loss = 3.2353, Val Accuracy = 23.92%\n","FL Round 54: Val Loss = 3.2968, Val Accuracy = 24.28%\n","FL Round 55: Val Loss = 3.2258, Val Accuracy = 25.02%\n","FL Round 56: Val Loss = 3.2073, Val Accuracy = 25.94%\n","FL Round 57: Val Loss = 3.2283, Val Accuracy = 25.10%\n","FL Round 58: Val Loss = 3.2783, Val Accuracy = 24.18%\n","FL Round 59: Val Loss = 3.2565, Val Accuracy = 24.14%\n","FL Round 60: Val Loss = 3.1912, Val Accuracy = 25.28%\n","FL Round 61: Val Loss = 3.2018, Val Accuracy = 24.98%\n","FL Round 62: Val Loss = 3.2196, Val Accuracy = 25.54%\n","FL Round 63: Val Loss = 3.2634, Val Accuracy = 24.64%\n","FL Round 64: Val Loss = 3.2412, Val Accuracy = 24.88%\n","FL Round 65: Val Loss = 3.2766, Val Accuracy = 23.24%\n","FL Round 66: Val Loss = 3.2394, Val Accuracy = 24.08%\n","FL Round 67: Val Loss = 3.2264, Val Accuracy = 25.54%\n","FL Round 68: Val Loss = 3.2878, Val Accuracy = 23.86%\n","FL Round 69: Val Loss = 3.2186, Val Accuracy = 24.62%\n","FL Round 70: Val Loss = 3.3005, Val Accuracy = 24.44%\n","FL Round 71: Val Loss = 3.2461, Val Accuracy = 24.10%\n","FL Round 72: Val Loss = 3.1891, Val Accuracy = 25.20%\n","FL Round 73: Val Loss = 3.2640, Val Accuracy = 24.28%\n","FL Round 74: Val Loss = 3.2297, Val Accuracy = 25.16%\n","FL Round 75: Val Loss = 3.2264, Val Accuracy = 24.56%\n","FL Round 76: Val Loss = 3.2288, Val Accuracy = 24.84%\n","FL Round 77: Val Loss = 3.1854, Val Accuracy = 25.46%\n","FL Round 78: Val Loss = 3.2450, Val Accuracy = 24.62%\n","FL Round 79: Val Loss = 3.2460, Val Accuracy = 24.24%\n","FL Round 80: Val Loss = 3.2578, Val Accuracy = 25.06%\n","FL Round 81: Val Loss = 3.2513, Val Accuracy = 25.36%\n","FL Round 82: Val Loss = 3.2405, Val Accuracy = 25.24%\n","FL Round 83: Val Loss = 3.2460, Val Accuracy = 25.24%\n","FL Round 84: Val Loss = 3.2453, Val Accuracy = 24.90%\n","FL Round 85: Val Loss = 3.2483, Val Accuracy = 25.42%\n","FL Round 86: Val Loss = 3.2574, Val Accuracy = 25.38%\n","FL Round 87: Val Loss = 3.2060, Val Accuracy = 25.74%\n","FL Round 88: Val Loss = 3.3016, Val Accuracy = 23.94%\n","FL Round 89: Val Loss = 3.3503, Val Accuracy = 23.20%\n","FL Round 90: Val Loss = 3.3168, Val Accuracy = 24.38%\n","FL Round 91: Val Loss = 3.2215, Val Accuracy = 25.24%\n","FL Round 92: Val Loss = 3.1817, Val Accuracy = 25.96%\n","FL Round 93: Val Loss = 3.3027, Val Accuracy = 24.42%\n","FL Round 94: Val Loss = 3.2788, Val Accuracy = 24.70%\n","FL Round 95: Val Loss = 3.2452, Val Accuracy = 25.22%\n","FL Round 96: Val Loss = 3.2443, Val Accuracy = 25.60%\n","FL Round 97: Val Loss = 3.3018, Val Accuracy = 25.50%\n","FL Round 98: Val Loss = 3.2560, Val Accuracy = 25.68%\n","FL Round 99: Val Loss = 3.2881, Val Accuracy = 23.96%\n","FL Round 100: Val Loss = 3.2974, Val Accuracy = 24.68%\n","FL Round 101: Val Loss = 3.3759, Val Accuracy = 23.78%\n","FL Round 102: Val Loss = 3.3491, Val Accuracy = 24.32%\n","FL Round 103: Val Loss = 3.2876, Val Accuracy = 25.22%\n","FL Round 104: Val Loss = 3.3053, Val Accuracy = 25.60%\n","FL Round 105: Val Loss = 3.3812, Val Accuracy = 23.12%\n","FL Round 106: Val Loss = 3.3223, Val Accuracy = 24.78%\n","FL Round 107: Val Loss = 3.3275, Val Accuracy = 23.90%\n","FL Round 108: Val Loss = 3.2420, Val Accuracy = 25.54%\n","FL Round 109: Val Loss = 3.2483, Val Accuracy = 25.70%\n","FL Round 110: Val Loss = 3.2761, Val Accuracy = 24.86%\n","FL Round 111: Val Loss = 3.2812, Val Accuracy = 25.30%\n","FL Round 112: Val Loss = 3.2499, Val Accuracy = 24.92%\n","FL Round 113: Val Loss = 3.3061, Val Accuracy = 25.44%\n","FL Round 114: Val Loss = 3.2963, Val Accuracy = 24.66%\n","FL Round 115: Val Loss = 3.3104, Val Accuracy = 24.64%\n","FL Round 116: Val Loss = 3.2811, Val Accuracy = 25.98%\n","FL Round 117: Val Loss = 3.2997, Val Accuracy = 24.70%\n","FL Round 118: Val Loss = 3.2937, Val Accuracy = 24.64%\n","FL Round 119: Val Loss = 3.3176, Val Accuracy = 24.22%\n","FL Round 120: Val Loss = 3.3180, Val Accuracy = 24.76%\n","FL Round 121: Val Loss = 3.3245, Val Accuracy = 25.44%\n","FL Round 122: Val Loss = 3.3064, Val Accuracy = 24.76%\n","FL Round 123: Val Loss = 3.3457, Val Accuracy = 24.50%\n","FL Round 124: Val Loss = 3.2692, Val Accuracy = 24.88%\n","FL Round 125: Val Loss = 3.3899, Val Accuracy = 23.76%\n","FL Round 126: Val Loss = 3.3159, Val Accuracy = 25.04%\n","FL Round 127: Val Loss = 3.3138, Val Accuracy = 25.10%\n","FL Round 128: Val Loss = 3.3300, Val Accuracy = 25.10%\n","FL Round 129: Val Loss = 3.2929, Val Accuracy = 25.46%\n","FL Round 130: Val Loss = 3.3216, Val Accuracy = 24.88%\n","FL Round 131: Val Loss = 3.3303, Val Accuracy = 24.16%\n","FL Round 132: Val Loss = 3.3020, Val Accuracy = 25.48%\n","FL Round 133: Val Loss = 3.3465, Val Accuracy = 24.52%\n","FL Round 134: Val Loss = 3.2844, Val Accuracy = 25.48%\n","FL Round 135: Val Loss = 3.3713, Val Accuracy = 24.44%\n","FL Round 136: Val Loss = 3.2500, Val Accuracy = 26.68%\n","FL Round 137: Val Loss = 3.3222, Val Accuracy = 24.50%\n","FL Round 138: Val Loss = 3.3084, Val Accuracy = 24.58%\n","FL Round 139: Val Loss = 3.3192, Val Accuracy = 25.26%\n","FL Round 140: Val Loss = 3.3820, Val Accuracy = 24.56%\n","FL Round 141: Val Loss = 3.3525, Val Accuracy = 24.68%\n","FL Round 142: Val Loss = 3.2945, Val Accuracy = 25.38%\n","FL Round 143: Val Loss = 3.3280, Val Accuracy = 24.48%\n","FL Round 144: Val Loss = 3.3034, Val Accuracy = 25.70%\n","FL Round 145: Val Loss = 3.3488, Val Accuracy = 24.14%\n","FL Round 146: Val Loss = 3.3082, Val Accuracy = 25.14%\n","FL Round 147: Val Loss = 3.3253, Val Accuracy = 25.52%\n","FL Round 148: Val Loss = 3.3542, Val Accuracy = 25.22%\n","FL Round 149: Val Loss = 3.3326, Val Accuracy = 24.40%\n","FL Round 150: Val Loss = 3.3869, Val Accuracy = 24.58%\n","FL Round 151: Val Loss = 3.3904, Val Accuracy = 23.92%\n","FL Round 152: Val Loss = 3.3608, Val Accuracy = 24.98%\n","FL Round 153: Val Loss = 3.3470, Val Accuracy = 24.16%\n","FL Round 154: Val Loss = 3.3566, Val Accuracy = 24.36%\n","FL Round 155: Val Loss = 3.3662, Val Accuracy = 23.96%\n","FL Round 156: Val Loss = 3.3962, Val Accuracy = 23.92%\n","FL Round 157: Val Loss = 3.4566, Val Accuracy = 23.98%\n","FL Round 158: Val Loss = 3.4151, Val Accuracy = 23.82%\n","FL Round 159: Val Loss = 3.4632, Val Accuracy = 23.12%\n","FL Round 160: Val Loss = 3.3322, Val Accuracy = 25.56%\n","FL Round 161: Val Loss = 3.3292, Val Accuracy = 24.72%\n","FL Round 162: Val Loss = 3.3282, Val Accuracy = 25.40%\n","FL Round 163: Val Loss = 3.3351, Val Accuracy = 25.90%\n","FL Round 164: Val Loss = 3.3346, Val Accuracy = 24.32%\n","FL Round 165: Val Loss = 3.3568, Val Accuracy = 25.70%\n","FL Round 166: Val Loss = 3.4218, Val Accuracy = 23.06%\n","FL Round 167: Val Loss = 3.3545, Val Accuracy = 25.62%\n","FL Round 168: Val Loss = 3.3661, Val Accuracy = 24.08%\n","FL Round 169: Val Loss = 3.3498, Val Accuracy = 24.54%\n","FL Round 170: Val Loss = 3.3986, Val Accuracy = 24.26%\n","FL Round 171: Val Loss = 3.4031, Val Accuracy = 24.68%\n","FL Round 172: Val Loss = 3.3557, Val Accuracy = 24.72%\n","FL Round 173: Val Loss = 3.3709, Val Accuracy = 25.36%\n","FL Round 174: Val Loss = 3.3661, Val Accuracy = 25.44%\n","FL Round 175: Val Loss = 3.3957, Val Accuracy = 24.74%\n","FL Round 176: Val Loss = 3.3643, Val Accuracy = 25.30%\n","FL Round 177: Val Loss = 3.3382, Val Accuracy = 25.76%\n","FL Round 178: Val Loss = 3.2929, Val Accuracy = 26.02%\n","FL Round 179: Val Loss = 3.3767, Val Accuracy = 24.28%\n","FL Round 180: Val Loss = 3.3349, Val Accuracy = 25.80%\n","FL Round 181: Val Loss = 3.3317, Val Accuracy = 26.20%\n","FL Round 182: Val Loss = 3.3205, Val Accuracy = 25.74%\n","FL Round 183: Val Loss = 3.3625, Val Accuracy = 24.92%\n","FL Round 184: Val Loss = 3.3060, Val Accuracy = 26.94%\n","FL Round 185: Val Loss = 3.3532, Val Accuracy = 24.46%\n","FL Round 186: Val Loss = 3.3358, Val Accuracy = 26.50%\n","FL Round 187: Val Loss = 3.3944, Val Accuracy = 24.74%\n","FL Round 188: Val Loss = 3.3941, Val Accuracy = 24.92%\n","FL Round 189: Val Loss = 3.3743, Val Accuracy = 25.28%\n","FL Round 190: Val Loss = 3.3677, Val Accuracy = 25.48%\n","FL Round 191: Val Loss = 3.3676, Val Accuracy = 24.38%\n","FL Round 192: Val Loss = 3.3626, Val Accuracy = 25.30%\n","FL Round 193: Val Loss = 3.3486, Val Accuracy = 25.34%\n","FL Round 194: Val Loss = 3.3550, Val Accuracy = 25.36%\n","FL Round 195: Val Loss = 3.3586, Val Accuracy = 24.92%\n","FL Round 196: Val Loss = 3.4235, Val Accuracy = 25.42%\n","FL Round 197: Val Loss = 3.3645, Val Accuracy = 24.42%\n","FL Round 198: Val Loss = 3.3710, Val Accuracy = 25.32%\n","FL Round 199: Val Loss = 3.3869, Val Accuracy = 25.30%\n","FL Round 200: Val Loss = 3.4378, Val Accuracy = 23.64%\n","[Central-SGDM] Epoch 1: Train Loss: 4.3538, Train Acc: 3.70% | Val Loss: 4.3467, Val Acc: 4.36% | Test Loss: 4.3419, Test Acc: 5.06%\n","[Central-SGDM] Epoch 2: Train Loss: 4.2182, Train Acc: 5.52% | Val Loss: 4.2084, Val Acc: 5.86% | Test Loss: 4.1825, Test Acc: 6.39%\n","[Central-SGDM] Epoch 3: Train Loss: 4.1617, Train Acc: 6.40% | Val Loss: 4.1660, Val Acc: 6.40% | Test Loss: 4.1434, Test Acc: 6.86%\n","[Central-SGDM] Epoch 4: Train Loss: 4.1371, Train Acc: 6.81% | Val Loss: 4.1537, Val Acc: 6.68% | Test Loss: 4.1627, Test Acc: 7.26%\n","[Central-SGDM] Epoch 5: Train Loss: 4.1271, Train Acc: 7.29% | Val Loss: 4.1245, Val Acc: 7.54% | Test Loss: 4.1189, Test Acc: 7.67%\n","[Central-SGDM] Epoch 6: Train Loss: 4.0816, Train Acc: 8.00% | Val Loss: 4.1719, Val Acc: 6.90% | Test Loss: 4.1744, Test Acc: 6.37%\n","[Central-SGDM] Epoch 7: Train Loss: 4.0573, Train Acc: 8.27% | Val Loss: 4.0508, Val Acc: 9.00% | Test Loss: 4.0489, Test Acc: 8.85%\n","[Central-SGDM] Epoch 8: Train Loss: 3.9920, Train Acc: 9.54% | Val Loss: 4.0878, Val Acc: 8.30% | Test Loss: 4.0968, Test Acc: 8.38%\n","[Central-SGDM] Epoch 9: Train Loss: 4.0043, Train Acc: 9.43% | Val Loss: 4.0122, Val Acc: 8.96% | Test Loss: 4.0250, Test Acc: 9.02%\n","[Central-SGDM] Epoch 10: Train Loss: 3.9536, Train Acc: 9.90% | Val Loss: 4.0952, Val Acc: 8.70% | Test Loss: 4.1104, Test Acc: 8.17%\n","[Central-SGDM] Epoch 11: Train Loss: 3.9518, Train Acc: 10.52% | Val Loss: 3.9523, Val Acc: 10.58% | Test Loss: 3.9537, Test Acc: 10.50%\n","[Central-SGDM] Epoch 12: Train Loss: 3.9163, Train Acc: 10.80% | Val Loss: 4.0656, Val Acc: 8.88% | Test Loss: 4.0413, Test Acc: 9.07%\n","[Central-SGDM] Epoch 13: Train Loss: 3.8997, Train Acc: 11.26% | Val Loss: 3.9508, Val Acc: 10.92% | Test Loss: 3.9272, Test Acc: 10.97%\n","[Central-SGDM] Epoch 14: Train Loss: 3.8921, Train Acc: 11.59% | Val Loss: 3.9392, Val Acc: 11.46% | Test Loss: 3.9168, Test Acc: 11.75%\n","[Central-SGDM] Epoch 15: Train Loss: 3.8906, Train Acc: 11.38% | Val Loss: 3.9359, Val Acc: 11.30% | Test Loss: 3.9393, Test Acc: 11.61%\n","[Central-SGDM] Epoch 16: Train Loss: 3.9201, Train Acc: 11.64% | Val Loss: 4.0525, Val Acc: 9.88% | Test Loss: 4.0661, Test Acc: 9.85%\n","[Central-SGDM] Epoch 17: Train Loss: 3.8687, Train Acc: 12.11% | Val Loss: 3.9354, Val Acc: 10.58% | Test Loss: 3.9569, Test Acc: 10.62%\n","[Central-SGDM] Epoch 18: Train Loss: 3.8760, Train Acc: 11.91% | Val Loss: 3.8710, Val Acc: 11.08% | Test Loss: 3.8629, Test Acc: 12.22%\n","[Central-SGDM] Epoch 19: Train Loss: 3.8686, Train Acc: 12.08% | Val Loss: 3.9403, Val Acc: 11.64% | Test Loss: 3.9481, Test Acc: 11.36%\n","[Central-SGDM] Epoch 20: Train Loss: 3.8329, Train Acc: 12.86% | Val Loss: 4.1318, Val Acc: 10.52% | Test Loss: 4.1621, Test Acc: 10.65%\n","[Central-SGDM] Epoch 21: Train Loss: 3.8432, Train Acc: 12.60% | Val Loss: 3.9904, Val Acc: 11.56% | Test Loss: 3.9671, Test Acc: 11.89%\n","[Central-SGDM] Epoch 22: Train Loss: 3.8220, Train Acc: 13.20% | Val Loss: 3.9169, Val Acc: 11.80% | Test Loss: 3.9057, Test Acc: 11.57%\n","[Central-SGDM] Epoch 23: Train Loss: 3.8279, Train Acc: 12.84% | Val Loss: 3.9552, Val Acc: 11.40% | Test Loss: 3.9320, Test Acc: 12.19%\n","[Central-SGDM] Epoch 24: Train Loss: 3.8086, Train Acc: 13.17% | Val Loss: 3.9010, Val Acc: 11.60% | Test Loss: 3.8965, Test Acc: 12.03%\n","[Central-SGDM] Epoch 25: Train Loss: 3.7732, Train Acc: 13.48% | Val Loss: 3.9405, Val Acc: 10.28% | Test Loss: 3.9079, Test Acc: 11.26%\n","[Central-SGDM] Epoch 26: Train Loss: 3.7850, Train Acc: 13.69% | Val Loss: 3.7946, Val Acc: 13.96% | Test Loss: 3.8305, Test Acc: 12.83%\n","[Central-SGDM] Epoch 27: Train Loss: 3.7741, Train Acc: 13.79% | Val Loss: 3.9099, Val Acc: 13.14% | Test Loss: 3.8790, Test Acc: 13.11%\n","[Central-SGDM] Epoch 28: Train Loss: 3.7632, Train Acc: 13.98% | Val Loss: 4.0312, Val Acc: 10.44% | Test Loss: 4.0089, Test Acc: 11.17%\n","[Central-SGDM] Epoch 29: Train Loss: 3.7727, Train Acc: 13.72% | Val Loss: 3.8731, Val Acc: 12.46% | Test Loss: 3.8536, Test Acc: 12.98%\n","[Central-SGDM] Epoch 30: Train Loss: 3.7415, Train Acc: 14.48% | Val Loss: 3.8975, Val Acc: 13.12% | Test Loss: 3.8249, Test Acc: 13.79%\n","[Central-SGDM] Epoch 31: Train Loss: 3.7199, Train Acc: 14.69% | Val Loss: 4.0100, Val Acc: 12.32% | Test Loss: 3.9913, Test Acc: 13.30%\n","[Central-SGDM] Epoch 32: Train Loss: 3.6952, Train Acc: 14.99% | Val Loss: 3.8112, Val Acc: 13.66% | Test Loss: 3.7900, Test Acc: 13.96%\n","[Central-SGDM] Epoch 33: Train Loss: 3.6992, Train Acc: 15.33% | Val Loss: 4.2307, Val Acc: 11.52% | Test Loss: 4.1926, Test Acc: 11.36%\n","[Central-SGDM] Epoch 34: Train Loss: 3.6961, Train Acc: 15.30% | Val Loss: 3.7215, Val Acc: 14.74% | Test Loss: 3.7060, Test Acc: 15.19%\n","[Central-SGDM] Epoch 35: Train Loss: 3.6931, Train Acc: 15.39% | Val Loss: 3.8170, Val Acc: 12.88% | Test Loss: 3.7996, Test Acc: 13.33%\n","[Central-SGDM] Epoch 36: Train Loss: 3.6694, Train Acc: 15.56% | Val Loss: 3.8405, Val Acc: 13.38% | Test Loss: 3.8569, Test Acc: 13.30%\n","[Central-SGDM] Epoch 37: Train Loss: 3.6506, Train Acc: 15.93% | Val Loss: 3.7624, Val Acc: 14.52% | Test Loss: 3.7703, Test Acc: 14.56%\n","[Central-SGDM] Epoch 38: Train Loss: 3.6565, Train Acc: 16.22% | Val Loss: 3.7139, Val Acc: 13.98% | Test Loss: 3.6957, Test Acc: 15.15%\n","[Central-SGDM] Epoch 39: Train Loss: 3.6022, Train Acc: 16.77% | Val Loss: 3.7932, Val Acc: 14.06% | Test Loss: 3.7709, Test Acc: 14.41%\n","[Central-SGDM] Epoch 40: Train Loss: 3.5923, Train Acc: 17.03% | Val Loss: 3.8097, Val Acc: 13.46% | Test Loss: 3.8086, Test Acc: 13.94%\n","[Central-SGDM] Epoch 41: Train Loss: 3.5721, Train Acc: 17.52% | Val Loss: 3.8075, Val Acc: 14.26% | Test Loss: 3.7971, Test Acc: 14.32%\n","[Central-SGDM] Epoch 42: Train Loss: 3.5935, Train Acc: 17.17% | Val Loss: 3.7891, Val Acc: 15.44% | Test Loss: 3.7813, Test Acc: 15.02%\n","[Central-SGDM] Epoch 43: Train Loss: 3.6105, Train Acc: 16.88% | Val Loss: 3.8023, Val Acc: 14.46% | Test Loss: 3.7869, Test Acc: 14.15%\n","[Central-SGDM] Epoch 44: Train Loss: 3.5404, Train Acc: 18.11% | Val Loss: 3.6809, Val Acc: 16.60% | Test Loss: 3.6604, Test Acc: 16.04%\n","[Central-SGDM] Epoch 45: Train Loss: 3.5553, Train Acc: 17.96% | Val Loss: 3.7967, Val Acc: 15.70% | Test Loss: 3.7849, Test Acc: 15.37%\n","[Central-SGDM] Epoch 46: Train Loss: 3.5624, Train Acc: 17.82% | Val Loss: 3.6427, Val Acc: 16.54% | Test Loss: 3.6879, Test Acc: 16.06%\n","[Central-SGDM] Epoch 47: Train Loss: 3.5164, Train Acc: 18.36% | Val Loss: 3.8694, Val Acc: 13.00% | Test Loss: 3.8324, Test Acc: 13.71%\n","[Central-SGDM] Epoch 48: Train Loss: 3.4900, Train Acc: 19.09% | Val Loss: 3.6496, Val Acc: 16.32% | Test Loss: 3.5973, Test Acc: 16.84%\n","[Central-SGDM] Epoch 49: Train Loss: 3.4823, Train Acc: 19.13% | Val Loss: 3.8558, Val Acc: 13.42% | Test Loss: 3.8089, Test Acc: 13.90%\n","[Central-SGDM] Epoch 50: Train Loss: 3.4662, Train Acc: 19.64% | Val Loss: 3.6512, Val Acc: 16.04% | Test Loss: 3.5828, Test Acc: 17.42%\n","[Central-SGDM] Epoch 51: Train Loss: 3.3842, Train Acc: 20.74% | Val Loss: 3.6471, Val Acc: 16.08% | Test Loss: 3.6368, Test Acc: 17.21%\n","[Central-SGDM] Epoch 52: Train Loss: 3.3858, Train Acc: 20.82% | Val Loss: 3.5282, Val Acc: 19.02% | Test Loss: 3.5321, Test Acc: 19.27%\n","[Central-SGDM] Epoch 53: Train Loss: 3.3705, Train Acc: 21.26% | Val Loss: 3.6517, Val Acc: 17.86% | Test Loss: 3.6212, Test Acc: 18.18%\n","[Central-SGDM] Epoch 54: Train Loss: 3.3570, Train Acc: 21.56% | Val Loss: 3.6462, Val Acc: 18.28% | Test Loss: 3.5892, Test Acc: 18.85%\n","[Central-SGDM] Epoch 55: Train Loss: 3.3456, Train Acc: 21.63% | Val Loss: 3.6526, Val Acc: 17.88% | Test Loss: 3.6491, Test Acc: 16.86%\n","[Central-SGDM] Epoch 56: Train Loss: 3.3336, Train Acc: 22.05% | Val Loss: 3.6153, Val Acc: 16.78% | Test Loss: 3.6160, Test Acc: 17.48%\n","[Central-SGDM] Epoch 57: Train Loss: 3.3320, Train Acc: 22.23% | Val Loss: 3.6073, Val Acc: 17.88% | Test Loss: 3.5998, Test Acc: 17.86%\n","[Central-SGDM] Epoch 58: Train Loss: 3.2635, Train Acc: 23.56% | Val Loss: 3.5684, Val Acc: 18.38% | Test Loss: 3.5449, Test Acc: 18.25%\n","[Central-SGDM] Epoch 59: Train Loss: 3.2328, Train Acc: 24.01% | Val Loss: 3.6213, Val Acc: 18.32% | Test Loss: 3.6389, Test Acc: 17.66%\n","[Central-SGDM] Epoch 60: Train Loss: 3.2474, Train Acc: 23.68% | Val Loss: 3.5575, Val Acc: 19.90% | Test Loss: 3.5504, Test Acc: 19.51%\n","[Central-SGDM] Epoch 61: Train Loss: 3.1698, Train Acc: 25.06% | Val Loss: 3.6190, Val Acc: 18.88% | Test Loss: 3.6337, Test Acc: 18.77%\n","[Central-SGDM] Epoch 62: Train Loss: 3.1403, Train Acc: 25.78% | Val Loss: 3.5640, Val Acc: 19.76% | Test Loss: 3.5721, Test Acc: 19.48%\n","[Central-SGDM] Epoch 63: Train Loss: 3.1456, Train Acc: 25.85% | Val Loss: 3.5910, Val Acc: 17.68% | Test Loss: 3.5604, Test Acc: 18.33%\n","[Central-SGDM] Epoch 64: Train Loss: 3.1091, Train Acc: 26.32% | Val Loss: 3.5694, Val Acc: 18.76% | Test Loss: 3.5182, Test Acc: 19.79%\n","[Central-SGDM] Epoch 65: Train Loss: 3.0868, Train Acc: 27.06% | Val Loss: 3.5323, Val Acc: 20.18% | Test Loss: 3.4984, Test Acc: 20.59%\n","[Central-SGDM] Epoch 66: Train Loss: 3.0279, Train Acc: 27.99% | Val Loss: 3.5300, Val Acc: 20.94% | Test Loss: 3.5189, Test Acc: 20.82%\n","[Central-SGDM] Epoch 67: Train Loss: 3.0260, Train Acc: 28.09% | Val Loss: 3.5575, Val Acc: 19.74% | Test Loss: 3.5790, Test Acc: 19.98%\n","[Central-SGDM] Epoch 68: Train Loss: 2.9682, Train Acc: 29.14% | Val Loss: 3.4763, Val Acc: 20.58% | Test Loss: 3.4606, Test Acc: 20.85%\n","[Central-SGDM] Epoch 69: Train Loss: 2.9554, Train Acc: 29.68% | Val Loss: 3.5430, Val Acc: 20.44% | Test Loss: 3.5147, Test Acc: 20.70%\n","[Central-SGDM] Epoch 70: Train Loss: 2.8972, Train Acc: 30.63% | Val Loss: 3.4734, Val Acc: 22.44% | Test Loss: 3.4582, Test Acc: 22.27%\n","[Central-SGDM] Epoch 71: Train Loss: 2.8618, Train Acc: 31.58% | Val Loss: 3.4421, Val Acc: 22.04% | Test Loss: 3.4119, Test Acc: 22.88%\n","[Central-SGDM] Epoch 72: Train Loss: 2.8255, Train Acc: 32.21% | Val Loss: 3.4149, Val Acc: 21.38% | Test Loss: 3.4131, Test Acc: 22.03%\n","[Central-SGDM] Epoch 73: Train Loss: 2.7602, Train Acc: 33.64% | Val Loss: 3.5503, Val Acc: 20.72% | Test Loss: 3.4616, Test Acc: 22.14%\n","[Central-SGDM] Epoch 74: Train Loss: 2.7188, Train Acc: 34.50% | Val Loss: 3.3918, Val Acc: 23.56% | Test Loss: 3.3619, Test Acc: 23.98%\n","[Central-SGDM] Epoch 75: Train Loss: 2.6711, Train Acc: 35.21% | Val Loss: 3.4142, Val Acc: 24.12% | Test Loss: 3.4081, Test Acc: 24.13%\n","[Central-SGDM] Epoch 76: Train Loss: 2.6191, Train Acc: 36.37% | Val Loss: 3.4137, Val Acc: 23.72% | Test Loss: 3.4299, Test Acc: 23.55%\n","[Central-SGDM] Epoch 77: Train Loss: 2.5596, Train Acc: 37.52% | Val Loss: 3.5476, Val Acc: 22.70% | Test Loss: 3.5105, Test Acc: 23.27%\n","[Central-SGDM] Epoch 78: Train Loss: 2.5406, Train Acc: 37.75% | Val Loss: 3.5555, Val Acc: 22.10% | Test Loss: 3.5498, Test Acc: 22.70%\n","[Central-SGDM] Epoch 79: Train Loss: 2.4486, Train Acc: 39.56% | Val Loss: 3.4526, Val Acc: 23.72% | Test Loss: 3.4538, Test Acc: 24.37%\n","[Central-SGDM] Epoch 80: Train Loss: 2.4120, Train Acc: 40.47% | Val Loss: 3.4335, Val Acc: 25.34% | Test Loss: 3.4408, Test Acc: 24.79%\n","[Central-SGDM] Epoch 81: Train Loss: 2.3731, Train Acc: 41.52% | Val Loss: 3.4838, Val Acc: 24.40% | Test Loss: 3.4842, Test Acc: 24.20%\n","[Central-SGDM] Epoch 82: Train Loss: 2.3201, Train Acc: 42.61% | Val Loss: 3.5151, Val Acc: 23.80% | Test Loss: 3.5201, Test Acc: 23.91%\n","[Central-SGDM] Epoch 83: Train Loss: 2.2799, Train Acc: 43.46% | Val Loss: 3.5342, Val Acc: 24.22% | Test Loss: 3.5630, Test Acc: 23.40%\n","[Central-SGDM] Epoch 84: Train Loss: 2.2300, Train Acc: 44.36% | Val Loss: 3.5057, Val Acc: 25.52% | Test Loss: 3.5007, Test Acc: 24.85%\n","[Central-SGDM] Epoch 85: Train Loss: 2.1346, Train Acc: 46.62% | Val Loss: 3.6105, Val Acc: 24.48% | Test Loss: 3.5440, Test Acc: 24.20%\n","[Central-SGDM] Epoch 86: Train Loss: 2.0653, Train Acc: 47.83% | Val Loss: 3.5104, Val Acc: 26.56% | Test Loss: 3.5035, Test Acc: 25.96%\n","[Central-SGDM] Epoch 87: Train Loss: 2.0280, Train Acc: 48.77% | Val Loss: 3.5363, Val Acc: 25.38% | Test Loss: 3.5407, Test Acc: 24.84%\n","[Central-SGDM] Epoch 88: Train Loss: 1.9985, Train Acc: 49.44% | Val Loss: 3.6738, Val Acc: 25.16% | Test Loss: 3.6823, Test Acc: 25.76%\n","[Central-SGDM] Epoch 89: Train Loss: 1.9095, Train Acc: 51.32% | Val Loss: 3.6097, Val Acc: 25.74% | Test Loss: 3.5892, Test Acc: 25.82%\n","[Central-SGDM] Epoch 90: Train Loss: 1.8259, Train Acc: 53.14% | Val Loss: 3.6096, Val Acc: 26.06% | Test Loss: 3.6448, Test Acc: 25.84%\n","[Central-SGDM] Epoch 91: Train Loss: 1.7783, Train Acc: 54.12% | Val Loss: 3.7535, Val Acc: 24.52% | Test Loss: 3.7345, Test Acc: 24.77%\n","[Central-SGDM] Epoch 92: Train Loss: 1.6977, Train Acc: 55.96% | Val Loss: 3.6622, Val Acc: 26.44% | Test Loss: 3.6876, Test Acc: 26.23%\n","[Central-SGDM] Epoch 93: Train Loss: 1.6309, Train Acc: 57.47% | Val Loss: 3.7448, Val Acc: 25.94% | Test Loss: 3.7404, Test Acc: 26.32%\n","[Central-SGDM] Epoch 94: Train Loss: 1.5736, Train Acc: 58.70% | Val Loss: 3.8264, Val Acc: 25.56% | Test Loss: 3.8105, Test Acc: 26.08%\n","[Central-SGDM] Epoch 95: Train Loss: 1.4806, Train Acc: 60.76% | Val Loss: 3.8973, Val Acc: 25.70% | Test Loss: 3.9140, Test Acc: 25.61%\n","[Central-SGDM] Epoch 96: Train Loss: 1.4252, Train Acc: 62.06% | Val Loss: 3.8398, Val Acc: 26.30% | Test Loss: 3.8631, Test Acc: 26.20%\n","[Central-SGDM] Epoch 97: Train Loss: 1.3249, Train Acc: 64.33% | Val Loss: 3.9225, Val Acc: 27.88% | Test Loss: 3.9362, Test Acc: 26.71%\n","[Central-SGDM] Epoch 98: Train Loss: 1.2255, Train Acc: 66.71% | Val Loss: 3.9764, Val Acc: 26.54% | Test Loss: 4.0631, Test Acc: 26.26%\n","[Central-SGDM] Epoch 99: Train Loss: 1.1805, Train Acc: 67.78% | Val Loss: 4.0883, Val Acc: 27.08% | Test Loss: 4.0872, Test Acc: 26.42%\n","[Central-SGDM] Epoch 100: Train Loss: 1.1050, Train Acc: 69.56% | Val Loss: 4.1449, Val Acc: 26.82% | Test Loss: 4.1450, Test Acc: 25.80%\n","[Central-SGDM] Epoch 101: Train Loss: 1.0018, Train Acc: 71.84% | Val Loss: 4.2359, Val Acc: 27.14% | Test Loss: 4.2266, Test Acc: 26.58%\n","[Central-SGDM] Epoch 102: Train Loss: 0.9168, Train Acc: 74.06% | Val Loss: 4.4275, Val Acc: 27.72% | Test Loss: 4.4265, Test Acc: 26.50%\n","[Central-SGDM] Epoch 103: Train Loss: 0.8760, Train Acc: 74.95% | Val Loss: 4.5821, Val Acc: 27.44% | Test Loss: 4.5389, Test Acc: 26.23%\n","[Central-SGDM] Epoch 104: Train Loss: 0.7848, Train Acc: 77.50% | Val Loss: 4.5340, Val Acc: 27.78% | Test Loss: 4.4786, Test Acc: 26.63%\n","[Central-SGDM] Epoch 105: Train Loss: 0.7346, Train Acc: 78.86% | Val Loss: 4.7331, Val Acc: 26.26% | Test Loss: 4.7240, Test Acc: 25.77%\n","[Central-SGDM] Epoch 106: Train Loss: 0.6322, Train Acc: 81.56% | Val Loss: 4.7616, Val Acc: 28.10% | Test Loss: 4.7955, Test Acc: 26.89%\n","[Central-SGDM] Epoch 107: Train Loss: 0.5870, Train Acc: 82.57% | Val Loss: 4.9628, Val Acc: 27.54% | Test Loss: 4.9792, Test Acc: 26.93%\n","[Central-SGDM] Epoch 108: Train Loss: 0.4931, Train Acc: 85.35% | Val Loss: 5.0550, Val Acc: 27.62% | Test Loss: 5.0471, Test Acc: 27.37%\n","[Central-SGDM] Epoch 109: Train Loss: 0.3833, Train Acc: 88.47% | Val Loss: 5.1202, Val Acc: 29.36% | Test Loss: 5.1506, Test Acc: 27.70%\n","[Central-SGDM] Epoch 110: Train Loss: 0.3185, Train Acc: 90.64% | Val Loss: 5.1785, Val Acc: 29.90% | Test Loss: 5.2327, Test Acc: 27.84%\n","[Central-SGDM] Epoch 111: Train Loss: 0.2240, Train Acc: 93.34% | Val Loss: 5.3221, Val Acc: 29.30% | Test Loss: 5.3991, Test Acc: 28.49%\n","[Central-SGDM] Epoch 112: Train Loss: 0.1600, Train Acc: 95.45% | Val Loss: 5.3714, Val Acc: 30.08% | Test Loss: 5.4541, Test Acc: 28.67%\n","[Central-SGDM] Epoch 113: Train Loss: 0.1160, Train Acc: 96.97% | Val Loss: 5.3252, Val Acc: 30.66% | Test Loss: 5.4041, Test Acc: 29.20%\n","[Central-SGDM] Epoch 114: Train Loss: 0.0650, Train Acc: 98.62% | Val Loss: 5.2989, Val Acc: 30.94% | Test Loss: 5.3546, Test Acc: 29.85%\n","[Central-SGDM] Epoch 115: Train Loss: 0.0352, Train Acc: 99.52% | Val Loss: 5.2285, Val Acc: 31.98% | Test Loss: 5.2687, Test Acc: 30.86%\n","[Central-SGDM] Epoch 116: Train Loss: 0.0276, Train Acc: 99.70% | Val Loss: 5.1702, Val Acc: 32.64% | Test Loss: 5.1766, Test Acc: 31.48%\n","[Central-SGDM] Epoch 117: Train Loss: 0.0231, Train Acc: 99.80% | Val Loss: 5.0868, Val Acc: 33.04% | Test Loss: 5.0912, Test Acc: 31.85%\n","[Central-SGDM] Epoch 118: Train Loss: 0.0215, Train Acc: 99.84% | Val Loss: 5.0201, Val Acc: 33.36% | Test Loss: 5.0224, Test Acc: 31.99%\n","[Central-SGDM] Epoch 119: Train Loss: 0.0214, Train Acc: 99.88% | Val Loss: 4.9387, Val Acc: 33.10% | Test Loss: 4.9457, Test Acc: 32.09%\n","[Central-SGDM] Epoch 120: Train Loss: 0.0219, Train Acc: 99.90% | Val Loss: 4.9285, Val Acc: 33.28% | Test Loss: 4.9227, Test Acc: 32.51%\n","[Central-SGDM] Epoch 121: Train Loss: 0.0220, Train Acc: 99.90% | Val Loss: 4.8601, Val Acc: 33.62% | Test Loss: 4.8534, Test Acc: 32.41%\n","[Central-SGDM] Epoch 122: Train Loss: 0.0221, Train Acc: 99.93% | Val Loss: 4.8537, Val Acc: 33.70% | Test Loss: 4.8475, Test Acc: 32.61%\n","[Central-SGDM] Epoch 123: Train Loss: 0.0223, Train Acc: 99.92% | Val Loss: 4.8517, Val Acc: 33.66% | Test Loss: 4.8399, Test Acc: 32.64%\n","[Central-SGDM] Epoch 124: Train Loss: 0.0223, Train Acc: 99.95% | Val Loss: 4.8373, Val Acc: 33.58% | Test Loss: 4.8279, Test Acc: 32.83%\n","[Central-SGDM] Epoch 125: Train Loss: 0.0219, Train Acc: 99.95% | Val Loss: 4.8455, Val Acc: 33.68% | Test Loss: 4.8311, Test Acc: 32.71%\n","[Central-SGDM] Epoch 126: Train Loss: 0.0219, Train Acc: 99.94% | Val Loss: 4.8238, Val Acc: 33.60% | Test Loss: 4.8163, Test Acc: 32.81%\n","[Central-SGDM] Epoch 127: Train Loss: 0.0219, Train Acc: 99.95% | Val Loss: 4.8193, Val Acc: 33.80% | Test Loss: 4.8111, Test Acc: 33.04%\n","[Central-SGDM] Epoch 128: Train Loss: 0.0216, Train Acc: 99.96% | Val Loss: 4.8202, Val Acc: 33.78% | Test Loss: 4.8062, Test Acc: 32.98%\n","[Central-SGDM] Epoch 129: Train Loss: 0.0215, Train Acc: 99.96% | Val Loss: 4.8229, Val Acc: 34.00% | Test Loss: 4.8139, Test Acc: 32.94%\n","[Central-SGDM] Epoch 130: Train Loss: 0.0213, Train Acc: 99.96% | Val Loss: 4.8086, Val Acc: 33.74% | Test Loss: 4.8027, Test Acc: 33.07%\n","[Central-SGDM] Epoch 131: Train Loss: 0.0212, Train Acc: 99.96% | Val Loss: 4.8221, Val Acc: 33.64% | Test Loss: 4.8127, Test Acc: 33.17%\n","[Central-SGDM] Epoch 132: Train Loss: 0.0210, Train Acc: 99.97% | Val Loss: 4.8240, Val Acc: 33.84% | Test Loss: 4.8148, Test Acc: 33.03%\n","[Central-SGDM] Epoch 133: Train Loss: 0.0208, Train Acc: 99.97% | Val Loss: 4.8217, Val Acc: 33.96% | Test Loss: 4.8123, Test Acc: 32.98%\n","[Central-SGDM] Epoch 134: Train Loss: 0.0207, Train Acc: 99.97% | Val Loss: 4.8155, Val Acc: 33.94% | Test Loss: 4.8081, Test Acc: 33.08%\n","[Central-SGDM] Epoch 135: Train Loss: 0.0205, Train Acc: 99.97% | Val Loss: 4.8215, Val Acc: 33.96% | Test Loss: 4.8113, Test Acc: 33.25%\n","[Central-SGDM] Epoch 136: Train Loss: 0.0204, Train Acc: 99.97% | Val Loss: 4.8293, Val Acc: 34.10% | Test Loss: 4.8197, Test Acc: 33.18%\n","[Central-SGDM] Epoch 137: Train Loss: 0.0203, Train Acc: 99.97% | Val Loss: 4.8208, Val Acc: 34.20% | Test Loss: 4.8116, Test Acc: 33.10%\n","[Central-SGDM] Epoch 138: Train Loss: 0.0201, Train Acc: 99.97% | Val Loss: 4.8170, Val Acc: 34.14% | Test Loss: 4.8094, Test Acc: 33.08%\n","[Central-SGDM] Epoch 139: Train Loss: 0.0200, Train Acc: 99.97% | Val Loss: 4.8222, Val Acc: 34.22% | Test Loss: 4.8124, Test Acc: 33.19%\n","[Central-SGDM] Epoch 140: Train Loss: 0.0199, Train Acc: 99.97% | Val Loss: 4.8222, Val Acc: 34.18% | Test Loss: 4.8131, Test Acc: 33.18%\n","[Central-SGDM] Epoch 141: Train Loss: 0.0198, Train Acc: 99.97% | Val Loss: 4.8220, Val Acc: 34.22% | Test Loss: 4.8108, Test Acc: 33.20%\n","[Central-SGDM] Epoch 142: Train Loss: 0.0197, Train Acc: 99.97% | Val Loss: 4.8214, Val Acc: 34.30% | Test Loss: 4.8117, Test Acc: 33.19%\n","[Central-SGDM] Epoch 143: Train Loss: 0.0196, Train Acc: 99.97% | Val Loss: 4.8221, Val Acc: 34.22% | Test Loss: 4.8118, Test Acc: 33.17%\n","[Central-SGDM] Epoch 144: Train Loss: 0.0195, Train Acc: 99.98% | Val Loss: 4.8232, Val Acc: 34.24% | Test Loss: 4.8122, Test Acc: 33.21%\n","[Central-SGDM] Epoch 145: Train Loss: 0.0195, Train Acc: 99.97% | Val Loss: 4.8229, Val Acc: 34.22% | Test Loss: 4.8119, Test Acc: 33.19%\n","[Central-SGDM] Epoch 146: Train Loss: 0.0194, Train Acc: 99.98% | Val Loss: 4.8218, Val Acc: 34.24% | Test Loss: 4.8110, Test Acc: 33.20%\n","[Central-SGDM] Epoch 147: Train Loss: 0.0194, Train Acc: 99.97% | Val Loss: 4.8231, Val Acc: 34.20% | Test Loss: 4.8122, Test Acc: 33.17%\n","[Central-SGDM] Epoch 148: Train Loss: 0.0193, Train Acc: 99.98% | Val Loss: 4.8223, Val Acc: 34.22% | Test Loss: 4.8115, Test Acc: 33.18%\n","[Central-SGDM] Epoch 149: Train Loss: 0.0193, Train Acc: 99.98% | Val Loss: 4.8225, Val Acc: 34.20% | Test Loss: 4.8116, Test Acc: 33.18%\n","[Central-SGDM] Epoch 150: Train Loss: 0.0193, Train Acc: 99.98% | Val Loss: 4.8225, Val Acc: 34.20% | Test Loss: 4.8116, Test Acc: 33.18%\n","[Central-AdamW] Epoch 1: Train Loss: 3.7740, Train Acc: 11.98% | Val Loss: 3.3642, Val Acc: 18.10% | Test Loss: 3.3359, Test Acc: 19.21%\n","[Central-AdamW] Epoch 2: Train Loss: 3.1223, Train Acc: 23.21% | Val Loss: 3.0317, Val Acc: 25.76% | Test Loss: 3.0080, Test Acc: 26.01%\n","[Central-AdamW] Epoch 3: Train Loss: 2.7952, Train Acc: 29.36% | Val Loss: 2.8402, Val Acc: 29.54% | Test Loss: 2.8118, Test Acc: 29.78%\n","[Central-AdamW] Epoch 4: Train Loss: 2.5737, Train Acc: 33.96% | Val Loss: 2.7072, Val Acc: 32.74% | Test Loss: 2.6682, Test Acc: 32.52%\n","[Central-AdamW] Epoch 5: Train Loss: 2.3842, Train Acc: 37.83% | Val Loss: 2.7055, Val Acc: 32.86% | Test Loss: 2.6664, Test Acc: 33.48%\n","[Central-AdamW] Epoch 6: Train Loss: 2.2486, Train Acc: 40.35% | Val Loss: 2.6274, Val Acc: 34.96% | Test Loss: 2.5862, Test Acc: 34.91%\n","[Central-AdamW] Epoch 7: Train Loss: 2.1117, Train Acc: 43.57% | Val Loss: 2.5786, Val Acc: 35.94% | Test Loss: 2.5469, Test Acc: 35.85%\n","[Central-AdamW] Epoch 8: Train Loss: 2.0006, Train Acc: 45.74% | Val Loss: 2.6345, Val Acc: 36.34% | Test Loss: 2.6174, Test Acc: 36.00%\n","[Central-AdamW] Epoch 9: Train Loss: 1.8947, Train Acc: 48.21% | Val Loss: 2.6036, Val Acc: 36.38% | Test Loss: 2.5685, Test Acc: 36.68%\n","[Central-AdamW] Epoch 10: Train Loss: 1.7852, Train Acc: 50.67% | Val Loss: 2.7006, Val Acc: 36.52% | Test Loss: 2.6526, Test Acc: 36.44%\n","[Central-AdamW] Epoch 11: Train Loss: 1.6968, Train Acc: 52.77% | Val Loss: 2.7853, Val Acc: 35.30% | Test Loss: 2.7028, Test Acc: 36.20%\n","[Central-AdamW] Epoch 12: Train Loss: 1.6118, Train Acc: 54.55% | Val Loss: 2.7581, Val Acc: 37.12% | Test Loss: 2.7081, Test Acc: 36.11%\n","[Central-AdamW] Epoch 13: Train Loss: 1.5259, Train Acc: 56.47% | Val Loss: 2.8917, Val Acc: 36.74% | Test Loss: 2.8098, Test Acc: 36.75%\n","[Central-AdamW] Epoch 14: Train Loss: 1.4384, Train Acc: 58.80% | Val Loss: 3.0229, Val Acc: 35.78% | Test Loss: 2.9376, Test Acc: 35.84%\n","[Central-AdamW] Epoch 15: Train Loss: 1.3681, Train Acc: 60.34% | Val Loss: 3.1178, Val Acc: 36.08% | Test Loss: 3.0288, Test Acc: 35.51%\n","[Central-AdamW] Epoch 16: Train Loss: 1.2932, Train Acc: 62.09% | Val Loss: 3.1262, Val Acc: 35.84% | Test Loss: 3.0305, Test Acc: 35.86%\n","[Central-AdamW] Epoch 17: Train Loss: 1.2383, Train Acc: 63.71% | Val Loss: 3.2802, Val Acc: 34.98% | Test Loss: 3.2193, Test Acc: 35.33%\n","[Central-AdamW] Epoch 18: Train Loss: 1.1687, Train Acc: 65.52% | Val Loss: 3.4389, Val Acc: 34.56% | Test Loss: 3.3563, Test Acc: 34.97%\n","[Central-AdamW] Epoch 19: Train Loss: 1.1040, Train Acc: 67.18% | Val Loss: 3.5170, Val Acc: 33.84% | Test Loss: 3.4078, Test Acc: 34.03%\n","[Central-AdamW] Epoch 20: Train Loss: 1.0436, Train Acc: 68.65% | Val Loss: 3.6783, Val Acc: 33.36% | Test Loss: 3.6157, Test Acc: 34.72%\n","[Central-AdamW] Epoch 21: Train Loss: 0.9925, Train Acc: 69.85% | Val Loss: 3.8340, Val Acc: 33.98% | Test Loss: 3.7192, Test Acc: 35.19%\n","[Central-AdamW] Epoch 22: Train Loss: 0.9373, Train Acc: 71.34% | Val Loss: 3.8347, Val Acc: 33.82% | Test Loss: 3.7093, Test Acc: 34.62%\n","[Central-AdamW] Epoch 23: Train Loss: 0.8936, Train Acc: 72.48% | Val Loss: 4.0427, Val Acc: 33.12% | Test Loss: 3.9343, Test Acc: 34.19%\n","[Central-AdamW] Epoch 24: Train Loss: 0.8402, Train Acc: 73.98% | Val Loss: 4.2011, Val Acc: 33.02% | Test Loss: 4.0906, Test Acc: 34.07%\n","[Central-AdamW] Epoch 25: Train Loss: 0.7997, Train Acc: 75.07% | Val Loss: 4.2168, Val Acc: 33.12% | Test Loss: 4.0859, Test Acc: 34.65%\n","[Central-AdamW] Epoch 26: Train Loss: 0.7629, Train Acc: 76.08% | Val Loss: 4.4366, Val Acc: 32.46% | Test Loss: 4.2990, Test Acc: 33.79%\n","[Central-AdamW] Epoch 27: Train Loss: 0.7276, Train Acc: 77.08% | Val Loss: 4.6263, Val Acc: 33.58% | Test Loss: 4.4897, Test Acc: 33.99%\n","[Central-AdamW] Epoch 28: Train Loss: 0.6864, Train Acc: 78.30% | Val Loss: 4.7001, Val Acc: 33.10% | Test Loss: 4.5679, Test Acc: 33.45%\n","[Central-AdamW] Epoch 29: Train Loss: 0.6512, Train Acc: 79.23% | Val Loss: 4.9266, Val Acc: 31.98% | Test Loss: 4.7860, Test Acc: 33.22%\n","[Central-AdamW] Epoch 30: Train Loss: 0.6162, Train Acc: 80.14% | Val Loss: 5.0626, Val Acc: 33.56% | Test Loss: 4.8498, Test Acc: 33.75%\n","[Central-AdamW] Epoch 31: Train Loss: 0.5901, Train Acc: 80.88% | Val Loss: 5.2417, Val Acc: 31.78% | Test Loss: 5.0879, Test Acc: 33.42%\n","[Central-AdamW] Epoch 32: Train Loss: 0.5484, Train Acc: 82.23% | Val Loss: 5.4307, Val Acc: 32.36% | Test Loss: 5.2013, Test Acc: 33.06%\n","[Central-AdamW] Epoch 33: Train Loss: 0.5523, Train Acc: 82.22% | Val Loss: 5.4109, Val Acc: 33.18% | Test Loss: 5.2673, Test Acc: 32.92%\n","[Central-AdamW] Epoch 34: Train Loss: 0.5003, Train Acc: 83.78% | Val Loss: 5.6858, Val Acc: 32.42% | Test Loss: 5.4856, Test Acc: 33.32%\n","[Central-AdamW] Epoch 35: Train Loss: 0.4983, Train Acc: 83.62% | Val Loss: 5.6813, Val Acc: 32.64% | Test Loss: 5.5521, Test Acc: 33.28%\n","[Central-AdamW] Epoch 36: Train Loss: 0.4626, Train Acc: 84.97% | Val Loss: 5.8851, Val Acc: 32.96% | Test Loss: 5.7743, Test Acc: 32.99%\n","[Central-AdamW] Epoch 37: Train Loss: 0.4575, Train Acc: 85.07% | Val Loss: 6.1485, Val Acc: 32.74% | Test Loss: 6.0705, Test Acc: 33.10%\n","[Central-AdamW] Epoch 38: Train Loss: 0.4165, Train Acc: 86.46% | Val Loss: 6.1752, Val Acc: 33.06% | Test Loss: 6.0779, Test Acc: 32.76%\n","[Central-AdamW] Epoch 39: Train Loss: 0.4206, Train Acc: 86.00% | Val Loss: 6.3397, Val Acc: 32.42% | Test Loss: 6.1204, Test Acc: 32.31%\n","[Central-AdamW] Epoch 40: Train Loss: 0.3932, Train Acc: 86.83% | Val Loss: 6.4823, Val Acc: 32.00% | Test Loss: 6.2531, Test Acc: 32.38%\n","[Central-AdamW] Epoch 41: Train Loss: 0.3643, Train Acc: 87.91% | Val Loss: 6.6321, Val Acc: 31.92% | Test Loss: 6.4915, Test Acc: 32.21%\n","[Central-AdamW] Epoch 42: Train Loss: 0.3742, Train Acc: 87.60% | Val Loss: 6.6787, Val Acc: 31.96% | Test Loss: 6.5068, Test Acc: 32.94%\n","[Central-AdamW] Epoch 43: Train Loss: 0.3451, Train Acc: 88.65% | Val Loss: 6.8494, Val Acc: 31.76% | Test Loss: 6.6598, Test Acc: 32.87%\n","[Central-AdamW] Epoch 44: Train Loss: 0.3543, Train Acc: 88.38% | Val Loss: 6.9068, Val Acc: 31.84% | Test Loss: 6.7453, Test Acc: 32.44%\n","[Central-AdamW] Epoch 45: Train Loss: 0.3340, Train Acc: 89.09% | Val Loss: 7.0980, Val Acc: 31.72% | Test Loss: 6.9023, Test Acc: 32.57%\n","[Central-AdamW] Epoch 46: Train Loss: 0.2837, Train Acc: 90.75% | Val Loss: 7.4594, Val Acc: 32.74% | Test Loss: 7.2679, Test Acc: 32.55%\n","[Central-AdamW] Epoch 47: Train Loss: 0.3128, Train Acc: 89.65% | Val Loss: 7.4654, Val Acc: 31.78% | Test Loss: 7.2667, Test Acc: 32.22%\n","[Central-AdamW] Epoch 48: Train Loss: 0.2852, Train Acc: 90.54% | Val Loss: 7.4794, Val Acc: 31.72% | Test Loss: 7.1878, Test Acc: 32.81%\n","[Central-AdamW] Epoch 49: Train Loss: 0.2893, Train Acc: 90.44% | Val Loss: 7.4864, Val Acc: 32.00% | Test Loss: 7.3481, Test Acc: 32.44%\n","[Central-AdamW] Epoch 50: Train Loss: 0.2609, Train Acc: 91.50% | Val Loss: 7.8949, Val Acc: 32.08% | Test Loss: 7.6505, Test Acc: 32.48%\n","[Central-AdamW] Epoch 51: Train Loss: 0.2642, Train Acc: 91.27% | Val Loss: 7.7335, Val Acc: 32.20% | Test Loss: 7.5670, Test Acc: 32.73%\n","[Central-AdamW] Epoch 52: Train Loss: 0.2557, Train Acc: 91.52% | Val Loss: 7.8247, Val Acc: 31.68% | Test Loss: 7.5409, Test Acc: 32.81%\n","[Central-AdamW] Epoch 53: Train Loss: 0.2468, Train Acc: 91.78% | Val Loss: 8.0365, Val Acc: 32.48% | Test Loss: 7.8799, Test Acc: 32.80%\n","[Central-AdamW] Epoch 54: Train Loss: 0.2173, Train Acc: 92.90% | Val Loss: 8.0030, Val Acc: 31.84% | Test Loss: 7.8363, Test Acc: 32.66%\n","[Central-AdamW] Epoch 55: Train Loss: 0.2153, Train Acc: 92.95% | Val Loss: 8.0878, Val Acc: 32.12% | Test Loss: 7.8328, Test Acc: 32.64%\n","[Central-AdamW] Epoch 56: Train Loss: 0.2063, Train Acc: 93.22% | Val Loss: 8.1959, Val Acc: 32.20% | Test Loss: 7.9773, Test Acc: 32.75%\n","[Central-AdamW] Epoch 57: Train Loss: 0.2022, Train Acc: 93.38% | Val Loss: 8.4741, Val Acc: 31.78% | Test Loss: 8.2650, Test Acc: 32.13%\n","[Central-AdamW] Epoch 58: Train Loss: 0.1939, Train Acc: 93.58% | Val Loss: 8.6387, Val Acc: 32.50% | Test Loss: 8.4505, Test Acc: 32.29%\n","[Central-AdamW] Epoch 59: Train Loss: 0.2124, Train Acc: 92.98% | Val Loss: 8.5427, Val Acc: 32.14% | Test Loss: 8.2633, Test Acc: 32.72%\n","[Central-AdamW] Epoch 60: Train Loss: 0.1741, Train Acc: 94.37% | Val Loss: 8.6813, Val Acc: 32.68% | Test Loss: 8.4828, Test Acc: 32.81%\n","[Central-AdamW] Epoch 61: Train Loss: 0.1535, Train Acc: 95.02% | Val Loss: 8.8665, Val Acc: 32.24% | Test Loss: 8.4593, Test Acc: 33.28%\n","[Central-AdamW] Epoch 62: Train Loss: 0.1743, Train Acc: 94.43% | Val Loss: 8.7590, Val Acc: 33.12% | Test Loss: 8.5011, Test Acc: 32.56%\n","[Central-AdamW] Epoch 63: Train Loss: 0.1592, Train Acc: 94.80% | Val Loss: 9.0119, Val Acc: 31.80% | Test Loss: 8.7283, Test Acc: 32.80%\n","[Central-AdamW] Epoch 64: Train Loss: 0.1585, Train Acc: 94.76% | Val Loss: 8.9261, Val Acc: 32.78% | Test Loss: 8.6188, Test Acc: 32.71%\n","[Central-AdamW] Epoch 65: Train Loss: 0.1404, Train Acc: 95.39% | Val Loss: 9.0559, Val Acc: 32.14% | Test Loss: 8.7618, Test Acc: 32.35%\n","[Central-AdamW] Epoch 66: Train Loss: 0.1277, Train Acc: 95.83% | Val Loss: 9.1905, Val Acc: 32.34% | Test Loss: 8.8704, Test Acc: 32.81%\n","[Central-AdamW] Epoch 67: Train Loss: 0.1557, Train Acc: 94.90% | Val Loss: 9.1194, Val Acc: 31.86% | Test Loss: 8.7541, Test Acc: 32.75%\n","[Central-AdamW] Epoch 68: Train Loss: 0.1180, Train Acc: 96.11% | Val Loss: 9.2336, Val Acc: 32.56% | Test Loss: 8.9423, Test Acc: 32.95%\n","[Central-AdamW] Epoch 69: Train Loss: 0.0945, Train Acc: 97.04% | Val Loss: 9.6363, Val Acc: 31.80% | Test Loss: 9.3397, Test Acc: 32.56%\n","[Central-AdamW] Epoch 70: Train Loss: 0.1442, Train Acc: 95.35% | Val Loss: 9.6598, Val Acc: 31.70% | Test Loss: 9.3141, Test Acc: 32.17%\n","[Central-AdamW] Epoch 71: Train Loss: 0.1180, Train Acc: 96.15% | Val Loss: 9.5778, Val Acc: 32.90% | Test Loss: 9.1668, Test Acc: 32.25%\n","[Central-AdamW] Epoch 72: Train Loss: 0.0859, Train Acc: 97.30% | Val Loss: 9.7600, Val Acc: 31.98% | Test Loss: 9.3382, Test Acc: 32.56%\n","[Central-AdamW] Epoch 73: Train Loss: 0.1106, Train Acc: 96.46% | Val Loss: 9.8735, Val Acc: 31.68% | Test Loss: 9.5646, Test Acc: 32.52%\n","[Central-AdamW] Epoch 74: Train Loss: 0.1116, Train Acc: 96.41% | Val Loss: 9.9114, Val Acc: 32.12% | Test Loss: 9.4695, Test Acc: 33.41%\n","[Central-AdamW] Epoch 75: Train Loss: 0.0753, Train Acc: 97.66% | Val Loss: 9.7592, Val Acc: 32.46% | Test Loss: 9.4068, Test Acc: 33.22%\n","[Central-AdamW] Epoch 76: Train Loss: 0.0954, Train Acc: 96.87% | Val Loss: 9.8210, Val Acc: 32.24% | Test Loss: 9.4164, Test Acc: 32.66%\n","[Central-AdamW] Epoch 77: Train Loss: 0.0901, Train Acc: 97.08% | Val Loss: 10.0065, Val Acc: 32.44% | Test Loss: 9.5930, Test Acc: 32.71%\n","[Central-AdamW] Epoch 78: Train Loss: 0.0614, Train Acc: 98.06% | Val Loss: 10.0683, Val Acc: 32.26% | Test Loss: 9.7699, Test Acc: 32.77%\n","[Central-AdamW] Epoch 79: Train Loss: 0.0705, Train Acc: 97.76% | Val Loss: 10.1524, Val Acc: 32.76% | Test Loss: 9.8581, Test Acc: 32.93%\n","[Central-AdamW] Epoch 80: Train Loss: 0.0765, Train Acc: 97.58% | Val Loss: 10.1230, Val Acc: 32.64% | Test Loss: 9.7747, Test Acc: 32.55%\n","[Central-AdamW] Epoch 81: Train Loss: 0.0611, Train Acc: 98.08% | Val Loss: 10.2785, Val Acc: 32.08% | Test Loss: 9.9008, Test Acc: 33.17%\n","[Central-AdamW] Epoch 82: Train Loss: 0.0600, Train Acc: 98.19% | Val Loss: 10.2112, Val Acc: 32.42% | Test Loss: 9.8424, Test Acc: 33.27%\n","[Central-AdamW] Epoch 83: Train Loss: 0.0635, Train Acc: 97.95% | Val Loss: 10.4143, Val Acc: 32.20% | Test Loss: 10.0605, Test Acc: 32.96%\n","[Central-AdamW] Epoch 84: Train Loss: 0.0460, Train Acc: 98.64% | Val Loss: 10.5478, Val Acc: 32.36% | Test Loss: 10.2974, Test Acc: 32.84%\n","[Central-AdamW] Epoch 85: Train Loss: 0.0532, Train Acc: 98.34% | Val Loss: 10.5449, Val Acc: 32.52% | Test Loss: 10.1903, Test Acc: 32.53%\n","[Central-AdamW] Epoch 86: Train Loss: 0.0538, Train Acc: 98.33% | Val Loss: 10.7211, Val Acc: 31.92% | Test Loss: 10.4222, Test Acc: 33.17%\n","[Central-AdamW] Epoch 87: Train Loss: 0.0371, Train Acc: 98.90% | Val Loss: 10.6582, Val Acc: 32.26% | Test Loss: 10.2742, Test Acc: 32.67%\n","[Central-AdamW] Epoch 88: Train Loss: 0.0522, Train Acc: 98.41% | Val Loss: 10.6625, Val Acc: 32.32% | Test Loss: 10.2563, Test Acc: 33.07%\n","[Central-AdamW] Epoch 89: Train Loss: 0.0297, Train Acc: 99.22% | Val Loss: 10.5736, Val Acc: 32.46% | Test Loss: 10.2074, Test Acc: 33.24%\n","[Central-AdamW] Epoch 90: Train Loss: 0.0217, Train Acc: 99.49% | Val Loss: 10.8005, Val Acc: 32.48% | Test Loss: 10.3890, Test Acc: 33.59%\n","[Central-AdamW] Epoch 91: Train Loss: 0.0457, Train Acc: 98.60% | Val Loss: 10.7041, Val Acc: 32.96% | Test Loss: 10.3970, Test Acc: 33.43%\n","[Central-AdamW] Epoch 92: Train Loss: 0.0413, Train Acc: 98.79% | Val Loss: 10.7350, Val Acc: 32.86% | Test Loss: 10.3562, Test Acc: 33.27%\n","[Central-AdamW] Epoch 93: Train Loss: 0.0209, Train Acc: 99.46% | Val Loss: 10.7152, Val Acc: 33.06% | Test Loss: 10.4071, Test Acc: 33.44%\n","[Central-AdamW] Epoch 94: Train Loss: 0.0124, Train Acc: 99.78% | Val Loss: 11.1234, Val Acc: 32.66% | Test Loss: 10.8105, Test Acc: 33.35%\n","[Central-AdamW] Epoch 95: Train Loss: 0.0266, Train Acc: 99.28% | Val Loss: 10.9593, Val Acc: 32.48% | Test Loss: 10.6145, Test Acc: 33.02%\n","[Central-AdamW] Epoch 96: Train Loss: 0.0378, Train Acc: 98.89% | Val Loss: 10.9805, Val Acc: 33.04% | Test Loss: 10.6996, Test Acc: 32.97%\n","[Central-AdamW] Epoch 97: Train Loss: 0.0105, Train Acc: 99.83% | Val Loss: 10.9341, Val Acc: 33.32% | Test Loss: 10.6027, Test Acc: 33.41%\n","[Central-AdamW] Epoch 98: Train Loss: 0.0068, Train Acc: 99.92% | Val Loss: 11.0978, Val Acc: 33.46% | Test Loss: 10.7833, Test Acc: 34.07%\n","[Central-AdamW] Epoch 99: Train Loss: 0.0199, Train Acc: 99.51% | Val Loss: 11.1554, Val Acc: 32.26% | Test Loss: 10.7572, Test Acc: 32.33%\n","[Central-AdamW] Epoch 100: Train Loss: 0.0450, Train Acc: 98.62% | Val Loss: 10.8999, Val Acc: 33.44% | Test Loss: 10.4943, Test Acc: 33.58%\n","[Central-AdamW] Epoch 101: Train Loss: 0.0076, Train Acc: 99.90% | Val Loss: 10.8468, Val Acc: 33.86% | Test Loss: 10.4732, Test Acc: 34.09%\n","[Central-AdamW] Epoch 102: Train Loss: 0.0039, Train Acc: 99.96% | Val Loss: 11.0138, Val Acc: 33.52% | Test Loss: 10.6262, Test Acc: 34.18%\n","[Central-AdamW] Epoch 103: Train Loss: 0.0040, Train Acc: 99.96% | Val Loss: 10.9748, Val Acc: 33.78% | Test Loss: 10.6156, Test Acc: 34.17%\n","[Central-AdamW] Epoch 104: Train Loss: 0.0327, Train Acc: 99.05% | Val Loss: 11.0011, Val Acc: 33.10% | Test Loss: 10.5921, Test Acc: 33.77%\n","[Central-AdamW] Epoch 105: Train Loss: 0.0122, Train Acc: 99.72% | Val Loss: 11.0357, Val Acc: 33.18% | Test Loss: 10.7031, Test Acc: 33.48%\n","[Central-AdamW] Epoch 106: Train Loss: 0.0042, Train Acc: 99.96% | Val Loss: 11.1358, Val Acc: 33.60% | Test Loss: 10.7812, Test Acc: 33.96%\n","[Central-AdamW] Epoch 107: Train Loss: 0.0039, Train Acc: 99.95% | Val Loss: 11.0618, Val Acc: 33.82% | Test Loss: 10.7461, Test Acc: 33.86%\n","[Central-AdamW] Epoch 108: Train Loss: 0.0061, Train Acc: 99.91% | Val Loss: 11.3243, Val Acc: 32.38% | Test Loss: 11.0111, Test Acc: 33.88%\n","[Central-AdamW] Epoch 109: Train Loss: 0.0085, Train Acc: 99.86% | Val Loss: 11.3929, Val Acc: 32.50% | Test Loss: 10.9015, Test Acc: 33.76%\n","[Central-AdamW] Epoch 110: Train Loss: 0.0063, Train Acc: 99.88% | Val Loss: 11.3244, Val Acc: 32.94% | Test Loss: 11.0090, Test Acc: 33.76%\n","[Central-AdamW] Epoch 111: Train Loss: 0.0112, Train Acc: 99.74% | Val Loss: 11.2663, Val Acc: 33.56% | Test Loss: 10.8685, Test Acc: 33.75%\n","[Central-AdamW] Epoch 112: Train Loss: 0.0046, Train Acc: 99.93% | Val Loss: 11.3058, Val Acc: 33.22% | Test Loss: 10.9300, Test Acc: 34.02%\n"]}]},{"cell_type":"code","source":["#code modified according also to chatgpt point 2\n","\"\"\"\n","2. Optimizer Variants\n","Project Requirement:\n","\n","Train the CNN with both the SGDM and AdamW optimizers.\n","\n","Your Code:\n","\n","The code only shows SGD with momentum (SGDM).\n","\n","Missing: The option to use the AdamW optimizer. You should add a mechanism (for example, via an argument or configuration) that lets you switch optimizers easily\n","\n","\"\"\"\n","\n","\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","import numpy as np\n","import os\n","\n","# -----------------------------\n","# 1. Dataset Preparation\n","# -----------------------------\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","])\n","\n","# Download CIFAR-100 dataset for training and validation splits\n","full_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n","val_size = int(0.1 * len(full_dataset))\n","train_size = len(full_dataset) - val_size\n","train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","# Also load the provided test set for centralized baseline evaluation\n","test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n","\n","# IID Sharding for FL simulation\n","def split_dataset_iid(dataset, num_clients):\n","    indices = np.random.permutation(len(dataset))\n","    split_size = len(dataset) // num_clients\n","    client_indices = [indices[i * split_size:(i + 1) * split_size] for i in range(num_clients)]\n","    return [Subset(dataset, inds) for inds in client_indices]\n","\n","# Non-IID Sharding: assign Nc classes per client (for extreme non-IID, use Nc=1)\n","def split_dataset_non_iid(dataset, num_clients, Nc):\n","    class_indices = {i: [] for i in range(100)}\n","    for idx, (_, target) in enumerate(dataset):\n","        class_indices[target].append(idx)\n","    client_subsets = []\n","    classes = list(range(100))\n","    for _ in range(num_clients):\n","        selected_classes = np.random.choice(classes, Nc, replace=False)\n","        client_idx = []\n","        for cls in selected_classes:\n","            cls_indices = class_indices[cls]\n","            num_samples = len(cls_indices) // num_clients\n","            client_idx.extend(cls_indices[:num_samples])\n","            class_indices[cls] = cls_indices[num_samples:]\n","        client_subsets.append(Subset(dataset, client_idx))\n","    return client_subsets\n","\n","# Set number of clients and choose the sharding method (uncomment one of the following)\n","num_clients = 10\n","client_datasets = split_dataset_iid(train_dataset, num_clients)\n","# client_datasets = split_dataset_non_iid(train_dataset, num_clients, Nc=1)\n","\n","# -----------------------------\n","# 2. Model Definition\n","# -----------------------------\n","class LeNetLikeCNN(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(LeNetLikeCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n","        self.fc2 = nn.Linear(384, 192)\n","        self.fc3 = nn.Linear(192, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x)))  # [Batch, 64, 14, 14]\n","        x = self.pool2(F.relu(self.conv2(x)))  # [Batch, 64, 5, 5]\n","        x = x.view(-1, 64 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# -----------------------------\n","# 3. Helper function: Optimizer Selector\n","# -----------------------------\n","def get_optimizer(model, optimizer_type, lr):\n","    if optimizer_type == 'SGDM':\n","        # SGD with momentum (SGDM)\n","        return optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=4e-4)\n","    elif optimizer_type == 'AdamW':\n","        # AdamW optimizer; adjust lr and weight_decay as needed\n","        return optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n","    else:\n","        raise ValueError(\"Unsupported optimizer type. Choose 'SGDM' or 'AdamW'.\")\n","\n","# -----------------------------\n","# 4. Federated Learning (FL) Simulation\n","# -----------------------------\n","def client_update(client_model, optimizer, train_loader, criterion, local_epochs=1, device='cpu'):\n","    client_model.train()\n","    for epoch in range(local_epochs):\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = client_model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","    return client_model.state_dict(), len(train_loader.dataset)\n","\n","def select_clients(client_datasets, fraction=0.1):\n","    num_clients_to_select = max(1, int(len(client_datasets) * fraction))\n","    selected = np.random.choice(len(client_datasets), num_clients_to_select, replace=False)\n","    return [client_datasets[i] for i in selected]\n","\n","def aggregate_updates(global_model, client_states, client_sizes):\n","    global_state = global_model.state_dict()\n","    total_samples = sum(client_sizes)\n","    for key in global_state.keys():\n","        global_state[key] = torch.zeros_like(global_state[key])\n","        for state, size in zip(client_states, client_sizes):\n","            global_state[key] += state[key] * (size / total_samples)\n","    global_model.load_state_dict(global_state)\n","    return global_model\n","\n","# Choose FL optimizer type for simulation ('SGDM' or 'AdamW')\n","fl_optimizer_type = 'SGDM'  # Change to 'AdamW' to use AdamW in the FL simulation\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","global_model_FL = LeNetLikeCNN(num_classes=100).to(device)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","criterion = nn.CrossEntropyLoss()\n","writer_FL = SummaryWriter(log_dir='./logs/fl')\n","checkpoint_dir = './checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","# Federated learning parameters\n","num_rounds = 200     # Number of federated rounds\n","local_epochs = 1     # Local epochs per round\n","client_fraction = 0.5  # Fraction of clients participating per round\n","\n","for round in range(1, num_rounds+1):\n","    selected_clients = select_clients(client_datasets, fraction=client_fraction)\n","    client_states = []\n","    client_sizes = []\n","    for client_data in selected_clients:\n","        client_loader = DataLoader(client_data, batch_size=64, shuffle=True)\n","        # Create a fresh copy of the global model for each client\n","        client_model = LeNetLikeCNN(num_classes=100).to(device)\n","        client_model.load_state_dict(global_model_FL.state_dict())\n","        # Select optimizer based on fl_optimizer_type\n","        optimizer = get_optimizer(client_model, fl_optimizer_type, lr=0.1 if fl_optimizer_type=='SGDM' else 0.001)\n","        state_dict, num_samples = client_update(client_model, optimizer, client_loader, criterion, local_epochs, device)\n","        client_states.append(state_dict)\n","        client_sizes.append(num_samples)\n","\n","    global_model_FL = aggregate_updates(global_model_FL, client_states, client_sizes)\n","\n","    # Evaluate global model on validation set\n","    global_model_FL.eval()\n","    total, correct, val_loss = 0, 0, 0.0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = global_model_FL(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","    avg_val_loss = val_loss / len(val_loader)\n","    accuracy = 100. * correct / total\n","    print(f\"FL Round {round}: Val Loss = {avg_val_loss:.4f}, Val Accuracy = {accuracy:.2f}%\")\n","    writer_FL.add_scalar('FL/Val_Loss', avg_val_loss, round)\n","    writer_FL.add_scalar('FL/Val_Accuracy', accuracy, round)\n","\n","    if round % 10 == 0:\n","        torch.save({\n","            'round': round,\n","            'global_model_state': global_model_FL.state_dict(),\n","        }, os.path.join(checkpoint_dir, f'global_model_round_{round}.pth'))\n","writer_FL.close()\n","\n","# -----------------------------\n","# 5. Centralized Baseline Training\n","# -----------------------------\n","# Create DataLoaders for centralized training:\n","batch_size = 64\n","central_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","central_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Function to compute loss and accuracy for evaluation\n","def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    total, correct, loss_sum = 0, 0, 0.0\n","    with torch.no_grad():\n","        for inputs, labels in loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss_sum += loss.item() * labels.size(0)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","    avg_loss = loss_sum / total\n","    accuracy = 100. * correct / total\n","    return avg_loss, accuracy\n","\n","# Centralized training loop with configurable optimizer\n","def centralized_training(optimizer_type='SGDM'):\n","    model = LeNetLikeCNN(num_classes=100).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    # Select optimizer based on optimizer_type argument\n","    optimizer = get_optimizer(model, optimizer_type, lr=0.1 if optimizer_type=='SGDM' else 0.001)\n","\n","    # Cosine annealing scheduler (T_max is set to 150 epochs)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=150)\n","\n","    writer = SummaryWriter(log_dir=f'./logs/central_{optimizer_type}')\n","    num_epochs = 150\n","    for epoch in range(1, num_epochs+1):\n","        model.train()\n","        running_loss = 0.0\n","        running_correct = 0\n","        total_samples = 0\n","\n","        for inputs, labels in central_train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * labels.size(0)\n","            _, predicted = outputs.max(1)\n","            total_samples += labels.size(0)\n","            running_correct += predicted.eq(labels).sum().item()\n","\n","        train_loss = running_loss / total_samples\n","        train_accuracy = 100. * running_correct / total_samples\n","\n","        val_loss, val_accuracy = evaluate(model, central_val_loader, criterion, device)\n","        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n","\n","        writer.add_scalar('Central/Train_Loss', train_loss, epoch)\n","        writer.add_scalar('Central/Train_Accuracy', train_accuracy, epoch)\n","        writer.add_scalar('Central/Val_Loss', val_loss, epoch)\n","        writer.add_scalar('Central/Val_Accuracy', val_accuracy, epoch)\n","        writer.add_scalar('Central/Test_Loss', test_loss, epoch)\n","        writer.add_scalar('Central/Test_Accuracy', test_accuracy, epoch)\n","\n","        print(f\"[Central-{optimizer_type}] Epoch {epoch}: \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n","              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}% | \"\n","              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n","\n","        scheduler.step()\n","\n","    writer.close()\n","    os.makedirs(f'./checkpoints/central_{optimizer_type}', exist_ok=True)\n","    torch.save(model.state_dict(), f'./checkpoints/central_{optimizer_type}/final_model.pth')\n","\n","# Run centralized training for both optimizers\n","centralized_training(optimizer_type='SGDM')\n","centralized_training(optimizer_type='AdamW')\n"],"metadata":{"id":"veUiWXcNk7kS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#chat gpt code modified according to point 3\n","\n","\n","\"\"\"\n","3. Cosine Annealing Scheduler and Epoch Count\n","Project Requirement:\n","\n","Train for 150 epochs and use a cosine annealing learning rate scheduler.\n","\n","Your Code:\n","\n","In your centralized baseline snippet, you set up a cosine annealing scheduler (CosineAnnealingLR). However, the FL loop is defined in terms of “communication rounds” (set to 200 rounds in the example) rather than training epochs.\n","\n","Difference: The centralized baseline (if implemented separately) should run for 150 epochs, while the FL simulation is typically described in rounds. You might consider providing both: a centralized run to compare with, and then the federated simulation.\n","\"\"\"\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","import numpy as np\n","import os\n","\n","# -----------------------------\n","# 1. Dataset Preparation\n","# -----------------------------\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","])\n","\n","# Download CIFAR-100 dataset for training and validation\n","full_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n","val_size = int(0.1 * len(full_dataset))\n","train_size = len(full_dataset) - val_size\n","train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","# Load the provided test set for centralized baseline evaluation\n","test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n","\n","# IID Sharding for FL simulation\n","def split_dataset_iid(dataset, num_clients):\n","    indices = np.random.permutation(len(dataset))\n","    split_size = len(dataset) // num_clients\n","    client_indices = [indices[i * split_size:(i + 1) * split_size] for i in range(num_clients)]\n","    return [Subset(dataset, inds) for inds in client_indices]\n","\n","# Non-IID Sharding: assign Nc classes per client (for extreme non-IID set Nc=1)\n","def split_dataset_non_iid(dataset, num_clients, Nc):\n","    class_indices = {i: [] for i in range(100)}\n","    for idx, (_, target) in enumerate(dataset):\n","        class_indices[target].append(idx)\n","    client_subsets = []\n","    classes = list(range(100))\n","    for _ in range(num_clients):\n","        selected_classes = np.random.choice(classes, Nc, replace=False)\n","        client_idx = []\n","        for cls in selected_classes:\n","            cls_indices = class_indices[cls]\n","            num_samples = len(cls_indices) // num_clients\n","            client_idx.extend(cls_indices[:num_samples])\n","            class_indices[cls] = cls_indices[num_samples:]\n","        client_subsets.append(Subset(dataset, client_idx))\n","    return client_subsets\n","\n","# Set number of clients and choose sharding method (IID is used here)\n","num_clients = 10\n","client_datasets = split_dataset_iid(train_dataset, num_clients)\n","# Uncomment the next line to use a non-IID split, e.g. with Nc=1\n","# client_datasets = split_dataset_non_iid(train_dataset, num_clients, Nc=1)\n","\n","# -----------------------------\n","# 2. Model Definition\n","# -----------------------------\n","class LeNetLikeCNN(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(LeNetLikeCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        # Feature map size becomes 5x5 for 32x32 input after two conv+pool layers\n","        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n","        self.fc2 = nn.Linear(384, 192)\n","        self.fc3 = nn.Linear(192, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x)))  # [B, 64, 14, 14]\n","        x = self.pool2(F.relu(self.conv2(x)))  # [B, 64, 5, 5]\n","        x = x.view(-1, 64 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# -----------------------------\n","# 3. Optimizer Selector Function\n","# -----------------------------\n","def get_optimizer(model, optimizer_type, lr):\n","    if optimizer_type == 'SGDM':\n","        return optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=4e-4)\n","    elif optimizer_type == 'AdamW':\n","        return optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n","    else:\n","        raise ValueError(\"Unsupported optimizer type. Choose 'SGDM' or 'AdamW'.\")\n","\n","# -----------------------------\n","# 4. Federated Learning (FL) Simulation\n","# -----------------------------\n","# Note: In FL simulation we describe training in terms of communication rounds.\n","# To roughly align with the centralized training of 150 epochs, we now set the number of rounds to 150.\n","def client_update(client_model, optimizer, train_loader, criterion, local_epochs=1, device='cpu'):\n","    client_model.train()\n","    for _ in range(local_epochs):\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = client_model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","    return client_model.state_dict(), len(train_loader.dataset)\n","\n","def select_clients(client_datasets, fraction=0.1):\n","    num_clients_to_select = max(1, int(len(client_datasets) * fraction))\n","    selected = np.random.choice(len(client_datasets), num_clients_to_select, replace=False)\n","    return [client_datasets[i] for i in selected]\n","\n","def aggregate_updates(global_model, client_states, client_sizes):\n","    global_state = global_model.state_dict()\n","    total_samples = sum(client_sizes)\n","    for key in global_state.keys():\n","        global_state[key] = torch.zeros_like(global_state[key])\n","        for state, size in zip(client_states, client_sizes):\n","            global_state[key] += state[key] * (size / total_samples)\n","    global_model.load_state_dict(global_state)\n","    return global_model\n","\n","# Choose FL optimizer type ('SGDM' or 'AdamW')\n","fl_optimizer_type = 'SGDM'  # Change to 'AdamW' to use AdamW for FL simulation\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","global_model_FL = LeNetLikeCNN(num_classes=100).to(device)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","criterion = nn.CrossEntropyLoss()\n","writer_FL = SummaryWriter(log_dir='./logs/fl')\n","checkpoint_dir = './checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","# Set FL simulation parameters: 150 rounds to roughly align with 150 epochs.\n","num_rounds = 150\n","local_epochs = 1     # Number of local epochs per round\n","client_fraction = 0.5  # Fraction of clients participating\n","\n","for round in range(1, num_rounds+1):\n","    selected_clients = select_clients(client_datasets, fraction=client_fraction)\n","    client_states = []\n","    client_sizes = []\n","    for client_data in selected_clients:\n","        client_loader = DataLoader(client_data, batch_size=64, shuffle=True)\n","        client_model = LeNetLikeCNN(num_classes=100).to(device)\n","        client_model.load_state_dict(global_model_FL.state_dict())\n","        # Select the optimizer based on fl_optimizer_type; use lr=0.1 for SGDM and 0.001 for AdamW\n","        optimizer = get_optimizer(client_model, fl_optimizer_type, lr=0.1 if fl_optimizer_type=='SGDM' else 0.001)\n","        state_dict, num_samples = client_update(client_model, optimizer, client_loader, criterion, local_epochs, device)\n","        client_states.append(state_dict)\n","        client_sizes.append(num_samples)\n","\n","    global_model_FL = aggregate_updates(global_model_FL, client_states, client_sizes)\n","\n","    # Evaluate global model on the validation set after each round\n","    global_model_FL.eval()\n","    total, correct, val_loss = 0, 0, 0.0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = global_model_FL(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","    avg_val_loss = val_loss / len(val_loader)\n","    accuracy = 100. * correct / total\n","    print(f\"[FL] Round {round}: Val Loss = {avg_val_loss:.4f}, Val Accuracy = {accuracy:.2f}%\")\n","    writer_FL.add_scalar('FL/Val_Loss', avg_val_loss, round)\n","    writer_FL.add_scalar('FL/Val_Accuracy', accuracy, round)\n","\n","    if round % 10 == 0:\n","        torch.save({\n","            'round': round,\n","            'global_model_state': global_model_FL.state_dict(),\n","        }, os.path.join(checkpoint_dir, f'global_model_round_{round}.pth'))\n","writer_FL.close()\n","\n","# -----------------------------\n","# 5. Centralized Baseline Training\n","# -----------------------------\n","# Centralized baseline training runs for 150 epochs and uses a cosine annealing scheduler.\n","# Create DataLoaders for centralized training:\n","batch_size = 64\n","central_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","central_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    total, correct, loss_sum = 0, 0, 0.0\n","    with torch.no_grad():\n","        for inputs, labels in loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss_sum += loss.item() * labels.size(0)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","    avg_loss = loss_sum / total\n","    accuracy = 100. * correct / total\n","    return avg_loss, accuracy\n","\n","def centralized_training(optimizer_type='SGDM'):\n","    model = LeNetLikeCNN(num_classes=100).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    # Choose optimizer based on the optimizer_type argument\n","    optimizer = get_optimizer(model, optimizer_type, lr=0.1 if optimizer_type == 'SGDM' else 0.001)\n","\n","    # Cosine annealing scheduler with T_max = 150 epochs\n","    scheduler = CosineAnnealingLR(optimizer, T_max=150)\n","\n","    writer = SummaryWriter(log_dir=f'./logs/central_{optimizer_type}')\n","    num_epochs = 150\n","    for epoch in range(1, num_epochs+1):\n","        model.train()\n","        running_loss = 0.0\n","        running_correct = 0\n","        total_samples = 0\n","\n","        for inputs, labels in central_train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * labels.size(0)\n","            _, predicted = outputs.max(1)\n","            total_samples += labels.size(0)\n","            running_correct += predicted.eq(labels).sum().item()\n","\n","        train_loss = running_loss / total_samples\n","        train_accuracy = 100. * running_correct / total_samples\n","\n","        val_loss, val_accuracy = evaluate(model, central_val_loader, criterion, device)\n","        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n","\n","        writer.add_scalar('Central/Train_Loss', train_loss, epoch)\n","        writer.add_scalar('Central/Train_Accuracy', train_accuracy, epoch)\n","        writer.add_scalar('Central/Val_Loss', val_loss, epoch)\n","        writer.add_scalar('Central/Val_Accuracy', val_accuracy, epoch)\n","        writer.add_scalar('Central/Test_Loss', test_loss, epoch)\n","        writer.add_scalar('Central/Test_Accuracy', test_accuracy, epoch)\n","\n","        print(f\"[Central-{optimizer_type}] Epoch {epoch}: \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n","              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}% | \"\n","              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n","\n","        scheduler.step()\n","\n","    writer.close()\n","    os.makedirs(f'./checkpoints/central_{optimizer_type}', exist_ok=True)\n","    torch.save(model.state_dict(), f'./checkpoints/central_{optimizer_type}/final_model.pth')\n","\n","# Run centralized baseline training for both optimizer variants:\n","centralized_training(optimizer_type='SGDM')\n","centralized_training(optimizer_type='AdamW')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uwL2bhsYcBFN","executionInfo":{"status":"ok","timestamp":1744240936128,"user_tz":-120,"elapsed":6900701,"user":{"displayName":"Mattia Cappellino","userId":"05271634366428679167"}},"outputId":"c936139e-bde4-4500-9569-fe8ce9660284"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:05<00:00, 29.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[FL] Round 1: Val Loss = 4.5709, Val Accuracy = 2.40%\n","[FL] Round 2: Val Loss = 4.5073, Val Accuracy = 2.64%\n","[FL] Round 3: Val Loss = 4.4536, Val Accuracy = 3.26%\n","[FL] Round 4: Val Loss = 4.4548, Val Accuracy = 3.16%\n","[FL] Round 5: Val Loss = 4.4189, Val Accuracy = 3.14%\n","[FL] Round 6: Val Loss = 4.3072, Val Accuracy = 6.30%\n","[FL] Round 7: Val Loss = 4.3215, Val Accuracy = 5.26%\n","[FL] Round 8: Val Loss = 4.2054, Val Accuracy = 7.38%\n","[FL] Round 9: Val Loss = 4.2032, Val Accuracy = 7.32%\n","[FL] Round 10: Val Loss = 3.9894, Val Accuracy = 9.42%\n","[FL] Round 11: Val Loss = 3.9582, Val Accuracy = 10.82%\n","[FL] Round 12: Val Loss = 4.0215, Val Accuracy = 11.08%\n","[FL] Round 13: Val Loss = 3.9585, Val Accuracy = 10.90%\n","[FL] Round 14: Val Loss = 3.9344, Val Accuracy = 11.02%\n","[FL] Round 15: Val Loss = 3.9003, Val Accuracy = 12.00%\n","[FL] Round 16: Val Loss = 3.8394, Val Accuracy = 12.16%\n","[FL] Round 17: Val Loss = 3.8156, Val Accuracy = 13.12%\n","[FL] Round 18: Val Loss = 3.7513, Val Accuracy = 13.88%\n","[FL] Round 19: Val Loss = 3.7758, Val Accuracy = 13.58%\n","[FL] Round 20: Val Loss = 3.6978, Val Accuracy = 15.56%\n","[FL] Round 21: Val Loss = 3.7138, Val Accuracy = 15.46%\n","[FL] Round 22: Val Loss = 3.6522, Val Accuracy = 16.76%\n","[FL] Round 23: Val Loss = 3.6214, Val Accuracy = 16.36%\n","[FL] Round 24: Val Loss = 3.5902, Val Accuracy = 16.86%\n","[FL] Round 25: Val Loss = 3.6536, Val Accuracy = 16.16%\n","[FL] Round 26: Val Loss = 3.6220, Val Accuracy = 17.52%\n","[FL] Round 27: Val Loss = 3.5679, Val Accuracy = 18.04%\n","[FL] Round 28: Val Loss = 3.5803, Val Accuracy = 17.36%\n","[FL] Round 29: Val Loss = 3.5619, Val Accuracy = 17.68%\n","[FL] Round 30: Val Loss = 3.4814, Val Accuracy = 19.74%\n","[FL] Round 31: Val Loss = 3.4743, Val Accuracy = 18.34%\n","[FL] Round 32: Val Loss = 3.4316, Val Accuracy = 20.42%\n","[FL] Round 33: Val Loss = 3.4577, Val Accuracy = 19.56%\n","[FL] Round 34: Val Loss = 3.4756, Val Accuracy = 19.92%\n","[FL] Round 35: Val Loss = 3.4242, Val Accuracy = 19.46%\n","[FL] Round 36: Val Loss = 3.3136, Val Accuracy = 22.08%\n","[FL] Round 37: Val Loss = 3.3460, Val Accuracy = 21.14%\n","[FL] Round 38: Val Loss = 3.3799, Val Accuracy = 21.70%\n","[FL] Round 39: Val Loss = 3.3057, Val Accuracy = 22.34%\n","[FL] Round 40: Val Loss = 3.4048, Val Accuracy = 21.40%\n","[FL] Round 41: Val Loss = 3.3956, Val Accuracy = 21.00%\n","[FL] Round 42: Val Loss = 3.3370, Val Accuracy = 20.98%\n","[FL] Round 43: Val Loss = 3.3569, Val Accuracy = 22.66%\n","[FL] Round 44: Val Loss = 3.3457, Val Accuracy = 22.08%\n","[FL] Round 45: Val Loss = 3.2829, Val Accuracy = 23.18%\n","[FL] Round 46: Val Loss = 3.3232, Val Accuracy = 22.32%\n","[FL] Round 47: Val Loss = 3.2895, Val Accuracy = 23.20%\n","[FL] Round 48: Val Loss = 3.2752, Val Accuracy = 22.52%\n","[FL] Round 49: Val Loss = 3.3300, Val Accuracy = 22.40%\n","[FL] Round 50: Val Loss = 3.3314, Val Accuracy = 21.78%\n","[FL] Round 51: Val Loss = 3.3491, Val Accuracy = 21.60%\n","[FL] Round 52: Val Loss = 3.2996, Val Accuracy = 23.30%\n","[FL] Round 53: Val Loss = 3.2936, Val Accuracy = 23.34%\n","[FL] Round 54: Val Loss = 3.2819, Val Accuracy = 23.36%\n","[FL] Round 55: Val Loss = 3.2609, Val Accuracy = 23.64%\n","[FL] Round 56: Val Loss = 3.2542, Val Accuracy = 23.86%\n","[FL] Round 57: Val Loss = 3.2368, Val Accuracy = 24.44%\n","[FL] Round 58: Val Loss = 3.2558, Val Accuracy = 24.16%\n","[FL] Round 59: Val Loss = 3.2810, Val Accuracy = 23.88%\n","[FL] Round 60: Val Loss = 3.2543, Val Accuracy = 24.16%\n","[FL] Round 61: Val Loss = 3.3298, Val Accuracy = 23.46%\n","[FL] Round 62: Val Loss = 3.2699, Val Accuracy = 23.32%\n","[FL] Round 63: Val Loss = 3.2434, Val Accuracy = 24.14%\n","[FL] Round 64: Val Loss = 3.2477, Val Accuracy = 23.92%\n","[FL] Round 65: Val Loss = 3.2396, Val Accuracy = 23.10%\n","[FL] Round 66: Val Loss = 3.2628, Val Accuracy = 23.60%\n","[FL] Round 67: Val Loss = 3.2765, Val Accuracy = 23.86%\n","[FL] Round 68: Val Loss = 3.2528, Val Accuracy = 23.92%\n","[FL] Round 69: Val Loss = 3.2372, Val Accuracy = 23.18%\n","[FL] Round 70: Val Loss = 3.3054, Val Accuracy = 23.28%\n","[FL] Round 71: Val Loss = 3.2850, Val Accuracy = 23.74%\n","[FL] Round 72: Val Loss = 3.2209, Val Accuracy = 25.30%\n","[FL] Round 73: Val Loss = 3.2309, Val Accuracy = 24.16%\n","[FL] Round 74: Val Loss = 3.2264, Val Accuracy = 24.68%\n","[FL] Round 75: Val Loss = 3.2875, Val Accuracy = 23.28%\n","[FL] Round 76: Val Loss = 3.2283, Val Accuracy = 24.18%\n","[FL] Round 77: Val Loss = 3.2645, Val Accuracy = 23.88%\n","[FL] Round 78: Val Loss = 3.2387, Val Accuracy = 23.98%\n","[FL] Round 79: Val Loss = 3.2349, Val Accuracy = 24.48%\n","[FL] Round 80: Val Loss = 3.2588, Val Accuracy = 24.52%\n","[FL] Round 81: Val Loss = 3.1988, Val Accuracy = 25.34%\n","[FL] Round 82: Val Loss = 3.2103, Val Accuracy = 25.74%\n","[FL] Round 83: Val Loss = 3.2335, Val Accuracy = 24.66%\n","[FL] Round 84: Val Loss = 3.2743, Val Accuracy = 23.98%\n","[FL] Round 85: Val Loss = 3.2767, Val Accuracy = 24.36%\n","[FL] Round 86: Val Loss = 3.2437, Val Accuracy = 24.32%\n","[FL] Round 87: Val Loss = 3.2087, Val Accuracy = 25.24%\n","[FL] Round 88: Val Loss = 3.2231, Val Accuracy = 24.86%\n","[FL] Round 89: Val Loss = 3.2704, Val Accuracy = 24.58%\n","[FL] Round 90: Val Loss = 3.2516, Val Accuracy = 24.24%\n","[FL] Round 91: Val Loss = 3.3565, Val Accuracy = 22.86%\n","[FL] Round 92: Val Loss = 3.2413, Val Accuracy = 24.82%\n","[FL] Round 93: Val Loss = 3.3249, Val Accuracy = 22.68%\n","[FL] Round 94: Val Loss = 3.2313, Val Accuracy = 24.74%\n","[FL] Round 95: Val Loss = 3.2473, Val Accuracy = 24.74%\n","[FL] Round 96: Val Loss = 3.2414, Val Accuracy = 24.74%\n","[FL] Round 97: Val Loss = 3.2670, Val Accuracy = 24.10%\n","[FL] Round 98: Val Loss = 3.3107, Val Accuracy = 23.38%\n","[FL] Round 99: Val Loss = 3.3452, Val Accuracy = 23.18%\n","[FL] Round 100: Val Loss = 3.2052, Val Accuracy = 24.90%\n","[FL] Round 101: Val Loss = 3.2975, Val Accuracy = 23.94%\n","[FL] Round 102: Val Loss = 3.2645, Val Accuracy = 24.84%\n","[FL] Round 103: Val Loss = 3.2640, Val Accuracy = 24.80%\n","[FL] Round 104: Val Loss = 3.2581, Val Accuracy = 25.38%\n","[FL] Round 105: Val Loss = 3.2740, Val Accuracy = 24.00%\n","[FL] Round 106: Val Loss = 3.3351, Val Accuracy = 22.94%\n","[FL] Round 107: Val Loss = 3.3005, Val Accuracy = 23.98%\n","[FL] Round 108: Val Loss = 3.2702, Val Accuracy = 24.08%\n","[FL] Round 109: Val Loss = 3.2820, Val Accuracy = 23.82%\n","[FL] Round 110: Val Loss = 3.3002, Val Accuracy = 23.54%\n","[FL] Round 111: Val Loss = 3.3255, Val Accuracy = 23.18%\n","[FL] Round 112: Val Loss = 3.3308, Val Accuracy = 23.94%\n","[FL] Round 113: Val Loss = 3.2656, Val Accuracy = 24.40%\n","[FL] Round 114: Val Loss = 3.2159, Val Accuracy = 25.88%\n","[FL] Round 115: Val Loss = 3.2391, Val Accuracy = 24.96%\n","[FL] Round 116: Val Loss = 3.2319, Val Accuracy = 25.64%\n","[FL] Round 117: Val Loss = 3.3068, Val Accuracy = 23.74%\n","[FL] Round 118: Val Loss = 3.2670, Val Accuracy = 25.20%\n","[FL] Round 119: Val Loss = 3.1999, Val Accuracy = 25.16%\n","[FL] Round 120: Val Loss = 3.2770, Val Accuracy = 23.88%\n","[FL] Round 121: Val Loss = 3.2448, Val Accuracy = 25.44%\n","[FL] Round 122: Val Loss = 3.2800, Val Accuracy = 24.08%\n","[FL] Round 123: Val Loss = 3.3129, Val Accuracy = 24.42%\n","[FL] Round 124: Val Loss = 3.2871, Val Accuracy = 24.40%\n","[FL] Round 125: Val Loss = 3.3268, Val Accuracy = 24.02%\n","[FL] Round 126: Val Loss = 3.3073, Val Accuracy = 23.62%\n","[FL] Round 127: Val Loss = 3.2806, Val Accuracy = 24.98%\n","[FL] Round 128: Val Loss = 3.3710, Val Accuracy = 22.76%\n","[FL] Round 129: Val Loss = 3.3098, Val Accuracy = 24.40%\n","[FL] Round 130: Val Loss = 3.3072, Val Accuracy = 25.28%\n","[FL] Round 131: Val Loss = 3.3130, Val Accuracy = 23.92%\n","[FL] Round 132: Val Loss = 3.3095, Val Accuracy = 25.16%\n","[FL] Round 133: Val Loss = 3.2781, Val Accuracy = 25.14%\n","[FL] Round 134: Val Loss = 3.2650, Val Accuracy = 24.76%\n","[FL] Round 135: Val Loss = 3.2638, Val Accuracy = 25.48%\n","[FL] Round 136: Val Loss = 3.3313, Val Accuracy = 24.16%\n","[FL] Round 137: Val Loss = 3.3492, Val Accuracy = 24.22%\n","[FL] Round 138: Val Loss = 3.3554, Val Accuracy = 24.48%\n","[FL] Round 139: Val Loss = 3.3074, Val Accuracy = 24.64%\n","[FL] Round 140: Val Loss = 3.2892, Val Accuracy = 24.98%\n","[FL] Round 141: Val Loss = 3.3583, Val Accuracy = 23.86%\n","[FL] Round 142: Val Loss = 3.2793, Val Accuracy = 25.74%\n","[FL] Round 143: Val Loss = 3.3362, Val Accuracy = 24.44%\n","[FL] Round 144: Val Loss = 3.2696, Val Accuracy = 25.72%\n","[FL] Round 145: Val Loss = 3.3014, Val Accuracy = 25.16%\n","[FL] Round 146: Val Loss = 3.3731, Val Accuracy = 23.58%\n","[FL] Round 147: Val Loss = 3.3583, Val Accuracy = 23.90%\n","[FL] Round 148: Val Loss = 3.3180, Val Accuracy = 24.60%\n","[FL] Round 149: Val Loss = 3.3448, Val Accuracy = 24.92%\n","[FL] Round 150: Val Loss = 3.3969, Val Accuracy = 24.00%\n","[Central-SGDM] Epoch 1: Train Loss: 4.3539, Train Acc: 3.84% | Val Loss: 4.3320, Val Acc: 4.18% | Test Loss: 4.3203, Test Acc: 3.83%\n","[Central-SGDM] Epoch 2: Train Loss: 4.2205, Train Acc: 5.73% | Val Loss: 4.2673, Val Acc: 5.80% | Test Loss: 4.2416, Test Acc: 5.77%\n","[Central-SGDM] Epoch 3: Train Loss: 4.1562, Train Acc: 6.81% | Val Loss: 4.1640, Val Acc: 6.40% | Test Loss: 4.1213, Test Acc: 7.08%\n","[Central-SGDM] Epoch 4: Train Loss: 4.1315, Train Acc: 7.08% | Val Loss: 4.2969, Val Acc: 6.32% | Test Loss: 4.2718, Test Acc: 6.69%\n","[Central-SGDM] Epoch 5: Train Loss: 4.0925, Train Acc: 7.90% | Val Loss: 4.1404, Val Acc: 8.42% | Test Loss: 4.1525, Test Acc: 8.43%\n","[Central-SGDM] Epoch 6: Train Loss: 4.0709, Train Acc: 8.44% | Val Loss: 4.1764, Val Acc: 8.44% | Test Loss: 4.1224, Test Acc: 8.41%\n","[Central-SGDM] Epoch 7: Train Loss: 4.0188, Train Acc: 9.03% | Val Loss: 4.1682, Val Acc: 8.00% | Test Loss: 4.1018, Test Acc: 8.34%\n","[Central-SGDM] Epoch 8: Train Loss: 4.0037, Train Acc: 9.80% | Val Loss: 4.3341, Val Acc: 7.66% | Test Loss: 4.3244, Test Acc: 7.89%\n","[Central-SGDM] Epoch 9: Train Loss: 4.0172, Train Acc: 9.37% | Val Loss: 4.0747, Val Acc: 8.86% | Test Loss: 4.0814, Test Acc: 8.63%\n","[Central-SGDM] Epoch 10: Train Loss: 3.9955, Train Acc: 9.82% | Val Loss: 3.9566, Val Acc: 9.62% | Test Loss: 3.9447, Test Acc: 9.56%\n","[Central-SGDM] Epoch 11: Train Loss: 3.9670, Train Acc: 10.09% | Val Loss: 4.0063, Val Acc: 10.04% | Test Loss: 3.9580, Test Acc: 10.67%\n","[Central-SGDM] Epoch 12: Train Loss: 3.9971, Train Acc: 10.12% | Val Loss: 4.1165, Val Acc: 8.84% | Test Loss: 4.1152, Test Acc: 8.51%\n","[Central-SGDM] Epoch 13: Train Loss: 3.9353, Train Acc: 10.76% | Val Loss: 3.8843, Val Acc: 11.54% | Test Loss: 3.8657, Test Acc: 11.84%\n","[Central-SGDM] Epoch 14: Train Loss: 3.9235, Train Acc: 11.30% | Val Loss: 3.9828, Val Acc: 10.76% | Test Loss: 3.9826, Test Acc: 10.19%\n","[Central-SGDM] Epoch 15: Train Loss: 3.9205, Train Acc: 11.06% | Val Loss: 4.0017, Val Acc: 9.84% | Test Loss: 3.9822, Test Acc: 10.66%\n","[Central-SGDM] Epoch 16: Train Loss: 3.9003, Train Acc: 11.51% | Val Loss: 4.1619, Val Acc: 7.32% | Test Loss: 4.1244, Test Acc: 7.62%\n","[Central-SGDM] Epoch 17: Train Loss: 3.9084, Train Acc: 11.36% | Val Loss: 4.0732, Val Acc: 8.96% | Test Loss: 4.0527, Test Acc: 9.81%\n","[Central-SGDM] Epoch 18: Train Loss: 3.8940, Train Acc: 11.64% | Val Loss: 3.9440, Val Acc: 10.50% | Test Loss: 3.9288, Test Acc: 11.36%\n","[Central-SGDM] Epoch 19: Train Loss: 3.8366, Train Acc: 12.37% | Val Loss: 3.8780, Val Acc: 12.26% | Test Loss: 3.8478, Test Acc: 12.73%\n","[Central-SGDM] Epoch 20: Train Loss: 3.8531, Train Acc: 12.33% | Val Loss: 3.9178, Val Acc: 11.40% | Test Loss: 3.8839, Test Acc: 12.78%\n","[Central-SGDM] Epoch 21: Train Loss: 3.8739, Train Acc: 12.11% | Val Loss: 3.9529, Val Acc: 10.70% | Test Loss: 3.9422, Test Acc: 11.07%\n","[Central-SGDM] Epoch 22: Train Loss: 3.8369, Train Acc: 12.78% | Val Loss: 3.9476, Val Acc: 11.08% | Test Loss: 3.9383, Test Acc: 11.78%\n","[Central-SGDM] Epoch 23: Train Loss: 3.8415, Train Acc: 12.80% | Val Loss: 3.8690, Val Acc: 11.48% | Test Loss: 3.8416, Test Acc: 11.95%\n","[Central-SGDM] Epoch 24: Train Loss: 3.8086, Train Acc: 13.11% | Val Loss: 3.8458, Val Acc: 12.22% | Test Loss: 3.8391, Test Acc: 13.08%\n","[Central-SGDM] Epoch 25: Train Loss: 3.8129, Train Acc: 12.96% | Val Loss: 3.8308, Val Acc: 13.00% | Test Loss: 3.8193, Test Acc: 13.23%\n","[Central-SGDM] Epoch 26: Train Loss: 3.8034, Train Acc: 13.65% | Val Loss: 3.9634, Val Acc: 10.92% | Test Loss: 3.9554, Test Acc: 11.16%\n","[Central-SGDM] Epoch 27: Train Loss: 3.8034, Train Acc: 13.54% | Val Loss: 3.8237, Val Acc: 13.00% | Test Loss: 3.7798, Test Acc: 13.80%\n","[Central-SGDM] Epoch 28: Train Loss: 3.7592, Train Acc: 14.11% | Val Loss: 3.9043, Val Acc: 12.70% | Test Loss: 3.8519, Test Acc: 13.16%\n","[Central-SGDM] Epoch 29: Train Loss: 3.7350, Train Acc: 14.45% | Val Loss: 3.9856, Val Acc: 11.58% | Test Loss: 3.9483, Test Acc: 11.78%\n","[Central-SGDM] Epoch 30: Train Loss: 3.7317, Train Acc: 14.29% | Val Loss: 3.8292, Val Acc: 13.70% | Test Loss: 3.7723, Test Acc: 14.60%\n","[Central-SGDM] Epoch 31: Train Loss: 3.7161, Train Acc: 14.74% | Val Loss: 3.8105, Val Acc: 12.36% | Test Loss: 3.7633, Test Acc: 13.36%\n","[Central-SGDM] Epoch 32: Train Loss: 3.7222, Train Acc: 14.67% | Val Loss: 3.9005, Val Acc: 13.22% | Test Loss: 3.8751, Test Acc: 13.50%\n","[Central-SGDM] Epoch 33: Train Loss: 3.7368, Train Acc: 14.70% | Val Loss: 3.7912, Val Acc: 13.96% | Test Loss: 3.7293, Test Acc: 15.30%\n","[Central-SGDM] Epoch 34: Train Loss: 3.7049, Train Acc: 15.14% | Val Loss: 3.9516, Val Acc: 10.86% | Test Loss: 3.9159, Test Acc: 11.57%\n","[Central-SGDM] Epoch 35: Train Loss: 3.6909, Train Acc: 15.34% | Val Loss: 3.9469, Val Acc: 12.96% | Test Loss: 3.9053, Test Acc: 12.34%\n","[Central-SGDM] Epoch 36: Train Loss: 3.6965, Train Acc: 15.24% | Val Loss: 3.8644, Val Acc: 12.18% | Test Loss: 3.8311, Test Acc: 12.69%\n","[Central-SGDM] Epoch 37: Train Loss: 3.6299, Train Acc: 16.25% | Val Loss: 3.7623, Val Acc: 14.08% | Test Loss: 3.7173, Test Acc: 14.75%\n","[Central-SGDM] Epoch 38: Train Loss: 3.6614, Train Acc: 15.88% | Val Loss: 3.7635, Val Acc: 14.00% | Test Loss: 3.7411, Test Acc: 14.73%\n","[Central-SGDM] Epoch 39: Train Loss: 3.6326, Train Acc: 16.21% | Val Loss: 4.1168, Val Acc: 8.88% | Test Loss: 4.0993, Test Acc: 9.12%\n","[Central-SGDM] Epoch 40: Train Loss: 3.6155, Train Acc: 16.53% | Val Loss: 3.9253, Val Acc: 13.40% | Test Loss: 3.9184, Test Acc: 13.56%\n","[Central-SGDM] Epoch 41: Train Loss: 3.6301, Train Acc: 16.63% | Val Loss: 3.8018, Val Acc: 15.12% | Test Loss: 3.7556, Test Acc: 15.22%\n","[Central-SGDM] Epoch 42: Train Loss: 3.6138, Train Acc: 16.93% | Val Loss: 3.8374, Val Acc: 13.60% | Test Loss: 3.7583, Test Acc: 15.01%\n","[Central-SGDM] Epoch 43: Train Loss: 3.5914, Train Acc: 17.48% | Val Loss: 3.8059, Val Acc: 14.90% | Test Loss: 3.7996, Test Acc: 14.49%\n","[Central-SGDM] Epoch 44: Train Loss: 3.5689, Train Acc: 17.83% | Val Loss: 3.6308, Val Acc: 17.48% | Test Loss: 3.6340, Test Acc: 17.12%\n","[Central-SGDM] Epoch 45: Train Loss: 3.5530, Train Acc: 17.84% | Val Loss: 3.7258, Val Acc: 15.34% | Test Loss: 3.6952, Test Acc: 15.33%\n","[Central-SGDM] Epoch 46: Train Loss: 3.5208, Train Acc: 18.45% | Val Loss: 3.7392, Val Acc: 15.32% | Test Loss: 3.7267, Test Acc: 15.15%\n","[Central-SGDM] Epoch 47: Train Loss: 3.5032, Train Acc: 18.77% | Val Loss: 3.8283, Val Acc: 13.34% | Test Loss: 3.7910, Test Acc: 14.40%\n","[Central-SGDM] Epoch 48: Train Loss: 3.4704, Train Acc: 19.42% | Val Loss: 3.6476, Val Acc: 16.06% | Test Loss: 3.5797, Test Acc: 17.02%\n","[Central-SGDM] Epoch 49: Train Loss: 3.4549, Train Acc: 19.62% | Val Loss: 3.8004, Val Acc: 15.80% | Test Loss: 3.7794, Test Acc: 15.87%\n","[Central-SGDM] Epoch 50: Train Loss: 3.4675, Train Acc: 19.40% | Val Loss: 3.7360, Val Acc: 14.72% | Test Loss: 3.6654, Test Acc: 16.61%\n","[Central-SGDM] Epoch 51: Train Loss: 3.4659, Train Acc: 19.97% | Val Loss: 3.6673, Val Acc: 16.80% | Test Loss: 3.6512, Test Acc: 16.80%\n","[Central-SGDM] Epoch 52: Train Loss: 3.3890, Train Acc: 21.08% | Val Loss: 3.5529, Val Acc: 19.82% | Test Loss: 3.4998, Test Acc: 18.95%\n","[Central-SGDM] Epoch 53: Train Loss: 3.3964, Train Acc: 20.92% | Val Loss: 3.7019, Val Acc: 16.20% | Test Loss: 3.6652, Test Acc: 16.86%\n","[Central-SGDM] Epoch 54: Train Loss: 3.3562, Train Acc: 21.39% | Val Loss: 3.5299, Val Acc: 19.38% | Test Loss: 3.5132, Test Acc: 19.40%\n","[Central-SGDM] Epoch 55: Train Loss: 3.3464, Train Acc: 22.09% | Val Loss: 3.6266, Val Acc: 17.32% | Test Loss: 3.5479, Test Acc: 18.91%\n","[Central-SGDM] Epoch 56: Train Loss: 3.3318, Train Acc: 21.91% | Val Loss: 3.6391, Val Acc: 17.40% | Test Loss: 3.6140, Test Acc: 18.27%\n","[Central-SGDM] Epoch 57: Train Loss: 3.3175, Train Acc: 22.27% | Val Loss: 3.5802, Val Acc: 18.18% | Test Loss: 3.5389, Test Acc: 19.11%\n","[Central-SGDM] Epoch 58: Train Loss: 3.2862, Train Acc: 23.10% | Val Loss: 3.7996, Val Acc: 15.08% | Test Loss: 3.7729, Test Acc: 15.51%\n","[Central-SGDM] Epoch 59: Train Loss: 3.2383, Train Acc: 24.26% | Val Loss: 3.6286, Val Acc: 18.48% | Test Loss: 3.6004, Test Acc: 18.83%\n","[Central-SGDM] Epoch 60: Train Loss: 3.2263, Train Acc: 24.50% | Val Loss: 3.7474, Val Acc: 18.06% | Test Loss: 3.6848, Test Acc: 18.47%\n","[Central-SGDM] Epoch 61: Train Loss: 3.2075, Train Acc: 24.53% | Val Loss: 3.6538, Val Acc: 17.62% | Test Loss: 3.5998, Test Acc: 18.00%\n","[Central-SGDM] Epoch 62: Train Loss: 3.1686, Train Acc: 25.50% | Val Loss: 3.5703, Val Acc: 19.26% | Test Loss: 3.5495, Test Acc: 19.79%\n","[Central-SGDM] Epoch 63: Train Loss: 3.1244, Train Acc: 26.24% | Val Loss: 3.5894, Val Acc: 19.18% | Test Loss: 3.5364, Test Acc: 19.66%\n","[Central-SGDM] Epoch 64: Train Loss: 3.0869, Train Acc: 27.05% | Val Loss: 3.5705, Val Acc: 18.80% | Test Loss: 3.4986, Test Acc: 20.04%\n","[Central-SGDM] Epoch 65: Train Loss: 3.0559, Train Acc: 27.82% | Val Loss: 3.5516, Val Acc: 20.40% | Test Loss: 3.5041, Test Acc: 21.18%\n","[Central-SGDM] Epoch 66: Train Loss: 3.0182, Train Acc: 28.60% | Val Loss: 3.5249, Val Acc: 21.46% | Test Loss: 3.4641, Test Acc: 21.63%\n","[Central-SGDM] Epoch 67: Train Loss: 2.9924, Train Acc: 28.96% | Val Loss: 3.5565, Val Acc: 20.72% | Test Loss: 3.5704, Test Acc: 20.79%\n","[Central-SGDM] Epoch 68: Train Loss: 2.9728, Train Acc: 29.55% | Val Loss: 3.5983, Val Acc: 21.22% | Test Loss: 3.5634, Test Acc: 21.36%\n","[Central-SGDM] Epoch 69: Train Loss: 2.9315, Train Acc: 30.26% | Val Loss: 3.6152, Val Acc: 20.84% | Test Loss: 3.5866, Test Acc: 20.74%\n","[Central-SGDM] Epoch 70: Train Loss: 2.8985, Train Acc: 30.96% | Val Loss: 3.6423, Val Acc: 19.84% | Test Loss: 3.6232, Test Acc: 20.42%\n","[Central-SGDM] Epoch 71: Train Loss: 2.8280, Train Acc: 32.34% | Val Loss: 3.5879, Val Acc: 19.66% | Test Loss: 3.5646, Test Acc: 19.97%\n","[Central-SGDM] Epoch 72: Train Loss: 2.8079, Train Acc: 32.66% | Val Loss: 3.5046, Val Acc: 22.26% | Test Loss: 3.4689, Test Acc: 22.65%\n","[Central-SGDM] Epoch 73: Train Loss: 2.7489, Train Acc: 34.18% | Val Loss: 3.4425, Val Acc: 22.44% | Test Loss: 3.4418, Test Acc: 22.16%\n","[Central-SGDM] Epoch 74: Train Loss: 2.7181, Train Acc: 34.48% | Val Loss: 3.5502, Val Acc: 22.86% | Test Loss: 3.5362, Test Acc: 22.24%\n","[Central-SGDM] Epoch 75: Train Loss: 2.6615, Train Acc: 35.69% | Val Loss: 3.4457, Val Acc: 22.60% | Test Loss: 3.4346, Test Acc: 23.04%\n","[Central-SGDM] Epoch 76: Train Loss: 2.5893, Train Acc: 37.00% | Val Loss: 3.4656, Val Acc: 23.16% | Test Loss: 3.4283, Test Acc: 23.60%\n","[Central-SGDM] Epoch 77: Train Loss: 2.5685, Train Acc: 37.62% | Val Loss: 3.4975, Val Acc: 24.16% | Test Loss: 3.4896, Test Acc: 24.26%\n","[Central-SGDM] Epoch 78: Train Loss: 2.5181, Train Acc: 38.39% | Val Loss: 3.5827, Val Acc: 22.64% | Test Loss: 3.5555, Test Acc: 23.04%\n","[Central-SGDM] Epoch 79: Train Loss: 2.4516, Train Acc: 39.89% | Val Loss: 3.4491, Val Acc: 23.96% | Test Loss: 3.4066, Test Acc: 23.65%\n","[Central-SGDM] Epoch 80: Train Loss: 2.3922, Train Acc: 41.01% | Val Loss: 3.5280, Val Acc: 23.76% | Test Loss: 3.5387, Test Acc: 24.12%\n","[Central-SGDM] Epoch 81: Train Loss: 2.3563, Train Acc: 41.79% | Val Loss: 3.4931, Val Acc: 24.78% | Test Loss: 3.4604, Test Acc: 24.35%\n","[Central-SGDM] Epoch 82: Train Loss: 2.2835, Train Acc: 43.33% | Val Loss: 3.5609, Val Acc: 23.74% | Test Loss: 3.5281, Test Acc: 24.14%\n","[Central-SGDM] Epoch 83: Train Loss: 2.2296, Train Acc: 44.67% | Val Loss: 3.5727, Val Acc: 23.14% | Test Loss: 3.5505, Test Acc: 23.96%\n","[Central-SGDM] Epoch 84: Train Loss: 2.1809, Train Acc: 45.58% | Val Loss: 3.5361, Val Acc: 24.52% | Test Loss: 3.5476, Test Acc: 24.56%\n","[Central-SGDM] Epoch 85: Train Loss: 2.0963, Train Acc: 47.57% | Val Loss: 3.6347, Val Acc: 24.48% | Test Loss: 3.6260, Test Acc: 24.74%\n","[Central-SGDM] Epoch 86: Train Loss: 2.0406, Train Acc: 48.58% | Val Loss: 3.5572, Val Acc: 26.32% | Test Loss: 3.5627, Test Acc: 26.50%\n","[Central-SGDM] Epoch 87: Train Loss: 1.9691, Train Acc: 50.14% | Val Loss: 3.6417, Val Acc: 24.80% | Test Loss: 3.6525, Test Acc: 25.26%\n","[Central-SGDM] Epoch 88: Train Loss: 1.8941, Train Acc: 51.56% | Val Loss: 3.5997, Val Acc: 24.60% | Test Loss: 3.5738, Test Acc: 25.16%\n","[Central-SGDM] Epoch 89: Train Loss: 1.8739, Train Acc: 52.20% | Val Loss: 3.8276, Val Acc: 24.24% | Test Loss: 3.7983, Test Acc: 25.23%\n","[Central-SGDM] Epoch 90: Train Loss: 1.7570, Train Acc: 54.61% | Val Loss: 3.7457, Val Acc: 24.94% | Test Loss: 3.7083, Test Acc: 25.79%\n","[Central-SGDM] Epoch 91: Train Loss: 1.6996, Train Acc: 55.94% | Val Loss: 3.7305, Val Acc: 24.40% | Test Loss: 3.7131, Test Acc: 25.02%\n","[Central-SGDM] Epoch 92: Train Loss: 1.6309, Train Acc: 57.52% | Val Loss: 3.8977, Val Acc: 24.34% | Test Loss: 3.8895, Test Acc: 24.61%\n","[Central-SGDM] Epoch 93: Train Loss: 1.5483, Train Acc: 59.38% | Val Loss: 3.8345, Val Acc: 25.56% | Test Loss: 3.8364, Test Acc: 25.54%\n","[Central-SGDM] Epoch 94: Train Loss: 1.4657, Train Acc: 61.07% | Val Loss: 3.8515, Val Acc: 25.18% | Test Loss: 3.8400, Test Acc: 26.00%\n","[Central-SGDM] Epoch 95: Train Loss: 1.3965, Train Acc: 62.99% | Val Loss: 3.9511, Val Acc: 24.30% | Test Loss: 3.8676, Test Acc: 25.15%\n","[Central-SGDM] Epoch 96: Train Loss: 1.3628, Train Acc: 63.36% | Val Loss: 4.0245, Val Acc: 25.12% | Test Loss: 3.9978, Test Acc: 26.16%\n","[Central-SGDM] Epoch 97: Train Loss: 1.2951, Train Acc: 65.04% | Val Loss: 4.1364, Val Acc: 25.64% | Test Loss: 4.0432, Test Acc: 26.51%\n","[Central-SGDM] Epoch 98: Train Loss: 1.1658, Train Acc: 68.24% | Val Loss: 4.0813, Val Acc: 25.98% | Test Loss: 4.0279, Test Acc: 27.29%\n","[Central-SGDM] Epoch 99: Train Loss: 1.1170, Train Acc: 69.27% | Val Loss: 4.2388, Val Acc: 24.64% | Test Loss: 4.1569, Test Acc: 25.84%\n","[Central-SGDM] Epoch 100: Train Loss: 1.0774, Train Acc: 70.31% | Val Loss: 4.4722, Val Acc: 26.18% | Test Loss: 4.3472, Test Acc: 25.99%\n","[Central-SGDM] Epoch 101: Train Loss: 0.9642, Train Acc: 72.89% | Val Loss: 4.4192, Val Acc: 26.64% | Test Loss: 4.3522, Test Acc: 27.06%\n","[Central-SGDM] Epoch 102: Train Loss: 0.8616, Train Acc: 75.45% | Val Loss: 4.4770, Val Acc: 26.24% | Test Loss: 4.3229, Test Acc: 27.00%\n","[Central-SGDM] Epoch 103: Train Loss: 0.8091, Train Acc: 76.81% | Val Loss: 4.7243, Val Acc: 25.68% | Test Loss: 4.5473, Test Acc: 26.78%\n","[Central-SGDM] Epoch 104: Train Loss: 0.7373, Train Acc: 78.78% | Val Loss: 4.7185, Val Acc: 25.96% | Test Loss: 4.5997, Test Acc: 27.22%\n","[Central-SGDM] Epoch 105: Train Loss: 0.7821, Train Acc: 77.61% | Val Loss: 4.6662, Val Acc: 26.12% | Test Loss: 4.6631, Test Acc: 26.95%\n","[Central-SGDM] Epoch 106: Train Loss: 0.6492, Train Acc: 81.11% | Val Loss: 4.9233, Val Acc: 28.08% | Test Loss: 4.7979, Test Acc: 27.92%\n","[Central-SGDM] Epoch 107: Train Loss: 0.5335, Train Acc: 83.98% | Val Loss: 5.1321, Val Acc: 26.84% | Test Loss: 5.0648, Test Acc: 27.07%\n","[Central-SGDM] Epoch 108: Train Loss: 0.4275, Train Acc: 87.19% | Val Loss: 5.2719, Val Acc: 27.78% | Test Loss: 5.1629, Test Acc: 27.97%\n","[Central-SGDM] Epoch 109: Train Loss: 0.3315, Train Acc: 90.14% | Val Loss: 5.2222, Val Acc: 28.28% | Test Loss: 5.1864, Test Acc: 28.42%\n","[Central-SGDM] Epoch 110: Train Loss: 0.2982, Train Acc: 91.09% | Val Loss: 5.4175, Val Acc: 28.28% | Test Loss: 5.3702, Test Acc: 28.08%\n","[Central-SGDM] Epoch 111: Train Loss: 0.2371, Train Acc: 92.88% | Val Loss: 5.5074, Val Acc: 28.00% | Test Loss: 5.3595, Test Acc: 28.65%\n","[Central-SGDM] Epoch 112: Train Loss: 0.1482, Train Acc: 95.99% | Val Loss: 5.5232, Val Acc: 28.88% | Test Loss: 5.4168, Test Acc: 28.86%\n","[Central-SGDM] Epoch 113: Train Loss: 0.0926, Train Acc: 97.78% | Val Loss: 5.5056, Val Acc: 29.70% | Test Loss: 5.4286, Test Acc: 30.15%\n","[Central-SGDM] Epoch 114: Train Loss: 0.0464, Train Acc: 99.25% | Val Loss: 5.4863, Val Acc: 31.16% | Test Loss: 5.3656, Test Acc: 30.79%\n","[Central-SGDM] Epoch 115: Train Loss: 0.0293, Train Acc: 99.64% | Val Loss: 5.3915, Val Acc: 31.46% | Test Loss: 5.2541, Test Acc: 31.31%\n","[Central-SGDM] Epoch 116: Train Loss: 0.0245, Train Acc: 99.76% | Val Loss: 5.2900, Val Acc: 31.96% | Test Loss: 5.1593, Test Acc: 31.57%\n","[Central-SGDM] Epoch 117: Train Loss: 0.0225, Train Acc: 99.84% | Val Loss: 5.2309, Val Acc: 32.22% | Test Loss: 5.0921, Test Acc: 31.46%\n","[Central-SGDM] Epoch 118: Train Loss: 0.0222, Train Acc: 99.86% | Val Loss: 5.1525, Val Acc: 32.36% | Test Loss: 5.0056, Test Acc: 32.00%\n","[Central-SGDM] Epoch 119: Train Loss: 0.0225, Train Acc: 99.89% | Val Loss: 5.0964, Val Acc: 32.46% | Test Loss: 4.9397, Test Acc: 32.18%\n","[Central-SGDM] Epoch 120: Train Loss: 0.0229, Train Acc: 99.91% | Val Loss: 5.0746, Val Acc: 32.60% | Test Loss: 4.9226, Test Acc: 32.50%\n","[Central-SGDM] Epoch 121: Train Loss: 0.0225, Train Acc: 99.92% | Val Loss: 5.0300, Val Acc: 32.54% | Test Loss: 4.8736, Test Acc: 32.64%\n","[Central-SGDM] Epoch 122: Train Loss: 0.0225, Train Acc: 99.93% | Val Loss: 5.0177, Val Acc: 32.72% | Test Loss: 4.8577, Test Acc: 32.90%\n","[Central-SGDM] Epoch 123: Train Loss: 0.0226, Train Acc: 99.92% | Val Loss: 4.9892, Val Acc: 32.80% | Test Loss: 4.8302, Test Acc: 32.81%\n","[Central-SGDM] Epoch 124: Train Loss: 0.0224, Train Acc: 99.95% | Val Loss: 5.0086, Val Acc: 32.58% | Test Loss: 4.8463, Test Acc: 32.96%\n","[Central-SGDM] Epoch 125: Train Loss: 0.0222, Train Acc: 99.95% | Val Loss: 4.9951, Val Acc: 32.64% | Test Loss: 4.8245, Test Acc: 33.17%\n","[Central-SGDM] Epoch 126: Train Loss: 0.0219, Train Acc: 99.96% | Val Loss: 4.9793, Val Acc: 32.74% | Test Loss: 4.8185, Test Acc: 33.13%\n","[Central-SGDM] Epoch 127: Train Loss: 0.0217, Train Acc: 99.96% | Val Loss: 5.0019, Val Acc: 32.76% | Test Loss: 4.8335, Test Acc: 33.23%\n","[Central-SGDM] Epoch 128: Train Loss: 0.0215, Train Acc: 99.96% | Val Loss: 5.0043, Val Acc: 32.68% | Test Loss: 4.8232, Test Acc: 33.27%\n","[Central-SGDM] Epoch 129: Train Loss: 0.0213, Train Acc: 99.97% | Val Loss: 4.9940, Val Acc: 32.70% | Test Loss: 4.8166, Test Acc: 33.43%\n","[Central-SGDM] Epoch 130: Train Loss: 0.0212, Train Acc: 99.97% | Val Loss: 4.9893, Val Acc: 32.80% | Test Loss: 4.8093, Test Acc: 33.37%\n","[Central-SGDM] Epoch 131: Train Loss: 0.0210, Train Acc: 99.97% | Val Loss: 4.9995, Val Acc: 32.80% | Test Loss: 4.8145, Test Acc: 33.55%\n","[Central-SGDM] Epoch 132: Train Loss: 0.0209, Train Acc: 99.97% | Val Loss: 4.9881, Val Acc: 32.76% | Test Loss: 4.8084, Test Acc: 33.70%\n","[Central-SGDM] Epoch 133: Train Loss: 0.0207, Train Acc: 99.97% | Val Loss: 4.9985, Val Acc: 32.74% | Test Loss: 4.8070, Test Acc: 33.51%\n","[Central-SGDM] Epoch 134: Train Loss: 0.0206, Train Acc: 99.97% | Val Loss: 4.9923, Val Acc: 32.78% | Test Loss: 4.8042, Test Acc: 33.58%\n","[Central-SGDM] Epoch 135: Train Loss: 0.0204, Train Acc: 99.97% | Val Loss: 4.9939, Val Acc: 32.66% | Test Loss: 4.8044, Test Acc: 33.63%\n","[Central-SGDM] Epoch 136: Train Loss: 0.0202, Train Acc: 99.97% | Val Loss: 4.9885, Val Acc: 32.80% | Test Loss: 4.8012, Test Acc: 33.79%\n","[Central-SGDM] Epoch 137: Train Loss: 0.0201, Train Acc: 99.98% | Val Loss: 4.9904, Val Acc: 32.86% | Test Loss: 4.7988, Test Acc: 33.78%\n","[Central-SGDM] Epoch 138: Train Loss: 0.0200, Train Acc: 99.98% | Val Loss: 4.9917, Val Acc: 32.86% | Test Loss: 4.8012, Test Acc: 33.85%\n","[Central-SGDM] Epoch 139: Train Loss: 0.0199, Train Acc: 99.98% | Val Loss: 5.0004, Val Acc: 32.80% | Test Loss: 4.8060, Test Acc: 33.84%\n","[Central-SGDM] Epoch 140: Train Loss: 0.0197, Train Acc: 99.98% | Val Loss: 4.9957, Val Acc: 32.98% | Test Loss: 4.8019, Test Acc: 33.78%\n","[Central-SGDM] Epoch 141: Train Loss: 0.0196, Train Acc: 99.98% | Val Loss: 4.9996, Val Acc: 32.80% | Test Loss: 4.8041, Test Acc: 33.86%\n","[Central-SGDM] Epoch 142: Train Loss: 0.0196, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.78% | Test Loss: 4.8023, Test Acc: 33.86%\n","[Central-SGDM] Epoch 143: Train Loss: 0.0195, Train Acc: 99.98% | Val Loss: 4.9968, Val Acc: 32.86% | Test Loss: 4.8015, Test Acc: 33.81%\n","[Central-SGDM] Epoch 144: Train Loss: 0.0194, Train Acc: 99.98% | Val Loss: 4.9951, Val Acc: 32.86% | Test Loss: 4.8000, Test Acc: 33.86%\n","[Central-SGDM] Epoch 145: Train Loss: 0.0193, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.84% | Test Loss: 4.8023, Test Acc: 33.84%\n","[Central-SGDM] Epoch 146: Train Loss: 0.0193, Train Acc: 99.98% | Val Loss: 4.9970, Val Acc: 32.84% | Test Loss: 4.8016, Test Acc: 33.80%\n","[Central-SGDM] Epoch 147: Train Loss: 0.0192, Train Acc: 99.98% | Val Loss: 4.9977, Val Acc: 32.84% | Test Loss: 4.8019, Test Acc: 33.85%\n","[Central-SGDM] Epoch 148: Train Loss: 0.0192, Train Acc: 99.98% | Val Loss: 4.9982, Val Acc: 32.86% | Test Loss: 4.8025, Test Acc: 33.82%\n","[Central-SGDM] Epoch 149: Train Loss: 0.0191, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.88% | Test Loss: 4.8023, Test Acc: 33.83%\n","[Central-SGDM] Epoch 150: Train Loss: 0.0191, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.88% | Test Loss: 4.8023, Test Acc: 33.82%\n","[Central-AdamW] Epoch 1: Train Loss: 3.7988, Train Acc: 11.72% | Val Loss: 3.4045, Val Acc: 18.18% | Test Loss: 3.3868, Test Acc: 18.09%\n","[Central-AdamW] Epoch 2: Train Loss: 3.1115, Train Acc: 23.34% | Val Loss: 3.0404, Val Acc: 25.68% | Test Loss: 3.0058, Test Acc: 26.03%\n","[Central-AdamW] Epoch 3: Train Loss: 2.7834, Train Acc: 29.74% | Val Loss: 2.8631, Val Acc: 28.26% | Test Loss: 2.8617, Test Acc: 28.97%\n","[Central-AdamW] Epoch 4: Train Loss: 2.5607, Train Acc: 33.92% | Val Loss: 2.7034, Val Acc: 32.58% | Test Loss: 2.7036, Test Acc: 32.39%\n","[Central-AdamW] Epoch 5: Train Loss: 2.3839, Train Acc: 37.62% | Val Loss: 2.6680, Val Acc: 34.66% | Test Loss: 2.6736, Test Acc: 33.63%\n","[Central-AdamW] Epoch 6: Train Loss: 2.2517, Train Acc: 40.42% | Val Loss: 2.6041, Val Acc: 35.04% | Test Loss: 2.6087, Test Acc: 34.56%\n","[Central-AdamW] Epoch 7: Train Loss: 2.1150, Train Acc: 43.40% | Val Loss: 2.6755, Val Acc: 34.74% | Test Loss: 2.6952, Test Acc: 34.20%\n","[Central-AdamW] Epoch 8: Train Loss: 2.0048, Train Acc: 45.71% | Val Loss: 2.6764, Val Acc: 35.22% | Test Loss: 2.6596, Test Acc: 35.76%\n","[Central-AdamW] Epoch 9: Train Loss: 1.9007, Train Acc: 47.92% | Val Loss: 2.6322, Val Acc: 35.94% | Test Loss: 2.6332, Test Acc: 35.46%\n","[Central-AdamW] Epoch 10: Train Loss: 1.7964, Train Acc: 50.11% | Val Loss: 2.6881, Val Acc: 36.00% | Test Loss: 2.6922, Test Acc: 35.48%\n","[Central-AdamW] Epoch 11: Train Loss: 1.7102, Train Acc: 52.12% | Val Loss: 2.7407, Val Acc: 36.54% | Test Loss: 2.7513, Test Acc: 35.60%\n","[Central-AdamW] Epoch 12: Train Loss: 1.6171, Train Acc: 54.24% | Val Loss: 2.8486, Val Acc: 36.14% | Test Loss: 2.8672, Test Acc: 35.31%\n","[Central-AdamW] Epoch 13: Train Loss: 1.5441, Train Acc: 56.18% | Val Loss: 2.8412, Val Acc: 35.18% | Test Loss: 2.8476, Test Acc: 35.69%\n","[Central-AdamW] Epoch 14: Train Loss: 1.4526, Train Acc: 58.40% | Val Loss: 3.0192, Val Acc: 34.36% | Test Loss: 3.0438, Test Acc: 34.64%\n","[Central-AdamW] Epoch 15: Train Loss: 1.3934, Train Acc: 59.78% | Val Loss: 3.0408, Val Acc: 34.74% | Test Loss: 3.0957, Test Acc: 35.20%\n","[Central-AdamW] Epoch 16: Train Loss: 1.3266, Train Acc: 61.30% | Val Loss: 3.0962, Val Acc: 34.56% | Test Loss: 3.1347, Test Acc: 34.52%\n","[Central-AdamW] Epoch 17: Train Loss: 1.2479, Train Acc: 63.28% | Val Loss: 3.2918, Val Acc: 34.74% | Test Loss: 3.3268, Test Acc: 33.56%\n","[Central-AdamW] Epoch 18: Train Loss: 1.1980, Train Acc: 64.60% | Val Loss: 3.3785, Val Acc: 33.94% | Test Loss: 3.4165, Test Acc: 34.07%\n","[Central-AdamW] Epoch 19: Train Loss: 1.1322, Train Acc: 66.11% | Val Loss: 3.4535, Val Acc: 34.46% | Test Loss: 3.4714, Test Acc: 34.61%\n","[Central-AdamW] Epoch 20: Train Loss: 1.0731, Train Acc: 67.93% | Val Loss: 3.5957, Val Acc: 34.54% | Test Loss: 3.6130, Test Acc: 34.30%\n","[Central-AdamW] Epoch 21: Train Loss: 1.0273, Train Acc: 68.82% | Val Loss: 3.6681, Val Acc: 33.64% | Test Loss: 3.7272, Test Acc: 33.26%\n","[Central-AdamW] Epoch 22: Train Loss: 0.9687, Train Acc: 70.33% | Val Loss: 4.0210, Val Acc: 33.52% | Test Loss: 4.0473, Test Acc: 33.14%\n","[Central-AdamW] Epoch 23: Train Loss: 0.9320, Train Acc: 71.20% | Val Loss: 3.9831, Val Acc: 32.94% | Test Loss: 3.9769, Test Acc: 33.27%\n","[Central-AdamW] Epoch 24: Train Loss: 0.8825, Train Acc: 72.86% | Val Loss: 4.1306, Val Acc: 32.82% | Test Loss: 4.1160, Test Acc: 32.73%\n","[Central-AdamW] Epoch 25: Train Loss: 0.8376, Train Acc: 74.06% | Val Loss: 4.3240, Val Acc: 33.16% | Test Loss: 4.3162, Test Acc: 33.10%\n","[Central-AdamW] Epoch 26: Train Loss: 0.7928, Train Acc: 75.15% | Val Loss: 4.3974, Val Acc: 32.90% | Test Loss: 4.4287, Test Acc: 32.88%\n","[Central-AdamW] Epoch 27: Train Loss: 0.7631, Train Acc: 75.98% | Val Loss: 4.5210, Val Acc: 32.84% | Test Loss: 4.5318, Test Acc: 32.64%\n","[Central-AdamW] Epoch 28: Train Loss: 0.7436, Train Acc: 76.64% | Val Loss: 4.8024, Val Acc: 31.82% | Test Loss: 4.7429, Test Acc: 32.88%\n","[Central-AdamW] Epoch 29: Train Loss: 0.6873, Train Acc: 78.15% | Val Loss: 4.8026, Val Acc: 32.30% | Test Loss: 4.8546, Test Acc: 32.24%\n","[Central-AdamW] Epoch 30: Train Loss: 0.6715, Train Acc: 78.61% | Val Loss: 5.0715, Val Acc: 31.86% | Test Loss: 4.9986, Test Acc: 32.93%\n","[Central-AdamW] Epoch 31: Train Loss: 0.6349, Train Acc: 79.69% | Val Loss: 5.0869, Val Acc: 31.68% | Test Loss: 5.1028, Test Acc: 32.16%\n","[Central-AdamW] Epoch 32: Train Loss: 0.6171, Train Acc: 80.30% | Val Loss: 5.2384, Val Acc: 31.76% | Test Loss: 5.2216, Test Acc: 31.82%\n","[Central-AdamW] Epoch 33: Train Loss: 0.5841, Train Acc: 80.98% | Val Loss: 5.3768, Val Acc: 31.58% | Test Loss: 5.3765, Test Acc: 32.13%\n","[Central-AdamW] Epoch 34: Train Loss: 0.5456, Train Acc: 82.23% | Val Loss: 5.6640, Val Acc: 31.74% | Test Loss: 5.6405, Test Acc: 32.22%\n","[Central-AdamW] Epoch 35: Train Loss: 0.5499, Train Acc: 82.25% | Val Loss: 5.7159, Val Acc: 31.84% | Test Loss: 5.7049, Test Acc: 31.85%\n","[Central-AdamW] Epoch 36: Train Loss: 0.5202, Train Acc: 83.12% | Val Loss: 5.7940, Val Acc: 31.42% | Test Loss: 5.7535, Test Acc: 31.93%\n","[Central-AdamW] Epoch 37: Train Loss: 0.4981, Train Acc: 83.70% | Val Loss: 5.9017, Val Acc: 31.94% | Test Loss: 6.0004, Test Acc: 31.88%\n","[Central-AdamW] Epoch 38: Train Loss: 0.4714, Train Acc: 84.60% | Val Loss: 6.2608, Val Acc: 32.36% | Test Loss: 6.2122, Test Acc: 31.49%\n","[Central-AdamW] Epoch 39: Train Loss: 0.4479, Train Acc: 85.53% | Val Loss: 6.2895, Val Acc: 31.72% | Test Loss: 6.3677, Test Acc: 31.68%\n","[Central-AdamW] Epoch 40: Train Loss: 0.4429, Train Acc: 85.61% | Val Loss: 6.3582, Val Acc: 31.88% | Test Loss: 6.4389, Test Acc: 31.62%\n","[Central-AdamW] Epoch 41: Train Loss: 0.4438, Train Acc: 85.46% | Val Loss: 6.4065, Val Acc: 31.56% | Test Loss: 6.4385, Test Acc: 31.43%\n","[Central-AdamW] Epoch 42: Train Loss: 0.3790, Train Acc: 87.44% | Val Loss: 6.5612, Val Acc: 32.08% | Test Loss: 6.5617, Test Acc: 31.37%\n","[Central-AdamW] Epoch 43: Train Loss: 0.3713, Train Acc: 87.80% | Val Loss: 6.8851, Val Acc: 32.14% | Test Loss: 6.8061, Test Acc: 31.16%\n","[Central-AdamW] Epoch 44: Train Loss: 0.4078, Train Acc: 86.75% | Val Loss: 6.8292, Val Acc: 31.70% | Test Loss: 6.7225, Test Acc: 31.41%\n","[Central-AdamW] Epoch 45: Train Loss: 0.3554, Train Acc: 88.39% | Val Loss: 6.9470, Val Acc: 31.44% | Test Loss: 6.9365, Test Acc: 31.45%\n","[Central-AdamW] Epoch 46: Train Loss: 0.3464, Train Acc: 88.59% | Val Loss: 7.0519, Val Acc: 31.34% | Test Loss: 7.0731, Test Acc: 31.33%\n","[Central-AdamW] Epoch 47: Train Loss: 0.3322, Train Acc: 89.05% | Val Loss: 7.2191, Val Acc: 31.96% | Test Loss: 7.2585, Test Acc: 31.51%\n","[Central-AdamW] Epoch 48: Train Loss: 0.3122, Train Acc: 89.63% | Val Loss: 7.4024, Val Acc: 31.78% | Test Loss: 7.4425, Test Acc: 31.41%\n","[Central-AdamW] Epoch 49: Train Loss: 0.3038, Train Acc: 90.00% | Val Loss: 7.5506, Val Acc: 31.38% | Test Loss: 7.5942, Test Acc: 31.00%\n","[Central-AdamW] Epoch 50: Train Loss: 0.2861, Train Acc: 90.49% | Val Loss: 7.7486, Val Acc: 31.54% | Test Loss: 7.6781, Test Acc: 31.48%\n","[Central-AdamW] Epoch 51: Train Loss: 0.2715, Train Acc: 90.89% | Val Loss: 7.8077, Val Acc: 31.10% | Test Loss: 7.8402, Test Acc: 31.50%\n","[Central-AdamW] Epoch 52: Train Loss: 0.2830, Train Acc: 90.74% | Val Loss: 7.8394, Val Acc: 31.84% | Test Loss: 7.8423, Test Acc: 31.50%\n","[Central-AdamW] Epoch 53: Train Loss: 0.2768, Train Acc: 90.83% | Val Loss: 7.9090, Val Acc: 31.88% | Test Loss: 7.9589, Test Acc: 31.53%\n","[Central-AdamW] Epoch 54: Train Loss: 0.2522, Train Acc: 91.77% | Val Loss: 8.0043, Val Acc: 31.70% | Test Loss: 7.9834, Test Acc: 31.19%\n","[Central-AdamW] Epoch 55: Train Loss: 0.2395, Train Acc: 91.95% | Val Loss: 8.2984, Val Acc: 31.74% | Test Loss: 8.3255, Test Acc: 31.65%\n","[Central-AdamW] Epoch 56: Train Loss: 0.2328, Train Acc: 92.21% | Val Loss: 8.3401, Val Acc: 30.74% | Test Loss: 8.1920, Test Acc: 30.81%\n","[Central-AdamW] Epoch 57: Train Loss: 0.2124, Train Acc: 92.92% | Val Loss: 8.4499, Val Acc: 31.40% | Test Loss: 8.4308, Test Acc: 31.56%\n","[Central-AdamW] Epoch 58: Train Loss: 0.2170, Train Acc: 92.77% | Val Loss: 8.5226, Val Acc: 31.14% | Test Loss: 8.5088, Test Acc: 30.91%\n","[Central-AdamW] Epoch 59: Train Loss: 0.2181, Train Acc: 92.89% | Val Loss: 8.5951, Val Acc: 31.56% | Test Loss: 8.6478, Test Acc: 31.34%\n","[Central-AdamW] Epoch 60: Train Loss: 0.2049, Train Acc: 93.27% | Val Loss: 8.4995, Val Acc: 31.16% | Test Loss: 8.4368, Test Acc: 31.44%\n","[Central-AdamW] Epoch 61: Train Loss: 0.2061, Train Acc: 93.08% | Val Loss: 8.6606, Val Acc: 31.08% | Test Loss: 8.6321, Test Acc: 31.35%\n","[Central-AdamW] Epoch 62: Train Loss: 0.1749, Train Acc: 94.24% | Val Loss: 8.7839, Val Acc: 31.26% | Test Loss: 8.8082, Test Acc: 31.41%\n","[Central-AdamW] Epoch 63: Train Loss: 0.1600, Train Acc: 94.71% | Val Loss: 8.8717, Val Acc: 30.88% | Test Loss: 8.7824, Test Acc: 31.70%\n","[Central-AdamW] Epoch 64: Train Loss: 0.1901, Train Acc: 93.73% | Val Loss: 9.2491, Val Acc: 30.98% | Test Loss: 9.2797, Test Acc: 31.11%\n","[Central-AdamW] Epoch 65: Train Loss: 0.1547, Train Acc: 94.85% | Val Loss: 9.0363, Val Acc: 31.14% | Test Loss: 9.0051, Test Acc: 31.38%\n","[Central-AdamW] Epoch 66: Train Loss: 0.1437, Train Acc: 95.34% | Val Loss: 9.2599, Val Acc: 31.44% | Test Loss: 9.3077, Test Acc: 31.05%\n","[Central-AdamW] Epoch 67: Train Loss: 0.1396, Train Acc: 95.46% | Val Loss: 9.5850, Val Acc: 31.30% | Test Loss: 9.5218, Test Acc: 31.37%\n","[Central-AdamW] Epoch 68: Train Loss: 0.1611, Train Acc: 94.76% | Val Loss: 9.2653, Val Acc: 31.12% | Test Loss: 9.3112, Test Acc: 30.93%\n","[Central-AdamW] Epoch 69: Train Loss: 0.1395, Train Acc: 95.46% | Val Loss: 9.5563, Val Acc: 31.20% | Test Loss: 9.5116, Test Acc: 31.22%\n","[Central-AdamW] Epoch 70: Train Loss: 0.1100, Train Acc: 96.57% | Val Loss: 9.5203, Val Acc: 31.32% | Test Loss: 9.4172, Test Acc: 31.73%\n","[Central-AdamW] Epoch 71: Train Loss: 0.1345, Train Acc: 95.57% | Val Loss: 9.5828, Val Acc: 31.52% | Test Loss: 9.5295, Test Acc: 31.76%\n","[Central-AdamW] Epoch 72: Train Loss: 0.0979, Train Acc: 96.90% | Val Loss: 9.8578, Val Acc: 31.60% | Test Loss: 9.7428, Test Acc: 31.44%\n","[Central-AdamW] Epoch 73: Train Loss: 0.1273, Train Acc: 95.78% | Val Loss: 9.5904, Val Acc: 31.28% | Test Loss: 9.5259, Test Acc: 31.60%\n","[Central-AdamW] Epoch 74: Train Loss: 0.1023, Train Acc: 96.71% | Val Loss: 9.8813, Val Acc: 31.20% | Test Loss: 9.9069, Test Acc: 31.49%\n","[Central-AdamW] Epoch 75: Train Loss: 0.0941, Train Acc: 97.06% | Val Loss: 10.0777, Val Acc: 31.40% | Test Loss: 10.0499, Test Acc: 31.40%\n","[Central-AdamW] Epoch 76: Train Loss: 0.0884, Train Acc: 97.20% | Val Loss: 9.9277, Val Acc: 31.66% | Test Loss: 9.8342, Test Acc: 31.51%\n","[Central-AdamW] Epoch 77: Train Loss: 0.0901, Train Acc: 97.21% | Val Loss: 10.2232, Val Acc: 30.66% | Test Loss: 10.1293, Test Acc: 31.53%\n","[Central-AdamW] Epoch 78: Train Loss: 0.1031, Train Acc: 96.76% | Val Loss: 9.9196, Val Acc: 31.16% | Test Loss: 9.9642, Test Acc: 31.18%\n","[Central-AdamW] Epoch 79: Train Loss: 0.0632, Train Acc: 98.12% | Val Loss: 10.2051, Val Acc: 31.58% | Test Loss: 10.0627, Test Acc: 31.74%\n","[Central-AdamW] Epoch 80: Train Loss: 0.0852, Train Acc: 97.18% | Val Loss: 10.0854, Val Acc: 30.62% | Test Loss: 9.9852, Test Acc: 31.05%\n","[Central-AdamW] Epoch 81: Train Loss: 0.0725, Train Acc: 97.71% | Val Loss: 10.4181, Val Acc: 31.60% | Test Loss: 10.2677, Test Acc: 31.13%\n","[Central-AdamW] Epoch 82: Train Loss: 0.0597, Train Acc: 98.19% | Val Loss: 10.6525, Val Acc: 31.02% | Test Loss: 10.5616, Test Acc: 31.66%\n","[Central-AdamW] Epoch 83: Train Loss: 0.0655, Train Acc: 97.95% | Val Loss: 10.5693, Val Acc: 31.44% | Test Loss: 10.3949, Test Acc: 31.47%\n","[Central-AdamW] Epoch 84: Train Loss: 0.0494, Train Acc: 98.58% | Val Loss: 10.5167, Val Acc: 32.18% | Test Loss: 10.4579, Test Acc: 31.81%\n","[Central-AdamW] Epoch 85: Train Loss: 0.0529, Train Acc: 98.44% | Val Loss: 10.6403, Val Acc: 31.44% | Test Loss: 10.5546, Test Acc: 31.63%\n","[Central-AdamW] Epoch 86: Train Loss: 0.0609, Train Acc: 98.16% | Val Loss: 10.8253, Val Acc: 31.34% | Test Loss: 10.7554, Test Acc: 31.66%\n","[Central-AdamW] Epoch 87: Train Loss: 0.0538, Train Acc: 98.40% | Val Loss: 10.7809, Val Acc: 31.68% | Test Loss: 10.6048, Test Acc: 31.77%\n","[Central-AdamW] Epoch 88: Train Loss: 0.0457, Train Acc: 98.68% | Val Loss: 10.8732, Val Acc: 32.18% | Test Loss: 10.7028, Test Acc: 32.38%\n","[Central-AdamW] Epoch 89: Train Loss: 0.0286, Train Acc: 99.27% | Val Loss: 10.7381, Val Acc: 31.52% | Test Loss: 10.7013, Test Acc: 31.78%\n","[Central-AdamW] Epoch 90: Train Loss: 0.0450, Train Acc: 98.64% | Val Loss: 10.8994, Val Acc: 31.28% | Test Loss: 10.8315, Test Acc: 31.34%\n","[Central-AdamW] Epoch 91: Train Loss: 0.0377, Train Acc: 98.94% | Val Loss: 11.1137, Val Acc: 31.56% | Test Loss: 10.9772, Test Acc: 31.92%\n","[Central-AdamW] Epoch 92: Train Loss: 0.0405, Train Acc: 98.84% | Val Loss: 10.9566, Val Acc: 31.58% | Test Loss: 10.8129, Test Acc: 31.40%\n","[Central-AdamW] Epoch 93: Train Loss: 0.0176, Train Acc: 99.63% | Val Loss: 11.0722, Val Acc: 31.56% | Test Loss: 10.9775, Test Acc: 31.76%\n","[Central-AdamW] Epoch 94: Train Loss: 0.0171, Train Acc: 99.68% | Val Loss: 11.1817, Val Acc: 31.56% | Test Loss: 11.1053, Test Acc: 31.83%\n","[Central-AdamW] Epoch 95: Train Loss: 0.0514, Train Acc: 98.38% | Val Loss: 11.1932, Val Acc: 31.98% | Test Loss: 11.0396, Test Acc: 31.33%\n","[Central-AdamW] Epoch 96: Train Loss: 0.0282, Train Acc: 99.26% | Val Loss: 11.2992, Val Acc: 32.00% | Test Loss: 11.1433, Test Acc: 32.16%\n","[Central-AdamW] Epoch 97: Train Loss: 0.0117, Train Acc: 99.80% | Val Loss: 11.3190, Val Acc: 31.56% | Test Loss: 11.1499, Test Acc: 31.98%\n","[Central-AdamW] Epoch 98: Train Loss: 0.0122, Train Acc: 99.76% | Val Loss: 11.3072, Val Acc: 31.84% | Test Loss: 11.1744, Test Acc: 32.16%\n","[Central-AdamW] Epoch 99: Train Loss: 0.0069, Train Acc: 99.90% | Val Loss: 11.3846, Val Acc: 31.36% | Test Loss: 11.2663, Test Acc: 31.82%\n","[Central-AdamW] Epoch 100: Train Loss: 0.0483, Train Acc: 98.48% | Val Loss: 11.3958, Val Acc: 32.02% | Test Loss: 11.2739, Test Acc: 32.04%\n","[Central-AdamW] Epoch 101: Train Loss: 0.0165, Train Acc: 99.62% | Val Loss: 11.3938, Val Acc: 32.04% | Test Loss: 11.2696, Test Acc: 31.93%\n","[Central-AdamW] Epoch 102: Train Loss: 0.0065, Train Acc: 99.90% | Val Loss: 11.4606, Val Acc: 32.02% | Test Loss: 11.2784, Test Acc: 32.27%\n","[Central-AdamW] Epoch 103: Train Loss: 0.0048, Train Acc: 99.93% | Val Loss: 11.5450, Val Acc: 31.86% | Test Loss: 11.3600, Test Acc: 32.42%\n","[Central-AdamW] Epoch 104: Train Loss: 0.0050, Train Acc: 99.94% | Val Loss: 11.5869, Val Acc: 31.52% | Test Loss: 11.4344, Test Acc: 31.83%\n","[Central-AdamW] Epoch 105: Train Loss: 0.0360, Train Acc: 98.97% | Val Loss: 11.4176, Val Acc: 31.70% | Test Loss: 11.3187, Test Acc: 31.77%\n","[Central-AdamW] Epoch 106: Train Loss: 0.0101, Train Acc: 99.79% | Val Loss: 11.4946, Val Acc: 31.82% | Test Loss: 11.3322, Test Acc: 31.81%\n","[Central-AdamW] Epoch 107: Train Loss: 0.0036, Train Acc: 99.97% | Val Loss: 11.5155, Val Acc: 31.96% | Test Loss: 11.3754, Test Acc: 32.19%\n","[Central-AdamW] Epoch 108: Train Loss: 0.0028, Train Acc: 99.97% | Val Loss: 11.5474, Val Acc: 32.48% | Test Loss: 11.4370, Test Acc: 32.20%\n","[Central-AdamW] Epoch 109: Train Loss: 0.0041, Train Acc: 99.94% | Val Loss: 11.6296, Val Acc: 31.64% | Test Loss: 11.5242, Test Acc: 32.33%\n","[Central-AdamW] Epoch 110: Train Loss: 0.0047, Train Acc: 99.95% | Val Loss: 11.6649, Val Acc: 31.98% | Test Loss: 11.5508, Test Acc: 32.33%\n","[Central-AdamW] Epoch 111: Train Loss: 0.0051, Train Acc: 99.92% | Val Loss: 11.9737, Val Acc: 32.22% | Test Loss: 11.8773, Test Acc: 32.05%\n","[Central-AdamW] Epoch 112: Train Loss: 0.0150, Train Acc: 99.66% | Val Loss: 11.6590, Val Acc: 32.32% | Test Loss: 11.4992, Test Acc: 32.07%\n","[Central-AdamW] Epoch 113: Train Loss: 0.0032, Train Acc: 99.96% | Val Loss: 11.7314, Val Acc: 32.40% | Test Loss: 11.5628, Test Acc: 32.38%\n","[Central-AdamW] Epoch 114: Train Loss: 0.0020, Train Acc: 99.98% | Val Loss: 11.8056, Val Acc: 31.98% | Test Loss: 11.6301, Test Acc: 32.50%\n","[Central-AdamW] Epoch 115: Train Loss: 0.0038, Train Acc: 99.94% | Val Loss: 11.9041, Val Acc: 32.52% | Test Loss: 11.7656, Test Acc: 32.36%\n","[Central-AdamW] Epoch 116: Train Loss: 0.0029, Train Acc: 99.96% | Val Loss: 11.9011, Val Acc: 32.10% | Test Loss: 11.7347, Test Acc: 32.35%\n","[Central-AdamW] Epoch 117: Train Loss: 0.0054, Train Acc: 99.88% | Val Loss: 11.8616, Val Acc: 32.32% | Test Loss: 11.7368, Test Acc: 31.79%\n","[Central-AdamW] Epoch 118: Train Loss: 0.0030, Train Acc: 99.96% | Val Loss: 11.9185, Val Acc: 32.28% | Test Loss: 11.8021, Test Acc: 32.24%\n","[Central-AdamW] Epoch 119: Train Loss: 0.0022, Train Acc: 99.96% | Val Loss: 11.9260, Val Acc: 32.56% | Test Loss: 11.8042, Test Acc: 32.48%\n","[Central-AdamW] Epoch 120: Train Loss: 0.0021, Train Acc: 99.96% | Val Loss: 11.9377, Val Acc: 32.30% | Test Loss: 11.7752, Test Acc: 32.33%\n","[Central-AdamW] Epoch 121: Train Loss: 0.0024, Train Acc: 99.96% | Val Loss: 12.1018, Val Acc: 32.14% | Test Loss: 11.9623, Test Acc: 32.58%\n","[Central-AdamW] Epoch 122: Train Loss: 0.0026, Train Acc: 99.96% | Val Loss: 12.0770, Val Acc: 32.42% | Test Loss: 11.9406, Test Acc: 32.54%\n","[Central-AdamW] Epoch 123: Train Loss: 0.0017, Train Acc: 99.97% | Val Loss: 12.1251, Val Acc: 32.38% | Test Loss: 12.0140, Test Acc: 32.32%\n","[Central-AdamW] Epoch 124: Train Loss: 0.0021, Train Acc: 99.96% | Val Loss: 12.2254, Val Acc: 32.52% | Test Loss: 12.0976, Test Acc: 32.43%\n","[Central-AdamW] Epoch 125: Train Loss: 0.0018, Train Acc: 99.96% | Val Loss: 12.2644, Val Acc: 32.56% | Test Loss: 12.1186, Test Acc: 32.59%\n","[Central-AdamW] Epoch 126: Train Loss: 0.0016, Train Acc: 99.96% | Val Loss: 12.3126, Val Acc: 32.22% | Test Loss: 12.1494, Test Acc: 32.64%\n","[Central-AdamW] Epoch 127: Train Loss: 0.0012, Train Acc: 99.97% | Val Loss: 12.4586, Val Acc: 32.48% | Test Loss: 12.3199, Test Acc: 32.62%\n","[Central-AdamW] Epoch 128: Train Loss: 0.0011, Train Acc: 99.98% | Val Loss: 12.4144, Val Acc: 32.38% | Test Loss: 12.2593, Test Acc: 32.80%\n","[Central-AdamW] Epoch 129: Train Loss: 0.0009, Train Acc: 99.98% | Val Loss: 12.5076, Val Acc: 32.38% | Test Loss: 12.3386, Test Acc: 32.69%\n","[Central-AdamW] Epoch 130: Train Loss: 0.0012, Train Acc: 99.98% | Val Loss: 12.5765, Val Acc: 32.84% | Test Loss: 12.4236, Test Acc: 32.86%\n","[Central-AdamW] Epoch 131: Train Loss: 0.0009, Train Acc: 99.97% | Val Loss: 12.7248, Val Acc: 32.20% | Test Loss: 12.5805, Test Acc: 32.75%\n","[Central-AdamW] Epoch 132: Train Loss: 0.0008, Train Acc: 99.97% | Val Loss: 12.6530, Val Acc: 32.70% | Test Loss: 12.4916, Test Acc: 32.57%\n","[Central-AdamW] Epoch 133: Train Loss: 0.0007, Train Acc: 99.98% | Val Loss: 12.7019, Val Acc: 32.52% | Test Loss: 12.5308, Test Acc: 32.65%\n","[Central-AdamW] Epoch 134: Train Loss: 0.0006, Train Acc: 99.98% | Val Loss: 12.9360, Val Acc: 32.36% | Test Loss: 12.7746, Test Acc: 32.82%\n","[Central-AdamW] Epoch 135: Train Loss: 0.0007, Train Acc: 99.98% | Val Loss: 12.8596, Val Acc: 32.62% | Test Loss: 12.7018, Test Acc: 32.90%\n","[Central-AdamW] Epoch 136: Train Loss: 0.0006, Train Acc: 99.98% | Val Loss: 12.9334, Val Acc: 32.66% | Test Loss: 12.7748, Test Acc: 32.81%\n","[Central-AdamW] Epoch 137: Train Loss: 0.0005, Train Acc: 99.98% | Val Loss: 13.0249, Val Acc: 32.50% | Test Loss: 12.8603, Test Acc: 32.81%\n","[Central-AdamW] Epoch 138: Train Loss: 0.0004, Train Acc: 99.99% | Val Loss: 13.0428, Val Acc: 32.62% | Test Loss: 12.8583, Test Acc: 32.75%\n","[Central-AdamW] Epoch 139: Train Loss: 0.0004, Train Acc: 99.98% | Val Loss: 13.0882, Val Acc: 32.62% | Test Loss: 12.9116, Test Acc: 32.72%\n","[Central-AdamW] Epoch 140: Train Loss: 0.0004, Train Acc: 99.98% | Val Loss: 13.1393, Val Acc: 32.58% | Test Loss: 12.9682, Test Acc: 32.75%\n","[Central-AdamW] Epoch 141: Train Loss: 0.0004, Train Acc: 99.99% | Val Loss: 13.1890, Val Acc: 32.52% | Test Loss: 13.0108, Test Acc: 32.79%\n","[Central-AdamW] Epoch 142: Train Loss: 0.0003, Train Acc: 99.99% | Val Loss: 13.2159, Val Acc: 32.64% | Test Loss: 13.0470, Test Acc: 32.88%\n","[Central-AdamW] Epoch 143: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2313, Val Acc: 32.62% | Test Loss: 13.0619, Test Acc: 32.76%\n","[Central-AdamW] Epoch 144: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2467, Val Acc: 32.64% | Test Loss: 13.0734, Test Acc: 32.80%\n","[Central-AdamW] Epoch 145: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2552, Val Acc: 32.62% | Test Loss: 13.0822, Test Acc: 32.76%\n","[Central-AdamW] Epoch 146: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2735, Val Acc: 32.64% | Test Loss: 13.1009, Test Acc: 32.78%\n","[Central-AdamW] Epoch 147: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2831, Val Acc: 32.68% | Test Loss: 13.1094, Test Acc: 32.76%\n","[Central-AdamW] Epoch 148: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2870, Val Acc: 32.68% | Test Loss: 13.1139, Test Acc: 32.75%\n","[Central-AdamW] Epoch 149: Train Loss: 0.0003, Train Acc: 99.99% | Val Loss: 13.2891, Val Acc: 32.66% | Test Loss: 13.1157, Test Acc: 32.74%\n","[Central-AdamW] Epoch 150: Train Loss: 0.0003, Train Acc: 99.99% | Val Loss: 13.2897, Val Acc: 32.66% | Test Loss: 13.1162, Test Acc: 32.75%\n"]}]},{"cell_type":"code","source":["#code above now appears to be compliant (chat gpt says)"],"metadata":{"id":"ocX4O6J2cBHU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"gWa0VtbG3yax"}},{"cell_type":"code","source":[],"metadata":{"id":"TmU01JmgcBJX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["100%|██████████| 169M/169M [00:05<00:00, 29.2MB/s]\n","[FL] Round 1: Val Loss = 4.5709, Val Accuracy = 2.40%\n","[FL] Round 2: Val Loss = 4.5073, Val Accuracy = 2.64%\n","[FL] Round 3: Val Loss = 4.4536, Val Accuracy = 3.26%\n","[FL] Round 4: Val Loss = 4.4548, Val Accuracy = 3.16%\n","[FL] Round 5: Val Loss = 4.4189, Val Accuracy = 3.14%\n","[FL] Round 6: Val Loss = 4.3072, Val Accuracy = 6.30%\n","[FL] Round 7: Val Loss = 4.3215, Val Accuracy = 5.26%\n","[FL] Round 8: Val Loss = 4.2054, Val Accuracy = 7.38%\n","[FL] Round 9: Val Loss = 4.2032, Val Accuracy = 7.32%\n","[FL] Round 10: Val Loss = 3.9894, Val Accuracy = 9.42%\n","[FL] Round 11: Val Loss = 3.9582, Val Accuracy = 10.82%\n","[FL] Round 12: Val Loss = 4.0215, Val Accuracy = 11.08%\n","[FL] Round 13: Val Loss = 3.9585, Val Accuracy = 10.90%\n","[FL] Round 14: Val Loss = 3.9344, Val Accuracy = 11.02%\n","[FL] Round 15: Val Loss = 3.9003, Val Accuracy = 12.00%\n","[FL] Round 16: Val Loss = 3.8394, Val Accuracy = 12.16%\n","[FL] Round 17: Val Loss = 3.8156, Val Accuracy = 13.12%\n","[FL] Round 18: Val Loss = 3.7513, Val Accuracy = 13.88%\n","[FL] Round 19: Val Loss = 3.7758, Val Accuracy = 13.58%\n","[FL] Round 20: Val Loss = 3.6978, Val Accuracy = 15.56%\n","[FL] Round 21: Val Loss = 3.7138, Val Accuracy = 15.46%\n","[FL] Round 22: Val Loss = 3.6522, Val Accuracy = 16.76%\n","[FL] Round 23: Val Loss = 3.6214, Val Accuracy = 16.36%\n","[FL] Round 24: Val Loss = 3.5902, Val Accuracy = 16.86%\n","[FL] Round 25: Val Loss = 3.6536, Val Accuracy = 16.16%\n","[FL] Round 26: Val Loss = 3.6220, Val Accuracy = 17.52%\n","[FL] Round 27: Val Loss = 3.5679, Val Accuracy = 18.04%\n","[FL] Round 28: Val Loss = 3.5803, Val Accuracy = 17.36%\n","[FL] Round 29: Val Loss = 3.5619, Val Accuracy = 17.68%\n","[FL] Round 30: Val Loss = 3.4814, Val Accuracy = 19.74%\n","[FL] Round 31: Val Loss = 3.4743, Val Accuracy = 18.34%\n","[FL] Round 32: Val Loss = 3.4316, Val Accuracy = 20.42%\n","[FL] Round 33: Val Loss = 3.4577, Val Accuracy = 19.56%\n","[FL] Round 34: Val Loss = 3.4756, Val Accuracy = 19.92%\n","[FL] Round 35: Val Loss = 3.4242, Val Accuracy = 19.46%\n","[FL] Round 36: Val Loss = 3.3136, Val Accuracy = 22.08%\n","[FL] Round 37: Val Loss = 3.3460, Val Accuracy = 21.14%\n","[FL] Round 38: Val Loss = 3.3799, Val Accuracy = 21.70%\n","[FL] Round 39: Val Loss = 3.3057, Val Accuracy = 22.34%\n","[FL] Round 40: Val Loss = 3.4048, Val Accuracy = 21.40%\n","[FL] Round 41: Val Loss = 3.3956, Val Accuracy = 21.00%\n","[FL] Round 42: Val Loss = 3.3370, Val Accuracy = 20.98%\n","[FL] Round 43: Val Loss = 3.3569, Val Accuracy = 22.66%\n","[FL] Round 44: Val Loss = 3.3457, Val Accuracy = 22.08%\n","[FL] Round 45: Val Loss = 3.2829, Val Accuracy = 23.18%\n","[FL] Round 46: Val Loss = 3.3232, Val Accuracy = 22.32%\n","[FL] Round 47: Val Loss = 3.2895, Val Accuracy = 23.20%\n","[FL] Round 48: Val Loss = 3.2752, Val Accuracy = 22.52%\n","[FL] Round 49: Val Loss = 3.3300, Val Accuracy = 22.40%\n","[FL] Round 50: Val Loss = 3.3314, Val Accuracy = 21.78%\n","[FL] Round 51: Val Loss = 3.3491, Val Accuracy = 21.60%\n","[FL] Round 52: Val Loss = 3.2996, Val Accuracy = 23.30%\n","[FL] Round 53: Val Loss = 3.2936, Val Accuracy = 23.34%\n","[FL] Round 54: Val Loss = 3.2819, Val Accuracy = 23.36%\n","[FL] Round 55: Val Loss = 3.2609, Val Accuracy = 23.64%\n","[FL] Round 56: Val Loss = 3.2542, Val Accuracy = 23.86%\n","[FL] Round 57: Val Loss = 3.2368, Val Accuracy = 24.44%\n","[FL] Round 58: Val Loss = 3.2558, Val Accuracy = 24.16%\n","[FL] Round 59: Val Loss = 3.2810, Val Accuracy = 23.88%\n","[FL] Round 60: Val Loss = 3.2543, Val Accuracy = 24.16%\n","[FL] Round 61: Val Loss = 3.3298, Val Accuracy = 23.46%\n","[FL] Round 62: Val Loss = 3.2699, Val Accuracy = 23.32%\n","[FL] Round 63: Val Loss = 3.2434, Val Accuracy = 24.14%\n","[FL] Round 64: Val Loss = 3.2477, Val Accuracy = 23.92%\n","[FL] Round 65: Val Loss = 3.2396, Val Accuracy = 23.10%\n","[FL] Round 66: Val Loss = 3.2628, Val Accuracy = 23.60%\n","[FL] Round 67: Val Loss = 3.2765, Val Accuracy = 23.86%\n","[FL] Round 68: Val Loss = 3.2528, Val Accuracy = 23.92%\n","[FL] Round 69: Val Loss = 3.2372, Val Accuracy = 23.18%\n","[FL] Round 70: Val Loss = 3.3054, Val Accuracy = 23.28%\n","[FL] Round 71: Val Loss = 3.2850, Val Accuracy = 23.74%\n","[FL] Round 72: Val Loss = 3.2209, Val Accuracy = 25.30%\n","[FL] Round 73: Val Loss = 3.2309, Val Accuracy = 24.16%\n","[FL] Round 74: Val Loss = 3.2264, Val Accuracy = 24.68%\n","[FL] Round 75: Val Loss = 3.2875, Val Accuracy = 23.28%\n","[FL] Round 76: Val Loss = 3.2283, Val Accuracy = 24.18%\n","[FL] Round 77: Val Loss = 3.2645, Val Accuracy = 23.88%\n","[FL] Round 78: Val Loss = 3.2387, Val Accuracy = 23.98%\n","[FL] Round 79: Val Loss = 3.2349, Val Accuracy = 24.48%\n","[FL] Round 80: Val Loss = 3.2588, Val Accuracy = 24.52%\n","[FL] Round 81: Val Loss = 3.1988, Val Accuracy = 25.34%\n","[FL] Round 82: Val Loss = 3.2103, Val Accuracy = 25.74%\n","[FL] Round 83: Val Loss = 3.2335, Val Accuracy = 24.66%\n","[FL] Round 84: Val Loss = 3.2743, Val Accuracy = 23.98%\n","[FL] Round 85: Val Loss = 3.2767, Val Accuracy = 24.36%\n","[FL] Round 86: Val Loss = 3.2437, Val Accuracy = 24.32%\n","[FL] Round 87: Val Loss = 3.2087, Val Accuracy = 25.24%\n","[FL] Round 88: Val Loss = 3.2231, Val Accuracy = 24.86%\n","[FL] Round 89: Val Loss = 3.2704, Val Accuracy = 24.58%\n","[FL] Round 90: Val Loss = 3.2516, Val Accuracy = 24.24%\n","[FL] Round 91: Val Loss = 3.3565, Val Accuracy = 22.86%\n","[FL] Round 92: Val Loss = 3.2413, Val Accuracy = 24.82%\n","[FL] Round 93: Val Loss = 3.3249, Val Accuracy = 22.68%\n","[FL] Round 94: Val Loss = 3.2313, Val Accuracy = 24.74%\n","[FL] Round 95: Val Loss = 3.2473, Val Accuracy = 24.74%\n","[FL] Round 96: Val Loss = 3.2414, Val Accuracy = 24.74%\n","[FL] Round 97: Val Loss = 3.2670, Val Accuracy = 24.10%\n","[FL] Round 98: Val Loss = 3.3107, Val Accuracy = 23.38%\n","[FL] Round 99: Val Loss = 3.3452, Val Accuracy = 23.18%\n","[FL] Round 100: Val Loss = 3.2052, Val Accuracy = 24.90%\n","[FL] Round 101: Val Loss = 3.2975, Val Accuracy = 23.94%\n","[FL] Round 102: Val Loss = 3.2645, Val Accuracy = 24.84%\n","[FL] Round 103: Val Loss = 3.2640, Val Accuracy = 24.80%\n","[FL] Round 104: Val Loss = 3.2581, Val Accuracy = 25.38%\n","[FL] Round 105: Val Loss = 3.2740, Val Accuracy = 24.00%\n","[FL] Round 106: Val Loss = 3.3351, Val Accuracy = 22.94%\n","[FL] Round 107: Val Loss = 3.3005, Val Accuracy = 23.98%\n","[FL] Round 108: Val Loss = 3.2702, Val Accuracy = 24.08%\n","[FL] Round 109: Val Loss = 3.2820, Val Accuracy = 23.82%\n","[FL] Round 110: Val Loss = 3.3002, Val Accuracy = 23.54%\n","[FL] Round 111: Val Loss = 3.3255, Val Accuracy = 23.18%\n","[FL] Round 112: Val Loss = 3.3308, Val Accuracy = 23.94%\n","[FL] Round 113: Val Loss = 3.2656, Val Accuracy = 24.40%\n","[FL] Round 114: Val Loss = 3.2159, Val Accuracy = 25.88%\n","[FL] Round 115: Val Loss = 3.2391, Val Accuracy = 24.96%\n","[FL] Round 116: Val Loss = 3.2319, Val Accuracy = 25.64%\n","[FL] Round 117: Val Loss = 3.3068, Val Accuracy = 23.74%\n","[FL] Round 118: Val Loss = 3.2670, Val Accuracy = 25.20%\n","[FL] Round 119: Val Loss = 3.1999, Val Accuracy = 25.16%\n","[FL] Round 120: Val Loss = 3.2770, Val Accuracy = 23.88%\n","[FL] Round 121: Val Loss = 3.2448, Val Accuracy = 25.44%\n","[FL] Round 122: Val Loss = 3.2800, Val Accuracy = 24.08%\n","[FL] Round 123: Val Loss = 3.3129, Val Accuracy = 24.42%\n","[FL] Round 124: Val Loss = 3.2871, Val Accuracy = 24.40%\n","[FL] Round 125: Val Loss = 3.3268, Val Accuracy = 24.02%\n","[FL] Round 126: Val Loss = 3.3073, Val Accuracy = 23.62%\n","[FL] Round 127: Val Loss = 3.2806, Val Accuracy = 24.98%\n","[FL] Round 128: Val Loss = 3.3710, Val Accuracy = 22.76%\n","[FL] Round 129: Val Loss = 3.3098, Val Accuracy = 24.40%\n","[FL] Round 130: Val Loss = 3.3072, Val Accuracy = 25.28%\n","[FL] Round 131: Val Loss = 3.3130, Val Accuracy = 23.92%\n","[FL] Round 132: Val Loss = 3.3095, Val Accuracy = 25.16%\n","[FL] Round 133: Val Loss = 3.2781, Val Accuracy = 25.14%\n","[FL] Round 134: Val Loss = 3.2650, Val Accuracy = 24.76%\n","[FL] Round 135: Val Loss = 3.2638, Val Accuracy = 25.48%\n","[FL] Round 136: Val Loss = 3.3313, Val Accuracy = 24.16%\n","[FL] Round 137: Val Loss = 3.3492, Val Accuracy = 24.22%\n","[FL] Round 138: Val Loss = 3.3554, Val Accuracy = 24.48%\n","[FL] Round 139: Val Loss = 3.3074, Val Accuracy = 24.64%\n","[FL] Round 140: Val Loss = 3.2892, Val Accuracy = 24.98%\n","[FL] Round 141: Val Loss = 3.3583, Val Accuracy = 23.86%\n","[FL] Round 142: Val Loss = 3.2793, Val Accuracy = 25.74%\n","[FL] Round 143: Val Loss = 3.3362, Val Accuracy = 24.44%\n","[FL] Round 144: Val Loss = 3.2696, Val Accuracy = 25.72%\n","[FL] Round 145: Val Loss = 3.3014, Val Accuracy = 25.16%\n","[FL] Round 146: Val Loss = 3.3731, Val Accuracy = 23.58%\n","[FL] Round 147: Val Loss = 3.3583, Val Accuracy = 23.90%\n","[FL] Round 148: Val Loss = 3.3180, Val Accuracy = 24.60%\n","[FL] Round 149: Val Loss = 3.3448, Val Accuracy = 24.92%\n","[FL] Round 150: Val Loss = 3.3969, Val Accuracy = 24.00%\n","[Central-SGDM] Epoch 1: Train Loss: 4.3539, Train Acc: 3.84% | Val Loss: 4.3320, Val Acc: 4.18% | Test Loss: 4.3203, Test Acc: 3.83%\n","[Central-SGDM] Epoch 2: Train Loss: 4.2205, Train Acc: 5.73% | Val Loss: 4.2673, Val Acc: 5.80% | Test Loss: 4.2416, Test Acc: 5.77%\n","[Central-SGDM] Epoch 3: Train Loss: 4.1562, Train Acc: 6.81% | Val Loss: 4.1640, Val Acc: 6.40% | Test Loss: 4.1213, Test Acc: 7.08%\n","[Central-SGDM] Epoch 4: Train Loss: 4.1315, Train Acc: 7.08% | Val Loss: 4.2969, Val Acc: 6.32% | Test Loss: 4.2718, Test Acc: 6.69%\n","[Central-SGDM] Epoch 5: Train Loss: 4.0925, Train Acc: 7.90% | Val Loss: 4.1404, Val Acc: 8.42% | Test Loss: 4.1525, Test Acc: 8.43%\n","[Central-SGDM] Epoch 6: Train Loss: 4.0709, Train Acc: 8.44% | Val Loss: 4.1764, Val Acc: 8.44% | Test Loss: 4.1224, Test Acc: 8.41%\n","[Central-SGDM] Epoch 7: Train Loss: 4.0188, Train Acc: 9.03% | Val Loss: 4.1682, Val Acc: 8.00% | Test Loss: 4.1018, Test Acc: 8.34%\n","[Central-SGDM] Epoch 8: Train Loss: 4.0037, Train Acc: 9.80% | Val Loss: 4.3341, Val Acc: 7.66% | Test Loss: 4.3244, Test Acc: 7.89%\n","[Central-SGDM] Epoch 9: Train Loss: 4.0172, Train Acc: 9.37% | Val Loss: 4.0747, Val Acc: 8.86% | Test Loss: 4.0814, Test Acc: 8.63%\n","[Central-SGDM] Epoch 10: Train Loss: 3.9955, Train Acc: 9.82% | Val Loss: 3.9566, Val Acc: 9.62% | Test Loss: 3.9447, Test Acc: 9.56%\n","[Central-SGDM] Epoch 11: Train Loss: 3.9670, Train Acc: 10.09% | Val Loss: 4.0063, Val Acc: 10.04% | Test Loss: 3.9580, Test Acc: 10.67%\n","[Central-SGDM] Epoch 12: Train Loss: 3.9971, Train Acc: 10.12% | Val Loss: 4.1165, Val Acc: 8.84% | Test Loss: 4.1152, Test Acc: 8.51%\n","[Central-SGDM] Epoch 13: Train Loss: 3.9353, Train Acc: 10.76% | Val Loss: 3.8843, Val Acc: 11.54% | Test Loss: 3.8657, Test Acc: 11.84%\n","[Central-SGDM] Epoch 14: Train Loss: 3.9235, Train Acc: 11.30% | Val Loss: 3.9828, Val Acc: 10.76% | Test Loss: 3.9826, Test Acc: 10.19%\n","[Central-SGDM] Epoch 15: Train Loss: 3.9205, Train Acc: 11.06% | Val Loss: 4.0017, Val Acc: 9.84% | Test Loss: 3.9822, Test Acc: 10.66%\n","[Central-SGDM] Epoch 16: Train Loss: 3.9003, Train Acc: 11.51% | Val Loss: 4.1619, Val Acc: 7.32% | Test Loss: 4.1244, Test Acc: 7.62%\n","[Central-SGDM] Epoch 17: Train Loss: 3.9084, Train Acc: 11.36% | Val Loss: 4.0732, Val Acc: 8.96% | Test Loss: 4.0527, Test Acc: 9.81%\n","[Central-SGDM] Epoch 18: Train Loss: 3.8940, Train Acc: 11.64% | Val Loss: 3.9440, Val Acc: 10.50% | Test Loss: 3.9288, Test Acc: 11.36%\n","[Central-SGDM] Epoch 19: Train Loss: 3.8366, Train Acc: 12.37% | Val Loss: 3.8780, Val Acc: 12.26% | Test Loss: 3.8478, Test Acc: 12.73%\n","[Central-SGDM] Epoch 20: Train Loss: 3.8531, Train Acc: 12.33% | Val Loss: 3.9178, Val Acc: 11.40% | Test Loss: 3.8839, Test Acc: 12.78%\n","[Central-SGDM] Epoch 21: Train Loss: 3.8739, Train Acc: 12.11% | Val Loss: 3.9529, Val Acc: 10.70% | Test Loss: 3.9422, Test Acc: 11.07%\n","[Central-SGDM] Epoch 22: Train Loss: 3.8369, Train Acc: 12.78% | Val Loss: 3.9476, Val Acc: 11.08% | Test Loss: 3.9383, Test Acc: 11.78%\n","[Central-SGDM] Epoch 23: Train Loss: 3.8415, Train Acc: 12.80% | Val Loss: 3.8690, Val Acc: 11.48% | Test Loss: 3.8416, Test Acc: 11.95%\n","[Central-SGDM] Epoch 24: Train Loss: 3.8086, Train Acc: 13.11% | Val Loss: 3.8458, Val Acc: 12.22% | Test Loss: 3.8391, Test Acc: 13.08%\n","[Central-SGDM] Epoch 25: Train Loss: 3.8129, Train Acc: 12.96% | Val Loss: 3.8308, Val Acc: 13.00% | Test Loss: 3.8193, Test Acc: 13.23%\n","[Central-SGDM] Epoch 26: Train Loss: 3.8034, Train Acc: 13.65% | Val Loss: 3.9634, Val Acc: 10.92% | Test Loss: 3.9554, Test Acc: 11.16%\n","[Central-SGDM] Epoch 27: Train Loss: 3.8034, Train Acc: 13.54% | Val Loss: 3.8237, Val Acc: 13.00% | Test Loss: 3.7798, Test Acc: 13.80%\n","[Central-SGDM] Epoch 28: Train Loss: 3.7592, Train Acc: 14.11% | Val Loss: 3.9043, Val Acc: 12.70% | Test Loss: 3.8519, Test Acc: 13.16%\n","[Central-SGDM] Epoch 29: Train Loss: 3.7350, Train Acc: 14.45% | Val Loss: 3.9856, Val Acc: 11.58% | Test Loss: 3.9483, Test Acc: 11.78%\n","[Central-SGDM] Epoch 30: Train Loss: 3.7317, Train Acc: 14.29% | Val Loss: 3.8292, Val Acc: 13.70% | Test Loss: 3.7723, Test Acc: 14.60%\n","[Central-SGDM] Epoch 31: Train Loss: 3.7161, Train Acc: 14.74% | Val Loss: 3.8105, Val Acc: 12.36% | Test Loss: 3.7633, Test Acc: 13.36%\n","[Central-SGDM] Epoch 32: Train Loss: 3.7222, Train Acc: 14.67% | Val Loss: 3.9005, Val Acc: 13.22% | Test Loss: 3.8751, Test Acc: 13.50%\n","[Central-SGDM] Epoch 33: Train Loss: 3.7368, Train Acc: 14.70% | Val Loss: 3.7912, Val Acc: 13.96% | Test Loss: 3.7293, Test Acc: 15.30%\n","[Central-SGDM] Epoch 34: Train Loss: 3.7049, Train Acc: 15.14% | Val Loss: 3.9516, Val Acc: 10.86% | Test Loss: 3.9159, Test Acc: 11.57%\n","[Central-SGDM] Epoch 35: Train Loss: 3.6909, Train Acc: 15.34% | Val Loss: 3.9469, Val Acc: 12.96% | Test Loss: 3.9053, Test Acc: 12.34%\n","[Central-SGDM] Epoch 36: Train Loss: 3.6965, Train Acc: 15.24% | Val Loss: 3.8644, Val Acc: 12.18% | Test Loss: 3.8311, Test Acc: 12.69%\n","[Central-SGDM] Epoch 37: Train Loss: 3.6299, Train Acc: 16.25% | Val Loss: 3.7623, Val Acc: 14.08% | Test Loss: 3.7173, Test Acc: 14.75%\n","[Central-SGDM] Epoch 38: Train Loss: 3.6614, Train Acc: 15.88% | Val Loss: 3.7635, Val Acc: 14.00% | Test Loss: 3.7411, Test Acc: 14.73%\n","[Central-SGDM] Epoch 39: Train Loss: 3.6326, Train Acc: 16.21% | Val Loss: 4.1168, Val Acc: 8.88% | Test Loss: 4.0993, Test Acc: 9.12%\n","[Central-SGDM] Epoch 40: Train Loss: 3.6155, Train Acc: 16.53% | Val Loss: 3.9253, Val Acc: 13.40% | Test Loss: 3.9184, Test Acc: 13.56%\n","[Central-SGDM] Epoch 41: Train Loss: 3.6301, Train Acc: 16.63% | Val Loss: 3.8018, Val Acc: 15.12% | Test Loss: 3.7556, Test Acc: 15.22%\n","[Central-SGDM] Epoch 42: Train Loss: 3.6138, Train Acc: 16.93% | Val Loss: 3.8374, Val Acc: 13.60% | Test Loss: 3.7583, Test Acc: 15.01%\n","[Central-SGDM] Epoch 43: Train Loss: 3.5914, Train Acc: 17.48% | Val Loss: 3.8059, Val Acc: 14.90% | Test Loss: 3.7996, Test Acc: 14.49%\n","[Central-SGDM] Epoch 44: Train Loss: 3.5689, Train Acc: 17.83% | Val Loss: 3.6308, Val Acc: 17.48% | Test Loss: 3.6340, Test Acc: 17.12%\n","[Central-SGDM] Epoch 45: Train Loss: 3.5530, Train Acc: 17.84% | Val Loss: 3.7258, Val Acc: 15.34% | Test Loss: 3.6952, Test Acc: 15.33%\n","[Central-SGDM] Epoch 46: Train Loss: 3.5208, Train Acc: 18.45% | Val Loss: 3.7392, Val Acc: 15.32% | Test Loss: 3.7267, Test Acc: 15.15%\n","[Central-SGDM] Epoch 47: Train Loss: 3.5032, Train Acc: 18.77% | Val Loss: 3.8283, Val Acc: 13.34% | Test Loss: 3.7910, Test Acc: 14.40%\n","[Central-SGDM] Epoch 48: Train Loss: 3.4704, Train Acc: 19.42% | Val Loss: 3.6476, Val Acc: 16.06% | Test Loss: 3.5797, Test Acc: 17.02%\n","[Central-SGDM] Epoch 49: Train Loss: 3.4549, Train Acc: 19.62% | Val Loss: 3.8004, Val Acc: 15.80% | Test Loss: 3.7794, Test Acc: 15.87%\n","[Central-SGDM] Epoch 50: Train Loss: 3.4675, Train Acc: 19.40% | Val Loss: 3.7360, Val Acc: 14.72% | Test Loss: 3.6654, Test Acc: 16.61%\n","[Central-SGDM] Epoch 51: Train Loss: 3.4659, Train Acc: 19.97% | Val Loss: 3.6673, Val Acc: 16.80% | Test Loss: 3.6512, Test Acc: 16.80%\n","[Central-SGDM] Epoch 52: Train Loss: 3.3890, Train Acc: 21.08% | Val Loss: 3.5529, Val Acc: 19.82% | Test Loss: 3.4998, Test Acc: 18.95%\n","[Central-SGDM] Epoch 53: Train Loss: 3.3964, Train Acc: 20.92% | Val Loss: 3.7019, Val Acc: 16.20% | Test Loss: 3.6652, Test Acc: 16.86%\n","[Central-SGDM] Epoch 54: Train Loss: 3.3562, Train Acc: 21.39% | Val Loss: 3.5299, Val Acc: 19.38% | Test Loss: 3.5132, Test Acc: 19.40%\n","[Central-SGDM] Epoch 55: Train Loss: 3.3464, Train Acc: 22.09% | Val Loss: 3.6266, Val Acc: 17.32% | Test Loss: 3.5479, Test Acc: 18.91%\n","[Central-SGDM] Epoch 56: Train Loss: 3.3318, Train Acc: 21.91% | Val Loss: 3.6391, Val Acc: 17.40% | Test Loss: 3.6140, Test Acc: 18.27%\n","[Central-SGDM] Epoch 57: Train Loss: 3.3175, Train Acc: 22.27% | Val Loss: 3.5802, Val Acc: 18.18% | Test Loss: 3.5389, Test Acc: 19.11%\n","[Central-SGDM] Epoch 58: Train Loss: 3.2862, Train Acc: 23.10% | Val Loss: 3.7996, Val Acc: 15.08% | Test Loss: 3.7729, Test Acc: 15.51%\n","[Central-SGDM] Epoch 59: Train Loss: 3.2383, Train Acc: 24.26% | Val Loss: 3.6286, Val Acc: 18.48% | Test Loss: 3.6004, Test Acc: 18.83%\n","[Central-SGDM] Epoch 60: Train Loss: 3.2263, Train Acc: 24.50% | Val Loss: 3.7474, Val Acc: 18.06% | Test Loss: 3.6848, Test Acc: 18.47%\n","[Central-SGDM] Epoch 61: Train Loss: 3.2075, Train Acc: 24.53% | Val Loss: 3.6538, Val Acc: 17.62% | Test Loss: 3.5998, Test Acc: 18.00%\n","[Central-SGDM] Epoch 62: Train Loss: 3.1686, Train Acc: 25.50% | Val Loss: 3.5703, Val Acc: 19.26% | Test Loss: 3.5495, Test Acc: 19.79%\n","[Central-SGDM] Epoch 63: Train Loss: 3.1244, Train Acc: 26.24% | Val Loss: 3.5894, Val Acc: 19.18% | Test Loss: 3.5364, Test Acc: 19.66%\n","[Central-SGDM] Epoch 64: Train Loss: 3.0869, Train Acc: 27.05% | Val Loss: 3.5705, Val Acc: 18.80% | Test Loss: 3.4986, Test Acc: 20.04%\n","[Central-SGDM] Epoch 65: Train Loss: 3.0559, Train Acc: 27.82% | Val Loss: 3.5516, Val Acc: 20.40% | Test Loss: 3.5041, Test Acc: 21.18%\n","[Central-SGDM] Epoch 66: Train Loss: 3.0182, Train Acc: 28.60% | Val Loss: 3.5249, Val Acc: 21.46% | Test Loss: 3.4641, Test Acc: 21.63%\n","[Central-SGDM] Epoch 67: Train Loss: 2.9924, Train Acc: 28.96% | Val Loss: 3.5565, Val Acc: 20.72% | Test Loss: 3.5704, Test Acc: 20.79%\n","[Central-SGDM] Epoch 68: Train Loss: 2.9728, Train Acc: 29.55% | Val Loss: 3.5983, Val Acc: 21.22% | Test Loss: 3.5634, Test Acc: 21.36%\n","[Central-SGDM] Epoch 69: Train Loss: 2.9315, Train Acc: 30.26% | Val Loss: 3.6152, Val Acc: 20.84% | Test Loss: 3.5866, Test Acc: 20.74%\n","[Central-SGDM] Epoch 70: Train Loss: 2.8985, Train Acc: 30.96% | Val Loss: 3.6423, Val Acc: 19.84% | Test Loss: 3.6232, Test Acc: 20.42%\n","[Central-SGDM] Epoch 71: Train Loss: 2.8280, Train Acc: 32.34% | Val Loss: 3.5879, Val Acc: 19.66% | Test Loss: 3.5646, Test Acc: 19.97%\n","[Central-SGDM] Epoch 72: Train Loss: 2.8079, Train Acc: 32.66% | Val Loss: 3.5046, Val Acc: 22.26% | Test Loss: 3.4689, Test Acc: 22.65%\n","[Central-SGDM] Epoch 73: Train Loss: 2.7489, Train Acc: 34.18% | Val Loss: 3.4425, Val Acc: 22.44% | Test Loss: 3.4418, Test Acc: 22.16%\n","[Central-SGDM] Epoch 74: Train Loss: 2.7181, Train Acc: 34.48% | Val Loss: 3.5502, Val Acc: 22.86% | Test Loss: 3.5362, Test Acc: 22.24%\n","[Central-SGDM] Epoch 75: Train Loss: 2.6615, Train Acc: 35.69% | Val Loss: 3.4457, Val Acc: 22.60% | Test Loss: 3.4346, Test Acc: 23.04%\n","[Central-SGDM] Epoch 76: Train Loss: 2.5893, Train Acc: 37.00% | Val Loss: 3.4656, Val Acc: 23.16% | Test Loss: 3.4283, Test Acc: 23.60%\n","[Central-SGDM] Epoch 77: Train Loss: 2.5685, Train Acc: 37.62% | Val Loss: 3.4975, Val Acc: 24.16% | Test Loss: 3.4896, Test Acc: 24.26%\n","[Central-SGDM] Epoch 78: Train Loss: 2.5181, Train Acc: 38.39% | Val Loss: 3.5827, Val Acc: 22.64% | Test Loss: 3.5555, Test Acc: 23.04%\n","[Central-SGDM] Epoch 79: Train Loss: 2.4516, Train Acc: 39.89% | Val Loss: 3.4491, Val Acc: 23.96% | Test Loss: 3.4066, Test Acc: 23.65%\n","[Central-SGDM] Epoch 80: Train Loss: 2.3922, Train Acc: 41.01% | Val Loss: 3.5280, Val Acc: 23.76% | Test Loss: 3.5387, Test Acc: 24.12%\n","[Central-SGDM] Epoch 81: Train Loss: 2.3563, Train Acc: 41.79% | Val Loss: 3.4931, Val Acc: 24.78% | Test Loss: 3.4604, Test Acc: 24.35%\n","[Central-SGDM] Epoch 82: Train Loss: 2.2835, Train Acc: 43.33% | Val Loss: 3.5609, Val Acc: 23.74% | Test Loss: 3.5281, Test Acc: 24.14%\n","[Central-SGDM] Epoch 83: Train Loss: 2.2296, Train Acc: 44.67% | Val Loss: 3.5727, Val Acc: 23.14% | Test Loss: 3.5505, Test Acc: 23.96%\n","[Central-SGDM] Epoch 84: Train Loss: 2.1809, Train Acc: 45.58% | Val Loss: 3.5361, Val Acc: 24.52% | Test Loss: 3.5476, Test Acc: 24.56%\n","[Central-SGDM] Epoch 85: Train Loss: 2.0963, Train Acc: 47.57% | Val Loss: 3.6347, Val Acc: 24.48% | Test Loss: 3.6260, Test Acc: 24.74%\n","[Central-SGDM] Epoch 86: Train Loss: 2.0406, Train Acc: 48.58% | Val Loss: 3.5572, Val Acc: 26.32% | Test Loss: 3.5627, Test Acc: 26.50%\n","[Central-SGDM] Epoch 87: Train Loss: 1.9691, Train Acc: 50.14% | Val Loss: 3.6417, Val Acc: 24.80% | Test Loss: 3.6525, Test Acc: 25.26%\n","[Central-SGDM] Epoch 88: Train Loss: 1.8941, Train Acc: 51.56% | Val Loss: 3.5997, Val Acc: 24.60% | Test Loss: 3.5738, Test Acc: 25.16%\n","[Central-SGDM] Epoch 89: Train Loss: 1.8739, Train Acc: 52.20% | Val Loss: 3.8276, Val Acc: 24.24% | Test Loss: 3.7983, Test Acc: 25.23%\n","[Central-SGDM] Epoch 90: Train Loss: 1.7570, Train Acc: 54.61% | Val Loss: 3.7457, Val Acc: 24.94% | Test Loss: 3.7083, Test Acc: 25.79%\n","[Central-SGDM] Epoch 91: Train Loss: 1.6996, Train Acc: 55.94% | Val Loss: 3.7305, Val Acc: 24.40% | Test Loss: 3.7131, Test Acc: 25.02%\n","[Central-SGDM] Epoch 92: Train Loss: 1.6309, Train Acc: 57.52% | Val Loss: 3.8977, Val Acc: 24.34% | Test Loss: 3.8895, Test Acc: 24.61%\n","[Central-SGDM] Epoch 93: Train Loss: 1.5483, Train Acc: 59.38% | Val Loss: 3.8345, Val Acc: 25.56% | Test Loss: 3.8364, Test Acc: 25.54%\n","[Central-SGDM] Epoch 94: Train Loss: 1.4657, Train Acc: 61.07% | Val Loss: 3.8515, Val Acc: 25.18% | Test Loss: 3.8400, Test Acc: 26.00%\n","[Central-SGDM] Epoch 95: Train Loss: 1.3965, Train Acc: 62.99% | Val Loss: 3.9511, Val Acc: 24.30% | Test Loss: 3.8676, Test Acc: 25.15%\n","[Central-SGDM] Epoch 96: Train Loss: 1.3628, Train Acc: 63.36% | Val Loss: 4.0245, Val Acc: 25.12% | Test Loss: 3.9978, Test Acc: 26.16%\n","[Central-SGDM] Epoch 97: Train Loss: 1.2951, Train Acc: 65.04% | Val Loss: 4.1364, Val Acc: 25.64% | Test Loss: 4.0432, Test Acc: 26.51%\n","[Central-SGDM] Epoch 98: Train Loss: 1.1658, Train Acc: 68.24% | Val Loss: 4.0813, Val Acc: 25.98% | Test Loss: 4.0279, Test Acc: 27.29%\n","[Central-SGDM] Epoch 99: Train Loss: 1.1170, Train Acc: 69.27% | Val Loss: 4.2388, Val Acc: 24.64% | Test Loss: 4.1569, Test Acc: 25.84%\n","[Central-SGDM] Epoch 100: Train Loss: 1.0774, Train Acc: 70.31% | Val Loss: 4.4722, Val Acc: 26.18% | Test Loss: 4.3472, Test Acc: 25.99%\n","[Central-SGDM] Epoch 101: Train Loss: 0.9642, Train Acc: 72.89% | Val Loss: 4.4192, Val Acc: 26.64% | Test Loss: 4.3522, Test Acc: 27.06%\n","[Central-SGDM] Epoch 102: Train Loss: 0.8616, Train Acc: 75.45% | Val Loss: 4.4770, Val Acc: 26.24% | Test Loss: 4.3229, Test Acc: 27.00%\n","[Central-SGDM] Epoch 103: Train Loss: 0.8091, Train Acc: 76.81% | Val Loss: 4.7243, Val Acc: 25.68% | Test Loss: 4.5473, Test Acc: 26.78%\n","[Central-SGDM] Epoch 104: Train Loss: 0.7373, Train Acc: 78.78% | Val Loss: 4.7185, Val Acc: 25.96% | Test Loss: 4.5997, Test Acc: 27.22%\n","[Central-SGDM] Epoch 105: Train Loss: 0.7821, Train Acc: 77.61% | Val Loss: 4.6662, Val Acc: 26.12% | Test Loss: 4.6631, Test Acc: 26.95%\n","[Central-SGDM] Epoch 106: Train Loss: 0.6492, Train Acc: 81.11% | Val Loss: 4.9233, Val Acc: 28.08% | Test Loss: 4.7979, Test Acc: 27.92%\n","[Central-SGDM] Epoch 107: Train Loss: 0.5335, Train Acc: 83.98% | Val Loss: 5.1321, Val Acc: 26.84% | Test Loss: 5.0648, Test Acc: 27.07%\n","[Central-SGDM] Epoch 108: Train Loss: 0.4275, Train Acc: 87.19% | Val Loss: 5.2719, Val Acc: 27.78% | Test Loss: 5.1629, Test Acc: 27.97%\n","[Central-SGDM] Epoch 109: Train Loss: 0.3315, Train Acc: 90.14% | Val Loss: 5.2222, Val Acc: 28.28% | Test Loss: 5.1864, Test Acc: 28.42%\n","[Central-SGDM] Epoch 110: Train Loss: 0.2982, Train Acc: 91.09% | Val Loss: 5.4175, Val Acc: 28.28% | Test Loss: 5.3702, Test Acc: 28.08%\n","[Central-SGDM] Epoch 111: Train Loss: 0.2371, Train Acc: 92.88% | Val Loss: 5.5074, Val Acc: 28.00% | Test Loss: 5.3595, Test Acc: 28.65%\n","[Central-SGDM] Epoch 112: Train Loss: 0.1482, Train Acc: 95.99% | Val Loss: 5.5232, Val Acc: 28.88% | Test Loss: 5.4168, Test Acc: 28.86%\n","[Central-SGDM] Epoch 113: Train Loss: 0.0926, Train Acc: 97.78% | Val Loss: 5.5056, Val Acc: 29.70% | Test Loss: 5.4286, Test Acc: 30.15%\n","[Central-SGDM] Epoch 114: Train Loss: 0.0464, Train Acc: 99.25% | Val Loss: 5.4863, Val Acc: 31.16% | Test Loss: 5.3656, Test Acc: 30.79%\n","[Central-SGDM] Epoch 115: Train Loss: 0.0293, Train Acc: 99.64% | Val Loss: 5.3915, Val Acc: 31.46% | Test Loss: 5.2541, Test Acc: 31.31%\n","[Central-SGDM] Epoch 116: Train Loss: 0.0245, Train Acc: 99.76% | Val Loss: 5.2900, Val Acc: 31.96% | Test Loss: 5.1593, Test Acc: 31.57%\n","[Central-SGDM] Epoch 117: Train Loss: 0.0225, Train Acc: 99.84% | Val Loss: 5.2309, Val Acc: 32.22% | Test Loss: 5.0921, Test Acc: 31.46%\n","[Central-SGDM] Epoch 118: Train Loss: 0.0222, Train Acc: 99.86% | Val Loss: 5.1525, Val Acc: 32.36% | Test Loss: 5.0056, Test Acc: 32.00%\n","[Central-SGDM] Epoch 119: Train Loss: 0.0225, Train Acc: 99.89% | Val Loss: 5.0964, Val Acc: 32.46% | Test Loss: 4.9397, Test Acc: 32.18%\n","[Central-SGDM] Epoch 120: Train Loss: 0.0229, Train Acc: 99.91% | Val Loss: 5.0746, Val Acc: 32.60% | Test Loss: 4.9226, Test Acc: 32.50%\n","[Central-SGDM] Epoch 121: Train Loss: 0.0225, Train Acc: 99.92% | Val Loss: 5.0300, Val Acc: 32.54% | Test Loss: 4.8736, Test Acc: 32.64%\n","[Central-SGDM] Epoch 122: Train Loss: 0.0225, Train Acc: 99.93% | Val Loss: 5.0177, Val Acc: 32.72% | Test Loss: 4.8577, Test Acc: 32.90%\n","[Central-SGDM] Epoch 123: Train Loss: 0.0226, Train Acc: 99.92% | Val Loss: 4.9892, Val Acc: 32.80% | Test Loss: 4.8302, Test Acc: 32.81%\n","[Central-SGDM] Epoch 124: Train Loss: 0.0224, Train Acc: 99.95% | Val Loss: 5.0086, Val Acc: 32.58% | Test Loss: 4.8463, Test Acc: 32.96%\n","[Central-SGDM] Epoch 125: Train Loss: 0.0222, Train Acc: 99.95% | Val Loss: 4.9951, Val Acc: 32.64% | Test Loss: 4.8245, Test Acc: 33.17%\n","[Central-SGDM] Epoch 126: Train Loss: 0.0219, Train Acc: 99.96% | Val Loss: 4.9793, Val Acc: 32.74% | Test Loss: 4.8185, Test Acc: 33.13%\n","[Central-SGDM] Epoch 127: Train Loss: 0.0217, Train Acc: 99.96% | Val Loss: 5.0019, Val Acc: 32.76% | Test Loss: 4.8335, Test Acc: 33.23%\n","[Central-SGDM] Epoch 128: Train Loss: 0.0215, Train Acc: 99.96% | Val Loss: 5.0043, Val Acc: 32.68% | Test Loss: 4.8232, Test Acc: 33.27%\n","[Central-SGDM] Epoch 129: Train Loss: 0.0213, Train Acc: 99.97% | Val Loss: 4.9940, Val Acc: 32.70% | Test Loss: 4.8166, Test Acc: 33.43%\n","[Central-SGDM] Epoch 130: Train Loss: 0.0212, Train Acc: 99.97% | Val Loss: 4.9893, Val Acc: 32.80% | Test Loss: 4.8093, Test Acc: 33.37%\n","[Central-SGDM] Epoch 131: Train Loss: 0.0210, Train Acc: 99.97% | Val Loss: 4.9995, Val Acc: 32.80% | Test Loss: 4.8145, Test Acc: 33.55%\n","[Central-SGDM] Epoch 132: Train Loss: 0.0209, Train Acc: 99.97% | Val Loss: 4.9881, Val Acc: 32.76% | Test Loss: 4.8084, Test Acc: 33.70%\n","[Central-SGDM] Epoch 133: Train Loss: 0.0207, Train Acc: 99.97% | Val Loss: 4.9985, Val Acc: 32.74% | Test Loss: 4.8070, Test Acc: 33.51%\n","[Central-SGDM] Epoch 134: Train Loss: 0.0206, Train Acc: 99.97% | Val Loss: 4.9923, Val Acc: 32.78% | Test Loss: 4.8042, Test Acc: 33.58%\n","[Central-SGDM] Epoch 135: Train Loss: 0.0204, Train Acc: 99.97% | Val Loss: 4.9939, Val Acc: 32.66% | Test Loss: 4.8044, Test Acc: 33.63%\n","[Central-SGDM] Epoch 136: Train Loss: 0.0202, Train Acc: 99.97% | Val Loss: 4.9885, Val Acc: 32.80% | Test Loss: 4.8012, Test Acc: 33.79%\n","[Central-SGDM] Epoch 137: Train Loss: 0.0201, Train Acc: 99.98% | Val Loss: 4.9904, Val Acc: 32.86% | Test Loss: 4.7988, Test Acc: 33.78%\n","[Central-SGDM] Epoch 138: Train Loss: 0.0200, Train Acc: 99.98% | Val Loss: 4.9917, Val Acc: 32.86% | Test Loss: 4.8012, Test Acc: 33.85%\n","[Central-SGDM] Epoch 139: Train Loss: 0.0199, Train Acc: 99.98% | Val Loss: 5.0004, Val Acc: 32.80% | Test Loss: 4.8060, Test Acc: 33.84%\n","[Central-SGDM] Epoch 140: Train Loss: 0.0197, Train Acc: 99.98% | Val Loss: 4.9957, Val Acc: 32.98% | Test Loss: 4.8019, Test Acc: 33.78%\n","[Central-SGDM] Epoch 141: Train Loss: 0.0196, Train Acc: 99.98% | Val Loss: 4.9996, Val Acc: 32.80% | Test Loss: 4.8041, Test Acc: 33.86%\n","[Central-SGDM] Epoch 142: Train Loss: 0.0196, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.78% | Test Loss: 4.8023, Test Acc: 33.86%\n","[Central-SGDM] Epoch 143: Train Loss: 0.0195, Train Acc: 99.98% | Val Loss: 4.9968, Val Acc: 32.86% | Test Loss: 4.8015, Test Acc: 33.81%\n","[Central-SGDM] Epoch 144: Train Loss: 0.0194, Train Acc: 99.98% | Val Loss: 4.9951, Val Acc: 32.86% | Test Loss: 4.8000, Test Acc: 33.86%\n","[Central-SGDM] Epoch 145: Train Loss: 0.0193, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.84% | Test Loss: 4.8023, Test Acc: 33.84%\n","[Central-SGDM] Epoch 146: Train Loss: 0.0193, Train Acc: 99.98% | Val Loss: 4.9970, Val Acc: 32.84% | Test Loss: 4.8016, Test Acc: 33.80%\n","[Central-SGDM] Epoch 147: Train Loss: 0.0192, Train Acc: 99.98% | Val Loss: 4.9977, Val Acc: 32.84% | Test Loss: 4.8019, Test Acc: 33.85%\n","[Central-SGDM] Epoch 148: Train Loss: 0.0192, Train Acc: 99.98% | Val Loss: 4.9982, Val Acc: 32.86% | Test Loss: 4.8025, Test Acc: 33.82%\n","[Central-SGDM] Epoch 149: Train Loss: 0.0191, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.88% | Test Loss: 4.8023, Test Acc: 33.83%\n","[Central-SGDM] Epoch 150: Train Loss: 0.0191, Train Acc: 99.98% | Val Loss: 4.9981, Val Acc: 32.88% | Test Loss: 4.8023, Test Acc: 33.82%\n","[Central-AdamW] Epoch 1: Train Loss: 3.7988, Train Acc: 11.72% | Val Loss: 3.4045, Val Acc: 18.18% | Test Loss: 3.3868, Test Acc: 18.09%\n","[Central-AdamW] Epoch 2: Train Loss: 3.1115, Train Acc: 23.34% | Val Loss: 3.0404, Val Acc: 25.68% | Test Loss: 3.0058, Test Acc: 26.03%\n","[Central-AdamW] Epoch 3: Train Loss: 2.7834, Train Acc: 29.74% | Val Loss: 2.8631, Val Acc: 28.26% | Test Loss: 2.8617, Test Acc: 28.97%\n","[Central-AdamW] Epoch 4: Train Loss: 2.5607, Train Acc: 33.92% | Val Loss: 2.7034, Val Acc: 32.58% | Test Loss: 2.7036, Test Acc: 32.39%\n","[Central-AdamW] Epoch 5: Train Loss: 2.3839, Train Acc: 37.62% | Val Loss: 2.6680, Val Acc: 34.66% | Test Loss: 2.6736, Test Acc: 33.63%\n","[Central-AdamW] Epoch 6: Train Loss: 2.2517, Train Acc: 40.42% | Val Loss: 2.6041, Val Acc: 35.04% | Test Loss: 2.6087, Test Acc: 34.56%\n","[Central-AdamW] Epoch 7: Train Loss: 2.1150, Train Acc: 43.40% | Val Loss: 2.6755, Val Acc: 34.74% | Test Loss: 2.6952, Test Acc: 34.20%\n","[Central-AdamW] Epoch 8: Train Loss: 2.0048, Train Acc: 45.71% | Val Loss: 2.6764, Val Acc: 35.22% | Test Loss: 2.6596, Test Acc: 35.76%\n","[Central-AdamW] Epoch 9: Train Loss: 1.9007, Train Acc: 47.92% | Val Loss: 2.6322, Val Acc: 35.94% | Test Loss: 2.6332, Test Acc: 35.46%\n","[Central-AdamW] Epoch 10: Train Loss: 1.7964, Train Acc: 50.11% | Val Loss: 2.6881, Val Acc: 36.00% | Test Loss: 2.6922, Test Acc: 35.48%\n","[Central-AdamW] Epoch 11: Train Loss: 1.7102, Train Acc: 52.12% | Val Loss: 2.7407, Val Acc: 36.54% | Test Loss: 2.7513, Test Acc: 35.60%\n","[Central-AdamW] Epoch 12: Train Loss: 1.6171, Train Acc: 54.24% | Val Loss: 2.8486, Val Acc: 36.14% | Test Loss: 2.8672, Test Acc: 35.31%\n","[Central-AdamW] Epoch 13: Train Loss: 1.5441, Train Acc: 56.18% | Val Loss: 2.8412, Val Acc: 35.18% | Test Loss: 2.8476, Test Acc: 35.69%\n","[Central-AdamW] Epoch 14: Train Loss: 1.4526, Train Acc: 58.40% | Val Loss: 3.0192, Val Acc: 34.36% | Test Loss: 3.0438, Test Acc: 34.64%\n","[Central-AdamW] Epoch 15: Train Loss: 1.3934, Train Acc: 59.78% | Val Loss: 3.0408, Val Acc: 34.74% | Test Loss: 3.0957, Test Acc: 35.20%\n","[Central-AdamW] Epoch 16: Train Loss: 1.3266, Train Acc: 61.30% | Val Loss: 3.0962, Val Acc: 34.56% | Test Loss: 3.1347, Test Acc: 34.52%\n","[Central-AdamW] Epoch 17: Train Loss: 1.2479, Train Acc: 63.28% | Val Loss: 3.2918, Val Acc: 34.74% | Test Loss: 3.3268, Test Acc: 33.56%\n","[Central-AdamW] Epoch 18: Train Loss: 1.1980, Train Acc: 64.60% | Val Loss: 3.3785, Val Acc: 33.94% | Test Loss: 3.4165, Test Acc: 34.07%\n","[Central-AdamW] Epoch 19: Train Loss: 1.1322, Train Acc: 66.11% | Val Loss: 3.4535, Val Acc: 34.46% | Test Loss: 3.4714, Test Acc: 34.61%\n","[Central-AdamW] Epoch 20: Train Loss: 1.0731, Train Acc: 67.93% | Val Loss: 3.5957, Val Acc: 34.54% | Test Loss: 3.6130, Test Acc: 34.30%\n","[Central-AdamW] Epoch 21: Train Loss: 1.0273, Train Acc: 68.82% | Val Loss: 3.6681, Val Acc: 33.64% | Test Loss: 3.7272, Test Acc: 33.26%\n","[Central-AdamW] Epoch 22: Train Loss: 0.9687, Train Acc: 70.33% | Val Loss: 4.0210, Val Acc: 33.52% | Test Loss: 4.0473, Test Acc: 33.14%\n","[Central-AdamW] Epoch 23: Train Loss: 0.9320, Train Acc: 71.20% | Val Loss: 3.9831, Val Acc: 32.94% | Test Loss: 3.9769, Test Acc: 33.27%\n","[Central-AdamW] Epoch 24: Train Loss: 0.8825, Train Acc: 72.86% | Val Loss: 4.1306, Val Acc: 32.82% | Test Loss: 4.1160, Test Acc: 32.73%\n","[Central-AdamW] Epoch 25: Train Loss: 0.8376, Train Acc: 74.06% | Val Loss: 4.3240, Val Acc: 33.16% | Test Loss: 4.3162, Test Acc: 33.10%\n","[Central-AdamW] Epoch 26: Train Loss: 0.7928, Train Acc: 75.15% | Val Loss: 4.3974, Val Acc: 32.90% | Test Loss: 4.4287, Test Acc: 32.88%\n","[Central-AdamW] Epoch 27: Train Loss: 0.7631, Train Acc: 75.98% | Val Loss: 4.5210, Val Acc: 32.84% | Test Loss: 4.5318, Test Acc: 32.64%\n","[Central-AdamW] Epoch 28: Train Loss: 0.7436, Train Acc: 76.64% | Val Loss: 4.8024, Val Acc: 31.82% | Test Loss: 4.7429, Test Acc: 32.88%\n","[Central-AdamW] Epoch 29: Train Loss: 0.6873, Train Acc: 78.15% | Val Loss: 4.8026, Val Acc: 32.30% | Test Loss: 4.8546, Test Acc: 32.24%\n","[Central-AdamW] Epoch 30: Train Loss: 0.6715, Train Acc: 78.61% | Val Loss: 5.0715, Val Acc: 31.86% | Test Loss: 4.9986, Test Acc: 32.93%\n","[Central-AdamW] Epoch 31: Train Loss: 0.6349, Train Acc: 79.69% | Val Loss: 5.0869, Val Acc: 31.68% | Test Loss: 5.1028, Test Acc: 32.16%\n","[Central-AdamW] Epoch 32: Train Loss: 0.6171, Train Acc: 80.30% | Val Loss: 5.2384, Val Acc: 31.76% | Test Loss: 5.2216, Test Acc: 31.82%\n","[Central-AdamW] Epoch 33: Train Loss: 0.5841, Train Acc: 80.98% | Val Loss: 5.3768, Val Acc: 31.58% | Test Loss: 5.3765, Test Acc: 32.13%\n","[Central-AdamW] Epoch 34: Train Loss: 0.5456, Train Acc: 82.23% | Val Loss: 5.6640, Val Acc: 31.74% | Test Loss: 5.6405, Test Acc: 32.22%\n","[Central-AdamW] Epoch 35: Train Loss: 0.5499, Train Acc: 82.25% | Val Loss: 5.7159, Val Acc: 31.84% | Test Loss: 5.7049, Test Acc: 31.85%\n","[Central-AdamW] Epoch 36: Train Loss: 0.5202, Train Acc: 83.12% | Val Loss: 5.7940, Val Acc: 31.42% | Test Loss: 5.7535, Test Acc: 31.93%\n","[Central-AdamW] Epoch 37: Train Loss: 0.4981, Train Acc: 83.70% | Val Loss: 5.9017, Val Acc: 31.94% | Test Loss: 6.0004, Test Acc: 31.88%\n","[Central-AdamW] Epoch 38: Train Loss: 0.4714, Train Acc: 84.60% | Val Loss: 6.2608, Val Acc: 32.36% | Test Loss: 6.2122, Test Acc: 31.49%\n","[Central-AdamW] Epoch 39: Train Loss: 0.4479, Train Acc: 85.53% | Val Loss: 6.2895, Val Acc: 31.72% | Test Loss: 6.3677, Test Acc: 31.68%\n","[Central-AdamW] Epoch 40: Train Loss: 0.4429, Train Acc: 85.61% | Val Loss: 6.3582, Val Acc: 31.88% | Test Loss: 6.4389, Test Acc: 31.62%\n","[Central-AdamW] Epoch 41: Train Loss: 0.4438, Train Acc: 85.46% | Val Loss: 6.4065, Val Acc: 31.56% | Test Loss: 6.4385, Test Acc: 31.43%\n","[Central-AdamW] Epoch 42: Train Loss: 0.3790, Train Acc: 87.44% | Val Loss: 6.5612, Val Acc: 32.08% | Test Loss: 6.5617, Test Acc: 31.37%\n","[Central-AdamW] Epoch 43: Train Loss: 0.3713, Train Acc: 87.80% | Val Loss: 6.8851, Val Acc: 32.14% | Test Loss: 6.8061, Test Acc: 31.16%\n","[Central-AdamW] Epoch 44: Train Loss: 0.4078, Train Acc: 86.75% | Val Loss: 6.8292, Val Acc: 31.70% | Test Loss: 6.7225, Test Acc: 31.41%\n","[Central-AdamW] Epoch 45: Train Loss: 0.3554, Train Acc: 88.39% | Val Loss: 6.9470, Val Acc: 31.44% | Test Loss: 6.9365, Test Acc: 31.45%\n","[Central-AdamW] Epoch 46: Train Loss: 0.3464, Train Acc: 88.59% | Val Loss: 7.0519, Val Acc: 31.34% | Test Loss: 7.0731, Test Acc: 31.33%\n","[Central-AdamW] Epoch 47: Train Loss: 0.3322, Train Acc: 89.05% | Val Loss: 7.2191, Val Acc: 31.96% | Test Loss: 7.2585, Test Acc: 31.51%\n","[Central-AdamW] Epoch 48: Train Loss: 0.3122, Train Acc: 89.63% | Val Loss: 7.4024, Val Acc: 31.78% | Test Loss: 7.4425, Test Acc: 31.41%\n","[Central-AdamW] Epoch 49: Train Loss: 0.3038, Train Acc: 90.00% | Val Loss: 7.5506, Val Acc: 31.38% | Test Loss: 7.5942, Test Acc: 31.00%\n","[Central-AdamW] Epoch 50: Train Loss: 0.2861, Train Acc: 90.49% | Val Loss: 7.7486, Val Acc: 31.54% | Test Loss: 7.6781, Test Acc: 31.48%\n","[Central-AdamW] Epoch 51: Train Loss: 0.2715, Train Acc: 90.89% | Val Loss: 7.8077, Val Acc: 31.10% | Test Loss: 7.8402, Test Acc: 31.50%\n","[Central-AdamW] Epoch 52: Train Loss: 0.2830, Train Acc: 90.74% | Val Loss: 7.8394, Val Acc: 31.84% | Test Loss: 7.8423, Test Acc: 31.50%\n","[Central-AdamW] Epoch 53: Train Loss: 0.2768, Train Acc: 90.83% | Val Loss: 7.9090, Val Acc: 31.88% | Test Loss: 7.9589, Test Acc: 31.53%\n","[Central-AdamW] Epoch 54: Train Loss: 0.2522, Train Acc: 91.77% | Val Loss: 8.0043, Val Acc: 31.70% | Test Loss: 7.9834, Test Acc: 31.19%\n","[Central-AdamW] Epoch 55: Train Loss: 0.2395, Train Acc: 91.95% | Val Loss: 8.2984, Val Acc: 31.74% | Test Loss: 8.3255, Test Acc: 31.65%\n","[Central-AdamW] Epoch 56: Train Loss: 0.2328, Train Acc: 92.21% | Val Loss: 8.3401, Val Acc: 30.74% | Test Loss: 8.1920, Test Acc: 30.81%\n","[Central-AdamW] Epoch 57: Train Loss: 0.2124, Train Acc: 92.92% | Val Loss: 8.4499, Val Acc: 31.40% | Test Loss: 8.4308, Test Acc: 31.56%\n","[Central-AdamW] Epoch 58: Train Loss: 0.2170, Train Acc: 92.77% | Val Loss: 8.5226, Val Acc: 31.14% | Test Loss: 8.5088, Test Acc: 30.91%\n","[Central-AdamW] Epoch 59: Train Loss: 0.2181, Train Acc: 92.89% | Val Loss: 8.5951, Val Acc: 31.56% | Test Loss: 8.6478, Test Acc: 31.34%\n","[Central-AdamW] Epoch 60: Train Loss: 0.2049, Train Acc: 93.27% | Val Loss: 8.4995, Val Acc: 31.16% | Test Loss: 8.4368, Test Acc: 31.44%\n","[Central-AdamW] Epoch 61: Train Loss: 0.2061, Train Acc: 93.08% | Val Loss: 8.6606, Val Acc: 31.08% | Test Loss: 8.6321, Test Acc: 31.35%\n","[Central-AdamW] Epoch 62: Train Loss: 0.1749, Train Acc: 94.24% | Val Loss: 8.7839, Val Acc: 31.26% | Test Loss: 8.8082, Test Acc: 31.41%\n","[Central-AdamW] Epoch 63: Train Loss: 0.1600, Train Acc: 94.71% | Val Loss: 8.8717, Val Acc: 30.88% | Test Loss: 8.7824, Test Acc: 31.70%\n","[Central-AdamW] Epoch 64: Train Loss: 0.1901, Train Acc: 93.73% | Val Loss: 9.2491, Val Acc: 30.98% | Test Loss: 9.2797, Test Acc: 31.11%\n","[Central-AdamW] Epoch 65: Train Loss: 0.1547, Train Acc: 94.85% | Val Loss: 9.0363, Val Acc: 31.14% | Test Loss: 9.0051, Test Acc: 31.38%\n","[Central-AdamW] Epoch 66: Train Loss: 0.1437, Train Acc: 95.34% | Val Loss: 9.2599, Val Acc: 31.44% | Test Loss: 9.3077, Test Acc: 31.05%\n","[Central-AdamW] Epoch 67: Train Loss: 0.1396, Train Acc: 95.46% | Val Loss: 9.5850, Val Acc: 31.30% | Test Loss: 9.5218, Test Acc: 31.37%\n","[Central-AdamW] Epoch 68: Train Loss: 0.1611, Train Acc: 94.76% | Val Loss: 9.2653, Val Acc: 31.12% | Test Loss: 9.3112, Test Acc: 30.93%\n","[Central-AdamW] Epoch 69: Train Loss: 0.1395, Train Acc: 95.46% | Val Loss: 9.5563, Val Acc: 31.20% | Test Loss: 9.5116, Test Acc: 31.22%\n","[Central-AdamW] Epoch 70: Train Loss: 0.1100, Train Acc: 96.57% | Val Loss: 9.5203, Val Acc: 31.32% | Test Loss: 9.4172, Test Acc: 31.73%\n","[Central-AdamW] Epoch 71: Train Loss: 0.1345, Train Acc: 95.57% | Val Loss: 9.5828, Val Acc: 31.52% | Test Loss: 9.5295, Test Acc: 31.76%\n","[Central-AdamW] Epoch 72: Train Loss: 0.0979, Train Acc: 96.90% | Val Loss: 9.8578, Val Acc: 31.60% | Test Loss: 9.7428, Test Acc: 31.44%\n","[Central-AdamW] Epoch 73: Train Loss: 0.1273, Train Acc: 95.78% | Val Loss: 9.5904, Val Acc: 31.28% | Test Loss: 9.5259, Test Acc: 31.60%\n","[Central-AdamW] Epoch 74: Train Loss: 0.1023, Train Acc: 96.71% | Val Loss: 9.8813, Val Acc: 31.20% | Test Loss: 9.9069, Test Acc: 31.49%\n","[Central-AdamW] Epoch 75: Train Loss: 0.0941, Train Acc: 97.06% | Val Loss: 10.0777, Val Acc: 31.40% | Test Loss: 10.0499, Test Acc: 31.40%\n","[Central-AdamW] Epoch 76: Train Loss: 0.0884, Train Acc: 97.20% | Val Loss: 9.9277, Val Acc: 31.66% | Test Loss: 9.8342, Test Acc: 31.51%\n","[Central-AdamW] Epoch 77: Train Loss: 0.0901, Train Acc: 97.21% | Val Loss: 10.2232, Val Acc: 30.66% | Test Loss: 10.1293, Test Acc: 31.53%\n","[Central-AdamW] Epoch 78: Train Loss: 0.1031, Train Acc: 96.76% | Val Loss: 9.9196, Val Acc: 31.16% | Test Loss: 9.9642, Test Acc: 31.18%\n","[Central-AdamW] Epoch 79: Train Loss: 0.0632, Train Acc: 98.12% | Val Loss: 10.2051, Val Acc: 31.58% | Test Loss: 10.0627, Test Acc: 31.74%\n","[Central-AdamW] Epoch 80: Train Loss: 0.0852, Train Acc: 97.18% | Val Loss: 10.0854, Val Acc: 30.62% | Test Loss: 9.9852, Test Acc: 31.05%\n","[Central-AdamW] Epoch 81: Train Loss: 0.0725, Train Acc: 97.71% | Val Loss: 10.4181, Val Acc: 31.60% | Test Loss: 10.2677, Test Acc: 31.13%\n","[Central-AdamW] Epoch 82: Train Loss: 0.0597, Train Acc: 98.19% | Val Loss: 10.6525, Val Acc: 31.02% | Test Loss: 10.5616, Test Acc: 31.66%\n","[Central-AdamW] Epoch 83: Train Loss: 0.0655, Train Acc: 97.95% | Val Loss: 10.5693, Val Acc: 31.44% | Test Loss: 10.3949, Test Acc: 31.47%\n","[Central-AdamW] Epoch 84: Train Loss: 0.0494, Train Acc: 98.58% | Val Loss: 10.5167, Val Acc: 32.18% | Test Loss: 10.4579, Test Acc: 31.81%\n","[Central-AdamW] Epoch 85: Train Loss: 0.0529, Train Acc: 98.44% | Val Loss: 10.6403, Val Acc: 31.44% | Test Loss: 10.5546, Test Acc: 31.63%\n","[Central-AdamW] Epoch 86: Train Loss: 0.0609, Train Acc: 98.16% | Val Loss: 10.8253, Val Acc: 31.34% | Test Loss: 10.7554, Test Acc: 31.66%\n","[Central-AdamW] Epoch 87: Train Loss: 0.0538, Train Acc: 98.40% | Val Loss: 10.7809, Val Acc: 31.68% | Test Loss: 10.6048, Test Acc: 31.77%\n","[Central-AdamW] Epoch 88: Train Loss: 0.0457, Train Acc: 98.68% | Val Loss: 10.8732, Val Acc: 32.18% | Test Loss: 10.7028, Test Acc: 32.38%\n","[Central-AdamW] Epoch 89: Train Loss: 0.0286, Train Acc: 99.27% | Val Loss: 10.7381, Val Acc: 31.52% | Test Loss: 10.7013, Test Acc: 31.78%\n","[Central-AdamW] Epoch 90: Train Loss: 0.0450, Train Acc: 98.64% | Val Loss: 10.8994, Val Acc: 31.28% | Test Loss: 10.8315, Test Acc: 31.34%\n","[Central-AdamW] Epoch 91: Train Loss: 0.0377, Train Acc: 98.94% | Val Loss: 11.1137, Val Acc: 31.56% | Test Loss: 10.9772, Test Acc: 31.92%\n","[Central-AdamW] Epoch 92: Train Loss: 0.0405, Train Acc: 98.84% | Val Loss: 10.9566, Val Acc: 31.58% | Test Loss: 10.8129, Test Acc: 31.40%\n","[Central-AdamW] Epoch 93: Train Loss: 0.0176, Train Acc: 99.63% | Val Loss: 11.0722, Val Acc: 31.56% | Test Loss: 10.9775, Test Acc: 31.76%\n","[Central-AdamW] Epoch 94: Train Loss: 0.0171, Train Acc: 99.68% | Val Loss: 11.1817, Val Acc: 31.56% | Test Loss: 11.1053, Test Acc: 31.83%\n","[Central-AdamW] Epoch 95: Train Loss: 0.0514, Train Acc: 98.38% | Val Loss: 11.1932, Val Acc: 31.98% | Test Loss: 11.0396, Test Acc: 31.33%\n","[Central-AdamW] Epoch 96: Train Loss: 0.0282, Train Acc: 99.26% | Val Loss: 11.2992, Val Acc: 32.00% | Test Loss: 11.1433, Test Acc: 32.16%\n","[Central-AdamW] Epoch 97: Train Loss: 0.0117, Train Acc: 99.80% | Val Loss: 11.3190, Val Acc: 31.56% | Test Loss: 11.1499, Test Acc: 31.98%\n","[Central-AdamW] Epoch 98: Train Loss: 0.0122, Train Acc: 99.76% | Val Loss: 11.3072, Val Acc: 31.84% | Test Loss: 11.1744, Test Acc: 32.16%\n","[Central-AdamW] Epoch 99: Train Loss: 0.0069, Train Acc: 99.90% | Val Loss: 11.3846, Val Acc: 31.36% | Test Loss: 11.2663, Test Acc: 31.82%\n","[Central-AdamW] Epoch 100: Train Loss: 0.0483, Train Acc: 98.48% | Val Loss: 11.3958, Val Acc: 32.02% | Test Loss: 11.2739, Test Acc: 32.04%\n","[Central-AdamW] Epoch 101: Train Loss: 0.0165, Train Acc: 99.62% | Val Loss: 11.3938, Val Acc: 32.04% | Test Loss: 11.2696, Test Acc: 31.93%\n","[Central-AdamW] Epoch 102: Train Loss: 0.0065, Train Acc: 99.90% | Val Loss: 11.4606, Val Acc: 32.02% | Test Loss: 11.2784, Test Acc: 32.27%\n","[Central-AdamW] Epoch 103: Train Loss: 0.0048, Train Acc: 99.93% | Val Loss: 11.5450, Val Acc: 31.86% | Test Loss: 11.3600, Test Acc: 32.42%\n","[Central-AdamW] Epoch 104: Train Loss: 0.0050, Train Acc: 99.94% | Val Loss: 11.5869, Val Acc: 31.52% | Test Loss: 11.4344, Test Acc: 31.83%\n","[Central-AdamW] Epoch 105: Train Loss: 0.0360, Train Acc: 98.97% | Val Loss: 11.4176, Val Acc: 31.70% | Test Loss: 11.3187, Test Acc: 31.77%\n","[Central-AdamW] Epoch 106: Train Loss: 0.0101, Train Acc: 99.79% | Val Loss: 11.4946, Val Acc: 31.82% | Test Loss: 11.3322, Test Acc: 31.81%\n","[Central-AdamW] Epoch 107: Train Loss: 0.0036, Train Acc: 99.97% | Val Loss: 11.5155, Val Acc: 31.96% | Test Loss: 11.3754, Test Acc: 32.19%\n","[Central-AdamW] Epoch 108: Train Loss: 0.0028, Train Acc: 99.97% | Val Loss: 11.5474, Val Acc: 32.48% | Test Loss: 11.4370, Test Acc: 32.20%\n","[Central-AdamW] Epoch 109: Train Loss: 0.0041, Train Acc: 99.94% | Val Loss: 11.6296, Val Acc: 31.64% | Test Loss: 11.5242, Test Acc: 32.33%\n","[Central-AdamW] Epoch 110: Train Loss: 0.0047, Train Acc: 99.95% | Val Loss: 11.6649, Val Acc: 31.98% | Test Loss: 11.5508, Test Acc: 32.33%\n","[Central-AdamW] Epoch 111: Train Loss: 0.0051, Train Acc: 99.92% | Val Loss: 11.9737, Val Acc: 32.22% | Test Loss: 11.8773, Test Acc: 32.05%\n","[Central-AdamW] Epoch 112: Train Loss: 0.0150, Train Acc: 99.66% | Val Loss: 11.6590, Val Acc: 32.32% | Test Loss: 11.4992, Test Acc: 32.07%\n","[Central-AdamW] Epoch 113: Train Loss: 0.0032, Train Acc: 99.96% | Val Loss: 11.7314, Val Acc: 32.40% | Test Loss: 11.5628, Test Acc: 32.38%\n","[Central-AdamW] Epoch 114: Train Loss: 0.0020, Train Acc: 99.98% | Val Loss: 11.8056, Val Acc: 31.98% | Test Loss: 11.6301, Test Acc: 32.50%\n","[Central-AdamW] Epoch 115: Train Loss: 0.0038, Train Acc: 99.94% | Val Loss: 11.9041, Val Acc: 32.52% | Test Loss: 11.7656, Test Acc: 32.36%\n","[Central-AdamW] Epoch 116: Train Loss: 0.0029, Train Acc: 99.96% | Val Loss: 11.9011, Val Acc: 32.10% | Test Loss: 11.7347, Test Acc: 32.35%\n","[Central-AdamW] Epoch 117: Train Loss: 0.0054, Train Acc: 99.88% | Val Loss: 11.8616, Val Acc: 32.32% | Test Loss: 11.7368, Test Acc: 31.79%\n","[Central-AdamW] Epoch 118: Train Loss: 0.0030, Train Acc: 99.96% | Val Loss: 11.9185, Val Acc: 32.28% | Test Loss: 11.8021, Test Acc: 32.24%\n","[Central-AdamW] Epoch 119: Train Loss: 0.0022, Train Acc: 99.96% | Val Loss: 11.9260, Val Acc: 32.56% | Test Loss: 11.8042, Test Acc: 32.48%\n","[Central-AdamW] Epoch 120: Train Loss: 0.0021, Train Acc: 99.96% | Val Loss: 11.9377, Val Acc: 32.30% | Test Loss: 11.7752, Test Acc: 32.33%\n","[Central-AdamW] Epoch 121: Train Loss: 0.0024, Train Acc: 99.96% | Val Loss: 12.1018, Val Acc: 32.14% | Test Loss: 11.9623, Test Acc: 32.58%\n","[Central-AdamW] Epoch 122: Train Loss: 0.0026, Train Acc: 99.96% | Val Loss: 12.0770, Val Acc: 32.42% | Test Loss: 11.9406, Test Acc: 32.54%\n","[Central-AdamW] Epoch 123: Train Loss: 0.0017, Train Acc: 99.97% | Val Loss: 12.1251, Val Acc: 32.38% | Test Loss: 12.0140, Test Acc: 32.32%\n","[Central-AdamW] Epoch 124: Train Loss: 0.0021, Train Acc: 99.96% | Val Loss: 12.2254, Val Acc: 32.52% | Test Loss: 12.0976, Test Acc: 32.43%\n","[Central-AdamW] Epoch 125: Train Loss: 0.0018, Train Acc: 99.96% | Val Loss: 12.2644, Val Acc: 32.56% | Test Loss: 12.1186, Test Acc: 32.59%\n","[Central-AdamW] Epoch 126: Train Loss: 0.0016, Train Acc: 99.96% | Val Loss: 12.3126, Val Acc: 32.22% | Test Loss: 12.1494, Test Acc: 32.64%\n","[Central-AdamW] Epoch 127: Train Loss: 0.0012, Train Acc: 99.97% | Val Loss: 12.4586, Val Acc: 32.48% | Test Loss: 12.3199, Test Acc: 32.62%\n","[Central-AdamW] Epoch 128: Train Loss: 0.0011, Train Acc: 99.98% | Val Loss: 12.4144, Val Acc: 32.38% | Test Loss: 12.2593, Test Acc: 32.80%\n","[Central-AdamW] Epoch 129: Train Loss: 0.0009, Train Acc: 99.98% | Val Loss: 12.5076, Val Acc: 32.38% | Test Loss: 12.3386, Test Acc: 32.69%\n","[Central-AdamW] Epoch 130: Train Loss: 0.0012, Train Acc: 99.98% | Val Loss: 12.5765, Val Acc: 32.84% | Test Loss: 12.4236, Test Acc: 32.86%\n","[Central-AdamW] Epoch 131: Train Loss: 0.0009, Train Acc: 99.97% | Val Loss: 12.7248, Val Acc: 32.20% | Test Loss: 12.5805, Test Acc: 32.75%\n","[Central-AdamW] Epoch 132: Train Loss: 0.0008, Train Acc: 99.97% | Val Loss: 12.6530, Val Acc: 32.70% | Test Loss: 12.4916, Test Acc: 32.57%\n","[Central-AdamW] Epoch 133: Train Loss: 0.0007, Train Acc: 99.98% | Val Loss: 12.7019, Val Acc: 32.52% | Test Loss: 12.5308, Test Acc: 32.65%\n","[Central-AdamW] Epoch 134: Train Loss: 0.0006, Train Acc: 99.98% | Val Loss: 12.9360, Val Acc: 32.36% | Test Loss: 12.7746, Test Acc: 32.82%\n","[Central-AdamW] Epoch 135: Train Loss: 0.0007, Train Acc: 99.98% | Val Loss: 12.8596, Val Acc: 32.62% | Test Loss: 12.7018, Test Acc: 32.90%\n","[Central-AdamW] Epoch 136: Train Loss: 0.0006, Train Acc: 99.98% | Val Loss: 12.9334, Val Acc: 32.66% | Test Loss: 12.7748, Test Acc: 32.81%\n","[Central-AdamW] Epoch 137: Train Loss: 0.0005, Train Acc: 99.98% | Val Loss: 13.0249, Val Acc: 32.50% | Test Loss: 12.8603, Test Acc: 32.81%\n","[Central-AdamW] Epoch 138: Train Loss: 0.0004, Train Acc: 99.99% | Val Loss: 13.0428, Val Acc: 32.62% | Test Loss: 12.8583, Test Acc: 32.75%\n","[Central-AdamW] Epoch 139: Train Loss: 0.0004, Train Acc: 99.98% | Val Loss: 13.0882, Val Acc: 32.62% | Test Loss: 12.9116, Test Acc: 32.72%\n","[Central-AdamW] Epoch 140: Train Loss: 0.0004, Train Acc: 99.98% | Val Loss: 13.1393, Val Acc: 32.58% | Test Loss: 12.9682, Test Acc: 32.75%\n","[Central-AdamW] Epoch 141: Train Loss: 0.0004, Train Acc: 99.99% | Val Loss: 13.1890, Val Acc: 32.52% | Test Loss: 13.0108, Test Acc: 32.79%\n","[Central-AdamW] Epoch 142: Train Loss: 0.0003, Train Acc: 99.99% | Val Loss: 13.2159, Val Acc: 32.64% | Test Loss: 13.0470, Test Acc: 32.88%\n","[Central-AdamW] Epoch 143: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2313, Val Acc: 32.62% | Test Loss: 13.0619, Test Acc: 32.76%\n","[Central-AdamW] Epoch 144: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2467, Val Acc: 32.64% | Test Loss: 13.0734, Test Acc: 32.80%\n","[Central-AdamW] Epoch 145: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2552, Val Acc: 32.62% | Test Loss: 13.0822, Test Acc: 32.76%\n","[Central-AdamW] Epoch 146: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2735, Val Acc: 32.64% | Test Loss: 13.1009, Test Acc: 32.78%\n","[Central-AdamW] Epoch 147: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2831, Val Acc: 32.68% | Test Loss: 13.1094, Test Acc: 32.76%\n","[Central-AdamW] Epoch 148: Train Loss: 0.0003, Train Acc: 99.98% | Val Loss: 13.2870, Val Acc: 32.68% | Test Loss: 13.1139, Test Acc: 32.75%\n","[Central-AdamW] Epoch 149: Train Loss: 0.0003, Train Acc: 99.99% | Val Loss: 13.2891, Val Acc: 32.66% | Test Loss: 13.1157, Test Acc: 32.74%\n","[Central-AdamW] Epoch 150: Train Loss: 0.0003, Train Acc: 99.99% | Val Loss: 13.2897, Val Acc: 32.66% | Test Loss: 13.1162, Test Acc: 32.75%"],"metadata":{"id":"nQQ0i6R33zF-"}}]}